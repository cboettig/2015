issue,comment_id,user,created_at,updated_at,body,state,comments,title
4,20027343,sckott,2013-06-26T05:05:02Z,2013-06-26T05:05:02Z,"Although, we could make an R interface to git...",NA,NA,NA
4,20029787,karthik,2013-06-26T06:46:32Z,2013-06-26T06:46:32Z,"@SChamberlain How would you do that? I wondered that too at Scifoo where I met the creator of [Authorea](https://www.authorea.com/). We talked about a ropensci interface to Authorea but ultimately it boils down to a git interface. 
",NA,NA,NA
4,20030164,sckott,2013-06-26T06:59:40Z,2013-06-26T06:59:40Z,"Hmm, not sure, wil think about it

On Tuesday, June 25, 2013, Karthik Ram wrote:

> @SChamberlain <https://github.com/SChamberlain> How would you do that? I
> wondered that too at Scifoo where I met the creator of Authorea<https://www.authorea.com/>.
> We talked about a ropensci interface to Authorea but ultimately it boils
> down to a git interface.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/4#issuecomment-20029787>
> .
>",NA,NA,NA
4,20054853,cboettig,2013-06-26T15:19:21Z,2013-06-26T15:19:21Z,"One might start with an RCurl implementation of the Github API.  That would provide data for things of interest (commits, issues, activity, org, users, etc) http://developer.github.com/v3/

For git commands though you might be stuck with wrapping `system` calls (and possibly having to parse their return text from `intern=TRUE`).  Perhaps that could be adequate.  It would allow something like `eml_publish(..., dest=github)` but I'm not sure if that would actually be useful.  I'd want a function to write the URL endpoints into the EML but I might rather push myself...",NA,NA,NA
4,20062714,sckott,2013-06-26T16:50:09Z,2013-06-26T16:50:09Z,"I have some functions I already wrote to do that blog post on github stats on our repos - functions are in the `ropensci::sandbox` repo. Looks like you can actually create commits via the API, and perhaps push, though not positive about that. ",NA,NA,NA
4,20069504,sckott,2013-06-26T18:29:21Z,2013-06-26T18:29:21Z,"Okay, fixed user-agent string now required, just a few functions in the sandbox repo: https://github.com/ropensci/sandbox

Should probably make a separate repo for github...",NA,NA,NA
2,20134120,cboettig,2013-06-27T16:09:21Z,2013-06-27T16:09:21Z,"Basic writing of a valid EML object from an R data.frame is now implemented:

Given the data and metadata above, we simply do:

```ruby
eml$set(givenName = ""Carl"", surName = ""Boettiger"", email = ""cboettig@ropensci.org"")
eml_write(dat, col_metadata, unit_metadata, file=""my_eml_data.xml"")
```

- We translate R object types (integer, float, factor, character string, POSIXt time) automatically
- Generates minimal EML (A dataset node with a contact node and dataTable node)

Much to be done in providing additional metadata fields and support for alternative data types, units, and formats.  See issues under write eml.",NA,NA,NA
6,20172063,cboettig,2013-06-28T06:09:20Z,2013-06-28T06:09:20Z,"Read should be able to handle local files, not just online files.  Likewise an issue for `eml_write` #2 ",NA,NA,NA
6,20172319,cboettig,2013-06-28T06:19:49Z,2013-06-28T19:27:34Z,"Bug in date time parsing: 

```ruby
out <- eml_read(""http://harvardforest.fas.harvard.edu/data/eml/hf205.xml"")
head(out$dataframe)
out$col_metadata
out$unit_metadata
```


In other reading checks, note that this EML file also arbitrarily declares that the data has 9999 columns, while we see that it has 

```ruby
dim(out$dataframe)[1]
[1] 279224
```

### Issues and next steps 

Of course `eml_read` is still a work in progress.  In particular:

(Also note that `eml_read` __still needs a dateTime parser__ on the unit_metadata....)

- [x] How do we deal with dateTime when defined across multiple columns?

",NA,NA,NA
6,20209089,cboettig,2013-06-28T19:26:58Z,2013-06-28T19:26:58Z,"- dateTime bug fixed in 8b7ffee68e118f18b8f5780fb77bb8c5ff63f51d by coercing to `character`.  remaining dateTime issues are now in #17 

- The Harvard Forest `hf205.xml` example still does some wierd things, e.g. the run id numbers (1-6) are encoded as character strings, where a ordered factor might have been more appropriate.  Guess we should just obey the metadata we are given...

- The basic example proposed at the top of this issue does not demonstrate different encodings well, e.g. it uses only a trivial ratio unit (count) and no ordered factors or characters...

",NA,NA,NA
3,20217139,cboettig,2013-06-28T22:05:10Z,2013-06-28T22:05:10Z,"Need to think out workflow issues here.  

### Option 1

Generate the EML first with `eml_write`.  Then call `eml_publish(""file.eml"", destination=""figshare"")` to publish.  

This approach would rely on the EML file to generate the minimal metadata required by figshare, along with any additional metadata we choose to supply.  

This would create a problem for cases where the EML file (perhaps not generated by REML) did contain enough metadata for figshare publication (e.g. figshare category).  I suppose we could warn in those cases, or include an argument where the user could provide a category, etc.  

### Option 2

Call `eml_publish` directly with data and metadata, avoiding the need to call `eml_write` explicitly (in this version, `eml_publish` would obviously call `eml_write`.  In some ways this is easier to write, since the `eml_publish` just takes a list of all the arguments to `fs_new_article` (or the components thereof), along with all the arguments to `eml_write` (to which it could also append the figshare metadata).  

The down-side of this approach is that the function call itself may be a bit more clumsy.

### Both?

We might want to support both options, but providing a clean and intuitive API (function call) becomes tricky...

### Additional strategies and things to look out for

- Want to avoid re-writing metadata to the EML that already exists there.  
- Default settings probably want to include support for some figshare metadata, such as preferred category. 
- Maybe an `id=figshare` metadata node (with EML keywordList, etc) should be added?  Would be the easiest way
",NA,NA,NA
26,20234727,karthik,2013-06-29T18:32:45Z,2013-06-29T18:38:58Z,"I noticed the same issue too. Tried deleting and regenerating documentation in case an older name resulted in that leftover file. If I can't resolve it shortly I'll ping @cboettig 

OK, I resolved the duplicate man file but I cannot commit that change no matter what. After each staging  `man/eml_dataTable.Rd` it still as modified. 
",NA,NA,NA
20,20234859,mbjones,2013-06-29T18:40:40Z,2013-06-29T18:40:40Z,Should be pretty straightforward using the dataone library -- the only issue will be logging in via CILogin to get the user certificate before attempting the MN.create() operation from the dataone library.,NA,NA,NA
20,20235710,cboettig,2013-06-29T19:30:11Z,2013-06-29T19:30:11Z,"Sounds great.  We handle OAuth login already for rfigshare.  Not familiar
with CILogin though.


On Sat, Jun 29, 2013 at 11:40 AM, Matt Jones <notifications@github.com>wrote:

> Should be pretty straightforward using the dataone library -- the only
> issue will be logging in via CILogin to get the user certificate before
> attempting the MN.create() operation from the dataone library.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/20#issuecomment-20234859>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
15,20235942,mbjones,2013-06-29T19:44:15Z,2013-06-29T19:44:15Z,"We have XSL stylesheets that translate EML to FGDC BDP and BDP to EML.  Form SVN here:

https://code.ecoinformatics.org/code/eml/trunk/lib/eml2tonbii

There is also an EML to ISO19115 stylesheet I think as well, maybe started by GBIF.  Can try to track it down if needed.",NA,NA,NA
7,20236408,mbjones,2013-06-29T20:12:34Z,2013-06-29T20:12:34Z,"Note that the schema does not completely specify all of the restrictions of the EML Specification.  The conditional restrictions on uniqueness of IDs depending on the value of the scope attribute was simply not expressible in XML Schema.  So, we wrote the EML Validator to accompany the spec which fully validates the document.  This is available as a service on the web (http://knb.ecoinformatics.org/emlparser/), and can be run from a Java API call after compiling the EMLParser validation class.  Source code is in the EML SVN repo: https://code.ecoinformatics.org/code/eml/trunk/src/org/ecoinformatics/eml/EMLParser.java

",NA,NA,NA
5,20236972,mbjones,2013-06-29T20:51:44Z,2013-06-30T09:43:41Z,"1. Endpoints on the KNB should use the DataONE MN.get() REST endpoint, so for example, for doi:10.5063/AA/nceas.912.9:
    https://knb.ecoinformatics.org/knb/d1/mn/v1/object/doi:10.5063%2FAA%2Fnceas.912.9

However, note that we also recommend using the DataONE CN.resolve() service to find the list of nodes that might currently both have a copy of an object and are currently available on the network.  The resolve() call returns a list of nodes that contain the object and the REST url for retrieving it.  So, for example:

```bash
$ curl -s https://cn.dataone.org/cn/v1/resolve/doi%3A10.5063%2FAA%2Fnceas.912.9 | xmlstarlet fo
```
```xml
<?xml version=""1.0"" encoding=""UTF-8""?>
<d1:objectLocationList xmlns:d1=""http://ns.dataone.org/service/types/v1"">
  <identifier>doi:10.5063/AA/nceas.912.9</identifier>
  <objectLocation>
    <nodeIdentifier>urn:node:KNB</nodeIdentifier>
    <baseURL>https://knb.ecoinformatics.org/knb/d1/mn</baseURL>
    <version>v1</version>
    <url>https://knb.ecoinformatics.org/knb/d1/mn/v1/object/doi:10.5063%2FAA%2Fnceas.912.9</url>
  </objectLocation>
  <objectLocation>
    <nodeIdentifier>urn:node:CN</nodeIdentifier>
    <baseURL>https://cn.dataone.org/cn</baseURL>
    <version>v1</version>
    <url>https://cn.dataone.org/cn/v1/object/doi:10.5063%2FAA%2Fnceas.912.9</url>
  </objectLocation>
</d1:objectLocationList>
```

2. Regarding IDs, the EML spec leaves it open other than saying they must be unique in the document.  The point is to provide an unambiguous identifier to reference `<attribute>` definitions in EML.  These can then be used in other places to refer to those attribute definitions.

3. EML doesn't specify how to define an attribute beyond using the natural language definition.  That said, for OBOE we have come up with an annotation syntax that could be used in the additionalMetadata section to provide a linkage between the attribute definition in EML and an ontology.  Some examples of its use are in SVN (https://code.ecoinformatics.org/code/semtools/trunk/dev/sms/examples).  This is probably more complicated than you are looking for, as it maps several different semantic aspects of the data set, including the Characteristic being measured (what you are looking for I think), as well as the Entity being measured, the MeasurementStandard used (redundant with other fields in EML), and the Context.  This is the mapping we've been experimenting with in Semtools and is the basis of the figure that you included in issue #8.  There is an XML Schema for the annotation syntax in the directory above the examples.  The annotation is in XML, but it could also be done in RDF, which would merge better with the OBOE OWL ontology.  In addition, we debated over whether its better to include the annotation inline in the EML document (which nicely packages them together), or to provide a separate annotation file (which allows people other than the EML owner to provide annotations, and lets us annotate metadata files other than EML (such as FGDC).  Which is best is still under discussion in our group.  We have built out a prototype extension of Morpho that produces these annotations as separate files, and then a Metacat search service that knows how to use them to do semantic-driven searches and data integration tasks.  It would be great to discuss how this relates to what you are trying to do in R, and what we might adapt for compatibility.",NA,NA,NA
8,20236978,mbjones,2013-06-29T20:52:23Z,2013-06-29T20:52:23Z,See comments on this in issue #5 .,NA,NA,NA
23,20237065,mbjones,2013-06-29T20:58:17Z,2013-06-29T20:58:17Z,"We also have a DOI service when publishing to the KNB (using the standard DataONE MN.reserveIdentifier() service), so it would be nice if the reml API calls were the same for the different repositories.  In general, can we avoid having different API calls for uploading objects to the different repositories?",NA,NA,NA
7,20237086,cboettig,2013-06-29T20:59:43Z,2013-06-29T20:59:43Z,"Fantastic, was wondering about this.

Will have to see if I can figure out the java API call; we should have the
wrappers available in R but I have virtually no java experience. I suppose
we could alternatively bundle the java in the R package and validate
locally.

Looks like this gives us three levels of validation: whether the EML
parses, whether it validates according to the schema, and whether we meet
EML Validator checks for ids.  Any advice on the right workflow / user
interface for this would be good -- e.g. not sure that we would want to run
the EML validator every time a user tries to read in or write out some EML
-- it might be sufficient to know that it parses.  Still, we want to
provide support for these tools...

Also, I'm not actually clear on how / when we should be going about
generating element ids in the first place.  The only place I have element
ids currently is on `<attribute>` nodes (and one on the
`<additionalMetadata id = 'figshare'>` for specifying what metadata is
exposed to figshare's database).  Any advice on how to come up with
`<attribute>` ids?  (currently I create a hash from the
`<attributeDescription>` text,  just as a placeholder -- obviously this is
not what we actually want).


On Sat, Jun 29, 2013 at 1:12 PM, Matt Jones <notifications@github.com>wrote:

> Note that the schema does not completely specify all of the restrictions
> of the EML Specification. The conditional restrictions on uniqueness of IDs
> depending on the value of the scope attribute was simply not expressible in
> XML Schema. So, we wrote the EML Validator to accompany the spec which
> fully validates the document. This is available as a service on the web (
> http://knb.ecoinformatics.org/emlparser/), and can be run from a Java API
> call after compiling the EMLParser validation class. Source code is in the
> EML SVN repo:
> https://code.ecoinformatics.org/code/eml/trunk/src/org/ecoinformatics/eml/EMLParser.java
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/7#issuecomment-20236408>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
23,20237154,cboettig,2013-06-29T21:05:15Z,2013-06-29T21:05:15Z,"I agree entirely.

My thought was that a user would always upload the `eml_publish()`
https://github.com/ropensci/reml/blob/master/R/eml_publish.R.  I've tried
to make the `eml_publish()` API flexible enough to support different
metadata that we may need when pushing to different servers (it takes only
file, destination (e.g. figshare, knb) and ""..."" as arguments).

`eml_figshare` should probably be an internal function.  I'm always a bit
uncertain what to expose in the NAMESPACE and what not to.


On Sat, Jun 29, 2013 at 1:58 PM, Matt Jones <notifications@github.com>wrote:

> We also have a DOI service when publishing to the KNB (using the standard
> DataONE MN.reserveIdentifier() service), so it would be nice if the reml
> API calls were the same for the different repositories. In general, can we
> avoid having different API calls for uploading objects to the different
> repositories?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/23#issuecomment-20237065>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
7,20237713,mbjones,2013-06-29T21:41:15Z,2013-06-29T21:41:15Z,"Regarding the validator -- it doesn't check that much, and so I think it would be much better to just reimplement those checks in R.  The Java code just iterates across a bunch of XML elements, checking that the id attribute on those elements is unique within the document, and then checks that any `<references>` elements in the document point at an `@id` in the document -- i.e., there are no dangling pointers.  The list of elements that are checked is in a config file located here: https://code.ecoinformatics.org/code/eml/trunk/lib/config.xml

About when to check validity -- in Morpho we check validity when we try to upload to an external repository, as we want to ensure that everything is in good working order before distributing the document.  

The only time you really need to generate ids is when you want to have a handle to reference.  The main place for that is for `<attribute>` elements.  The other place that it is commonly used is for unit definitions in STMML in associatedMetadata, and to provide a globally-scoped identifier for individuals (e.g., provide an ORCID ID for someone when listing them as a Creator).",NA,NA,NA
26,20238467,mbjones,2013-06-29T22:37:27Z,2013-06-29T22:37:27Z,"Seems like it is working now -- the dataTable.Rd file is now checked out, and the lowercase file was removed AFAICT. ",NA,NA,NA
18,20239144,cboettig,2013-06-29T23:27:02Z,2013-06-29T23:27:02Z,"Fixed handling of settings in eml_contact, vignette should now compile.  ",NA,NA,NA
5,20239591,cboettig,2013-06-30T00:06:02Z,2013-06-30T00:06:02Z,"Re 1. This is great, can definitely implement this kind of call.

I am curious about what we can offer, if anything, by way of search interfaces for EML data through the reml R package.  Initially I was thinking about querying across large sets of EML files for matching column types, for data integration etc.  Though EML files are generally pretty small, still, downloading and parsing large numbers of them might not be the best way to go.  Thoughts?

Anyway, something to think about down the line at least.  ",NA,NA,NA
8,20239664,cboettig,2013-06-30T00:13:54Z,2013-06-30T00:13:54Z,"The approaches to semantics you outline in #5 sound promising: I see how annotations (e.g. this [one](https://code.ecoinformatics.org/code/semtools/trunk/dev/sms/examples/er-2008-ex1-annot.xml)) map onto the eml (e.g. [this one](https://code.ecoinformatics.org/code/semtools/trunk/dev/sms/examples/er-2008-ex1-eml.xml)), though I don't spot the corresponding lines in the EML that map to the annotation?  

More generally I am curious about how the user would specify semantics; it seems we might always have a variety of ways to actually implement them in.  My understanding of semantics is pretty limited, but I imagine the general idea would be to provide URIs in place of definitions.  In an ideal linked data world, it wouldn't matter if we all used the same URIs for species (e.g. ITIS TaxanomicTypeName) since that could be resolved...  If the user could manage to specify the URIs, we can then figure out behind the scenes whether we write that to a definition node directly or do something more intelligent like the examples you point to.  

Of course I image the first problem is both having URIs for terms users want and helping them discover them.  

I suppose for the moment this might be out of the scope of reml...",NA,NA,NA
28,20244686,mbjones,2013-06-30T09:41:29Z,2013-06-30T09:41:29Z,EML document that is produced now validates against the EML schema and the EMLParser.,NA,NA,NA
14,20317329,cboettig,2013-07-01T23:20:01Z,2013-07-01T23:21:43Z,"Well, we now have some basic unit tests written for the `testthat` package in `inst/tests`.  They are all a bit a crude, and generally involve writing some minimal EML file out and then testing it with some xpath.  

It would be nice to get some feedback on these tests.  I'm not worried about 100% function coverage or anything at this stage, just some reasonable tests to get us started.  But the current tests feel just a little clumsy and arbitrary.  

Closing this issue as it was only for the start of a unit test platform.  We will continue to flush out unit tests as we add functionality, and can add requests for additional unit tests that need writing as additional issues (with ""unit_test"" tag and the coverage tag (write, read, publish, etc))

",NA,NA,NA
16,20319969,cboettig,2013-07-02T00:38:02Z,2013-07-02T00:38:02Z,"- [ ] In a related vein, add the standard R terms for the missing value character (""NA""), unless otherwise specified (from the configuration settings?)  ",NA,NA,NA
12,20370218,cboettig,2013-07-02T19:38:15Z,2013-07-02T19:38:15Z,"@mbjones Can you point me to an example defining a custom unit?  E.g. how would I define a custom unit for ""dollars""?  Is there a linked-data solution to leverage existing definitions of currency? 

Also trying to think out the workflow for a typical user here.  I don't want a user to give up attempting to write data into EML just because their units aren't supported and they can't be bothered writing a STMML definition (though perhaps with a good API that can be easy enough...).  

Because EML is much more relaxed about defining character attributes, this makes it tempting to encode undefined units as characters with something like `<nominal><textDomain><definition>Dollars (US)`. I know we don't want to promote such hacks, be we also want an easy-to-use interface.  Would love feedback. 



",NA,NA,NA
12,20370775,cboettig,2013-07-02T19:47:12Z,2013-07-02T19:47:12Z,"A related issue is something I encounter in my own work: how to encode simulation data in which the units are arbitrary?  For instance, I might wish to indicate that I am simulating a population density or the costs of a conservation activity, but it seems wrong to label these as number Per Hectare or dollars when in fact the scaling is arbitrary.  ",NA,NA,NA
27,20375191,cboettig,2013-07-02T20:49:55Z,2013-07-02T20:49:55Z,"@mbjones When we push a dataset to a repository such as figshare and receive a DOI, where should we file this information?  In `<eml><dataset><publisher>...<something>`, or under `<eml><additionalMetadata> ... ?` Or in `<dataset><additionalIdentifer>`?  or as an id attribute to some node?  

Can you point to a good example of this?  ",NA,NA,NA
27,20382062,mbjones,2013-07-02T22:00:12Z,2013-07-02T22:00:12Z,"If its the main identifier for the EML data package, it should be put in the `<eml @packageId>` attribute, which is the designated location for the package identifier.  If it is a secondary identifier for the package (i.e., not the main one you want cited, but one that should also be synonymous with the package), then it could go into `<dataset>/<additionalIdentifier>`.",NA,NA,NA
27,20382399,cboettig,2013-07-02T22:06:23Z,2013-07-02T22:06:23Z,"Perfect, thanks. Is there a way to denote the identifier is a doing? (Eg a
namespace for identifier?)

---
Carl Boettiger
http://carlboettiger.info

sent from mobile device; my apologies for any terseness or typos
On Jul 2, 2013 3:00 PM, ""Matt Jones"" <notifications@github.com> wrote:

> If its the main identifier for the EML data package, it should be put in
> the <eml @packageId> attribute, which is the designated location for the
> package identifier. If it is a secondary identifier for the package (i.e.,
> not the main one you want cited, but one that should also be synonymous
> with the package), then it could go into <dataset>/<additionalIdentifier>.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/27#issuecomment-20382062>
> .
>",NA,NA,NA
12,20395868,mbjones,2013-07-03T05:04:24Z,2013-07-03T05:04:24Z,"Here's an example of a custom unit definition:

```xml
  <additionalMetadata>
    <unitList>
      <unit id=""numberPerSixtyMetersSquared"" multiplerToSI="".0166667"" name=""numberPerSixtyMetersSquared"" parentSI=""numberPerMeterSquared"" unitType=""arealDensity"">
        <description>Number per sixty meters squared</description>
      </unit>
    </unitList>
  </additionalMetadata>
```
which comes from a PISCO metadata document that shows pretty nice and complete metadata (available from https://knb.ecoinformatics.org/knb/metacat/pisco_subtidal.151.2/xml).

Creating custom units is easy when dealing with physical quantities like length or mass (as defined by NIST), as these all have base SI units and are easily convertible.  However, it gets tricky with other things that appear to be quantities, but really are not (such as currency).  There is no standard or base unit for currency, in that the value of every currency fluctuates over time (2009 dollars are different from 2010 dollars), and so currencies are not 'unit's in the traditional sense.  You could define a UnitType for Currency, then define a base unit for that (say Dollars), and then define all other currencies as a conversion from the base Dollar, but of course the conversions would be wrong as the values fluctuated.  You might have better luck defining a unit for a currency for a year, which are fairly well established (e.g., a '1970Dollar'), and then have each of the other annualized currency units convert from that, which would be more accurate, at least within the bounds of an annualized mean value.

Other types of units that you'll likely find problematic are for dimensionless values, such as counts, percentages, and ratios.  For example, there is technically no unit for the count of animals in a plot, or for the percent cover of a plant (m^2/m^2), or for the concentration of a substance (ppm, ppb, or ml/ml). Yet its still useful to know what was measured in the numerator and denominator of the ratio. There has been lots of discussion on how to handle these cases, and its an ongoing issue we've tried to address more thoroughly in OBOE with our semantic treatment of units there, which separates the measured value from the entity being measured.  

The LTER has also developed a 'Best Practices' for assigning units that you may find useful (http://im.lternet.edu/sites/im.lternet.edu/files/LTERunitBestPractices_V13.pdf).",NA,NA,NA
12,20428606,cboettig,2013-07-03T16:41:04Z,2013-07-03T16:41:04Z,"Ah, great information.  thanks for the example.  For units that are not
just combinations of basic SI units I can just ignore the attributes like
parentSI and multiplierToSI?  Essentially that would just provide a name
and a description, but preferable to the character hack.

Meanwhile, for the standard units I was thinking it might be nice to allow
a reml user to configure their prefered base units, and provide a function
to automatically convert imported data tables to these units. Is there an
intelligent way to go about doing that?


Not to go too much on a currency tangent per se, since I'm mostly using
this as a way to think about addressing custom unit that isn't just a
rescaling.  I assume someone in the linked data world has already figured
this out.  e.g. http://sdmx.org/ provides European Central Bank data with
daily exchange rates to each currency.  Seems like it would be sufficient
to indicate a currency unit (e.g. dollars) and a date (1970, today,
etc)....

In the context that I frequently encounter it, this is more about
indicating that the numbers reflect monetary value than that they reflect
meaningful dollar amounts that can be compared to other dollar amounts.
 Like you suggest for other abstract units, it would probably be best to
present this in a de-dimensionalized form that still indicates the kind of
measurements involved, e.g. dollars over dollars.


On Tue, Jul 2, 2013 at 10:04 PM, Matt Jones <notifications@github.com>wrote:

> Here's an example of a custom unit definition:
>
>   <additionalMetadata>
>     <unitList>
>       <unit id=""numberPerSixtyMetersSquared"" multiplerToSI="".0166667"" name=""numberPerSixtyMetersSquared"" parentSI=""numberPerMeterSquared"" unitType=""arealDensity"">
>         <description>Number per sixty meters squared</description>
>       </unit>
>     </unitList>
>   </additionalMetadata>
>
> which comes from a PISCO metadata document that shows pretty nice and
> complete metadata (available from
> https://knb.ecoinformatics.org/knb/metacat/pisco_subtidal.151.2/xml).
>
> Creating custom units is easy when dealing with physical quantities like
> length or mass (as defined by NIST), as these all have base SI units and
> are easily convertible. However, it gets tricky with other things that
> appear to be quantities, but really are not (such as currency). There is no
> standard or base unit for currency, in that the value of every currency
> fluctuates over time (2009 dollars are different from 2010 dollars), and so
> currencies are not 'unit's in the traditional sense. You could define a
> UnitType for Currency, then define a base unit for that (say Dollars), and
> then define all other currencies as a conversion from the base Dollar, but
> of course the conversions would be wrong as the values fluctuated. You
> might have better luck defining a unit for a currency for a year, which are
> fairly well established (e.g., a '1970Dollar'), and then have each of the
> other annualized currency units convert from that, which would be more
> accurate, at least within th e bounds of an annualized mean value.
>
> Other types of units that you'll likely find problematic are for
> dimensionless values, such as counts, percentages, and ratios. For example,
> there is technically no unit for the count of animals in a plot, or for the
> percent cover of a plant (m^2/m^2), or for the concentration of a substance
> (ppm, ppb, or ml/ml). Yet its still useful to know what was measured in the
> numerator and denominator of the ratio. There has been lots of discussion
> on how to handle these cases, and its an ongoing issue we've tried to
> address more thoroughly in OBOE with our semantic treatment of units there,
> which separates the measured value from the entity being measured.
>
> The LTER has also developed a 'Best Practices' for assigning units that
> you may find useful (
> http://im.lternet.edu/sites/im.lternet.edu/files/LTERunitBestPractices_V13.pdf
> ).
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/12#issuecomment-20395868>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
12,20433365,mbjones,2013-07-03T17:54:28Z,2013-07-03T17:54:28Z,"To convert units simply requires the conversion factors, which can be parsed out of the standard unit dictionary and then from any custom units provided.  I think it might be useful to provide a separate unit conversion package that can load a list of units and their conversion factors, and then allow for straightforward conversion of any dataframe attribute that has an attached unit label.  Poking around, I'm not seeing an R package for labeling units and doing conversion, but I didn't look for long. See http://stackoverflow.com/questions/7214781/converting-units-in-r

One issue of unit conversion is that people should be careful when they do conversions, as there is often a change in precision when doing unit conversion, and you can lose some sampling basis information.  The custom unit example I gave above was one where the sampling design was in a sixty square meter plot, and so its important to know that for some types of analysis (e.g., species/area relationships). So, it would be good if reml would facilitate conversion, but not make automatically convert to standard units unless the user requests it.
",NA,NA,NA
12,20433967,cboettig,2013-07-03T18:02:56Z,2013-07-03T18:02:56Z,"Excellent points!  Will hold off on unit conversions for the time being.
 It might be worth hitting on these unit issues in the manuscript too.

Wrapping existing unit conversion tools make senses, though it seems the
information for the conversions is already encoded in the EML...  @duncantl
might have some thoughts on how we might do this elegantly..


On Wed, Jul 3, 2013 at 10:54 AM, Matt Jones <notifications@github.com>wrote:

> To convert units simply requires the conversion factors, which can be
> parsed out of the standard unit dictionary and then from any custom units
> provided. I think it might be useful to provide a separate unit conversion
> package that can load a list of units and their conversion factors, and
> then allow for straightforward conversion of any dataframe attribute that
> has an attached unit label. Poking around, I'm not seeing an R package for
> labeling units and doing conversion, but I didn't look for long. See
> http://stackoverflow.com/questions/7214781/converting-units-in-r
>
> One issue of unit conversion is that people should be careful when they do
> conversions, as there is often a change in precision when doing unit
> conversion, and you can lose some sampling basis information. The custom
> unit example I gave above was one where the sampling design was in a sixty
> square meter plot, and so its important to know that for some types of
> analysis (e.g., species/area relationships). So, it would be good if reml
> would facilitate conversion, but not make automatically convert to standard
> units unless the user requests it.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/12#issuecomment-20433365>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
22,20561015,cboettig,2013-07-06T21:04:38Z,2013-07-06T21:04:38Z,@mbjones Is this a standard thing to do?  Recommendation for how we encode it?  ,NA,NA,NA
22,20561398,mbjones,2013-07-06T21:28:48Z,2013-07-06T21:28:48Z,"I think the best place to add it is in /eml/dataset/methods/methodStep/software, and in the sibling description element describe the role that reml played in generating the metadata.  You might also want to add the citation in that subtree to REML.  EML is pretty flexible, so there are other options as well, but I think this is the most appropriate.",NA,NA,NA
31,20561631,mbjones,2013-07-06T21:45:26Z,2013-07-06T21:45:26Z,"I think this is a good idea, but its probably best to build the interactive feature on top of the functions that take the metadata vectors.  Seems like a phase 2 implementation item to me if we were prioritizing.",NA,NA,NA
31,20561793,cboettig,2013-07-06T21:55:53Z,2013-07-06T21:55:53Z,"Good strategy and good prioritization.  Will create a milestone for phase 2
related issues.


On Sat, Jul 6, 2013 at 2:45 PM, Matt Jones <notifications@github.com> wrote:

> I think this is a good idea, but its probably best to build the
> interactive feature on top of the functions that take the metadata vectors.
> Seems like a phase 2 implementation item to me if we were prioritizing.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/31#issuecomment-20561631>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
22,20564935,cboettig,2013-07-07T03:13:32Z,2013-07-07T03:13:32Z,"Excellent.  Since this will create a software node, we may as well write `eml_software` first, and `eml_R_software`, then we can create the software node with `eml_software(""reml"")`, see #32

@mbjones  Um, I'm not spotting the documentation for how a sibling description element should be constructed?",NA,NA,NA
22,20565197,mbjones,2013-07-07T03:46:05Z,2013-07-07T03:46:05Z,"Schema diagram is here:
    http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-methods.png
I'm referring to the three sibling elements:
`/eml/dataset/methods/methodStep/software`
`/eml/dataset/methods/methodStep/description`
`/eml/dataset/methods/methodStep/citation`",NA,NA,NA
22,20572109,cboettig,2013-07-07T15:11:41Z,2013-07-07T15:11:41Z,"@mbjones Thanks, this makes sense.  First two are done, but trying to wrap my head around EML `citation` objects. 

I assume we cite software as `<generic>`?  

I'm a bit confused why I don't see things like `title` and `author` listed under fields such as Article: http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-literature.html#Article, I guess that's because we have `<title>` and `<creator>` defined elsewhere?  

I see that the citation object is built around the endnote format.  R's citation tools (and lots of other tools) can return citations in bibtex format; I'm wondering if there's anything clever that can be done here in place of just mapping each term by hand...",NA,NA,NA
22,20577230,mbjones,2013-07-07T20:45:55Z,2013-07-07T21:01:36Z,"@cboettig Yeah, software is probably best listed as `<generic>`.  At the time we did this, EndNote was massively predominant, and the likes of Mendeley and Zotero were far off on the horizon yet.  In retrospect I wish I had known more about Bibtex, as it seems to have survived the test of time, but in 1998-2000 there simply weren't any XML-based citation schemas available.  So, we ported endnote.  There must be a decent mapping to convert  to more modern standards like Bibtex or Bibo, but I haven't looked carefully for it. I think Dryad uses a subset of Bibo, but they had to define their own xml schema for that too, as Bibo doesn't have a schema doc.

Regarding why `<title>` and `<creator>` are not shown in the spec, its because they are part of another module that is included by reference, specifically `res:ResourceGroup`.  In XSD, you can include schema portions by reference to a group of elements, and as we need the bibliographic fields that describe resources in many places, we created `res:ResourceGroup` to be the common group for these fields.  So, near the top of the `CitationType` definition (http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-literature.html#CitationType), you will see this:

> A sequence of (
>     res:ResourceGroup	 	 
>     contact	optional	unbounded
>     ...
> }

which is a group inclusion.  Follow the link to res:ResourceGroup and you'll see all of the fields.  If you look at the diagram for eml-literature, you'll see that those fields in the group have been included by parsing the XSD (http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-literature.png).",NA,NA,NA
9,20581512,cboettig,2013-07-08T01:19:07Z,2013-07-08T01:19:07Z,"@mbjones What is the best way to connect information in a coverage node to that in a attributeList? e.g. if I have a column header that is a species, or I have a column consisting of different species names, it seems logical that I would want to tie this information to the taxanomic coverage section of the node.  I gather I can do this with `id` attribute and a `<reference>` node?  Can you give me a minimal example?  

Also, does each EML file have a single coverage node?  Does it always appear as a child of a dataset (or equivalent module?) or does it appear in deeper levels (e.g. at the attributeList level, etc).

I imagine the same would naturally apply to cases where time or geographic coordinates appear in attribute elements as well.  Therefore I'm also trying to imagine the best user interface to address this -- e.g. declaring a column to have species names automatically fills out the coverage for taxanomic coverage, etc...",NA,NA,NA
22,20583397,cboettig,2013-07-08T02:56:33Z,2013-07-08T02:56:33Z,"Sounds good.  Thanks for explaining the group inclusion with
res:ResourceGroup, clearly I'm brand new to XSD so these pointers help me
get up to speed.

Yeah, Dryad uses the custom:
https://raw.github.com/datadryad/dryad-repo/dryad-master/dspace/modules/xmlui/src/main/webapp/themes/Dryad/meta/schema/v3.1/bibo.xsd


I was hoping Shotton's group might be persuaded to make a xsd file for
fabio, an alternative to bibo with some advantages, including be OWL2 DL
instead of OWL full, see
http://semanticpublishing.wordpress.com/2011/06/29/comparison-of-bibo-and-fabio/
(I
have only a fuzzy understanding of the the differences, but probably means
more to you).  A moot point since they don't seem to have an XSD file
either at this time, and even if they did this would mean changing the EML
schema?  Or is it trivial to extend if you had such bibliographic schema?






On Sun, Jul 7, 2013 at 4:45 PM, Matt Jones <notifications@github.com> wrote:

> @cboettig <https://github.com/cboettig> Yeah, software is probably best
> listed as <generic>. At the time we did this, EndNote was massively
> predominant, and the likes of Mendeley and Zotero were far off on the
> horizon yet. In retrospect I wish I had known more about Bibtex, as it
> seems to have survived the test of time, but in 1998-2000 there simply
> weren't any XML-based citation schemas available. So, we ported endnote.
> There must be a decent mapping to convert to more modern standards like
> Bibtex or Bibo, but I haven't looked carefully for it. I think Dryad uses a
> subset of Bibo, but they had to define their own xml schema for that too,
> as Bibo doesn't have a schema doc.
>
> Regarding why <title> and <creator> are not shown in the spec, its
> because they are part of another module that is included by reference,
> specifically res:ResourceGroup. In XSD, you can include schema portions
> by reference to a group of elements, and as we need the bibliographic
> fields that describe resources in many places, we created 'res:ResourceGroupto
> be the common group for these fields. So, near the top of theCitationType`
> definition (
> http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-literature.html#CitationType),
> you will see this:
>
> A sequence of (
> res:ResourceGroup
>
> contact optional unbounded
> ...
> }
>
> which is a group inclusion. Follow the link to res:ResourceGroup and
> you'll see all of the fields. If you look at the diagram for
> eml-literature, you'll see that those fields in the group have been
> included by parsing the XSD (
> http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-literature.png).
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/22#issuecomment-20577230>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
22,20601265,mbjones,2013-07-08T12:15:49Z,2013-07-08T12:15:49Z,"Definitely not trivial to extend -- many groups around the world use the EML schema and have written software to generate it and consume it -- any schema changes, especially backwards incompatible ones, have a ripple effect on the community.  So, we try to avoid changes that break existing EML documents.  Adding something as an optional new field is more acceptable and can generally get approved by the EML community fairly quickly.",NA,NA,NA
9,20648148,mbjones,2013-07-09T01:31:27Z,2013-07-09T01:31:27Z,"@cboettig That's correct -- you give an id to the attribute you want to describe, then put a coverage element in the additionalMetadata section,  and use the `<describes>` element in additionalMetadata to point at the id of the attribute.  Although this ability to describe coverage for entities and attributes was designed into EML, we have not really seen it used, and most software that understands EML will probably not pay any attention to the linkage.  Most people simply provide overall coverage at the `<dataset>` level.

You can repeat the items under the coverage tree if you have discontinuous coverages, such as a temporal coverage from 1990-1993 and 1998-2000, or disjoint spatial areas.  Many groups use this to provide bounding boxes for discrete sampling areas that are not contiguous.
",NA,NA,NA
9,20649586,cboettig,2013-07-09T02:20:33Z,2013-07-09T02:20:54Z,"@mbjones Thanks, that implementation makes sense.  Just to make sure I understand, after assigning an id to the `<attribute>` I create an additional metadata element like so:

```xml
<additionalMetadata>
  <describes>1838f0b53178056585f5bda86818ca30</describes>
  <metadata>
    <coverage>
      ...
```

How about if the column values were species names (instead of the column headers), would I denote them as a nominal/enumerated domain and I would put the attribute on the ""definition"" element? e.g.

```xml
        <attribute id=""1838f0b53178056585f5bda86818ca30"">
          <attributeName>Species</attributeName>
          <attributeDefinition>Species name</attributeDefinition>
          <measurementScale>
            <nominal>
              <nonNumericDomain>
                <enumeratedDomain>
                  <codeDefinition>
                    <code>coho</code>
                    <definition>Oncorhynchus kisutch</definition>
```
and so on for all species in the attribute list?  Or would I do better to document at the level of the attribute and list all species?  (The latter seems more concise, but also seems to leave the mapping between the species names used in the columns and the semantically precise species names from the columns ambiguous?)  



It's a good point that this all might be rather academic if existing software ignores it. It would be good to keep this in mind at least.  Is there a list of software that consumes EML?  I've only come across metacat and morpho.  Harvard Forest tells me they generate their EML using a combination of generic XML editor Oxygen (for stuff above the `<entity>` level) and Morpho for the attributes.  It would be good to generate EML with an eye to what will make it most powerful for existing platforms that consume it (while still supporting the full power of the schema when desired).  

We could generate the `<additionalMetadata><describes>` nodes for every species in a column automatically, but perhaps that's not the best way to go.

In light of the 'standard usage', perhaps it would be better to handle this kind of thing using the kind of semantic annotation approaches discussed in #5 (also see issue #13)?  e.g. I've always been a bit confused by the division -- for some terms EML appears to give us semantically precise vocabulary for certain things: e.g. the taxonomic, geospatial, and temporal terms in ""coverage"", as well as the units.  On the other hand, perhaps the EML notion of ""gram"" is less powerful the OBOE notion http://ecoinformatics.org/oboe/oboe.1.0/oboe-standards.owl#Gram because the latter is part of an OWL ontology that enables further reasoning?  

Perhaps we wait to write a separate package provides such semantic annotation automatically when you declare column ""Species"" has species names, etc.  




",NA,NA,NA
7,21266963,cboettig,2013-07-19T18:10:59Z,2013-07-19T18:10:59Z,@duncantl added code to validate EML from R using the online Java tools from @mbjones in commit 1722a4be7297c8905179f945385092b0ebaedc4d,NA,NA,NA
7,21267256,cboettig,2013-07-19T18:15:25Z,2013-07-19T18:39:19Z,"Philosophical point: Validation is really a concern for us / developers, not for the end users. Our programmatically generated EML should always be valid.  Meanwhile, if we read in EML that is not valid, what are we going to do, just throw an error?  Better to make the most of it.  

Note: we can validate in R using the XML package (via `libxml2`), using the `xmlSchemaValidate()` function, though if I understand correctly, the code added above should also perform the additional validation that @mbjones describes.

- [ ] Add a unit test that includes validation",NA,NA,NA
7,21275433,duncantl,2013-07-19T20:30:06Z,2013-07-19T20:30:06Z,"There are two examples in a if(FALSE) {} block at the top of the file containing the functions.  These can be used for a unit test.
Also, we should either raise an error or put a class on the result of processValidateResponse if either of the tests were not passed.",NA,NA,NA
7,21275490,duncantl,2013-07-19T20:30:45Z,2013-07-19T20:30:45Z,(Not closed at all!),NA,NA,NA
7,21275819,duncantl,2013-07-19T20:36:23Z,2013-07-19T20:37:20Z,"The recursion problem in the XMLSchema package where a schema imports another schema which imports the first one is working now.  Use inline = FALSE in the call to readSchema.  
```R
 library(XMLSchema)
 x = readSchema(""~/Downloads/eml-2.1.1/eml.xsd"", inline = FALSE)
```
This cures the parsing and processing of the types. It remains to be seen if it breaks anything else.  And of course there will still be issues with the actual type descriptions it has created. ",NA,NA,NA
7,21295283,cboettig,2013-07-20T15:32:02Z,2013-07-20T15:33:10Z,"@duncantl Great, `readSchema` works for me.  Hitting an error when I then try `defineClasses`

```
Note: method with signature 'RestrictedStringDefinition#list' chosen for function 'resolve',
 target signature 'RestrictedStringDefinition#SchemaCollection'.
 ""SchemaType#SchemaCollection"" would also be valid
Error in .getClassFromCache(Class, where) : 
  attempt to use zero-length variable name
```



Also, `xmlSchemaValidate()` seems unhappy about my eml files, even though they validate fine with your new `eml_validate` function... 

```
> xmlSchemaValidate(""inst/xsd/eml.xsd"", ""inst/doc/my_eml_data.xml"")
$status
[1] 1845

$errors
[[1]]
$msg
[1] ""Element '{eml://ecoinformatics.org/eml-2.1.0}eml': No matching global declaration available for the validation root.\n""

$code
XML_SCHEMAV_CVC_ELT_1 
                 1845 

$domain
XML_FROM_SCHEMASV 
               17 

$line
[1] 2

$col
[1] 0

$level
XML_ERR_ERROR 
            2 

$filename
[1] ""inst/doc/my_eml_data.xml""

attr(,""class"")
[1] ""XMLError""

attr(,""class"")
[1] ""XMLStructuredErrorList""

attr(,""class"")
[1] ""XMLSchemaValidationResults""
```


",NA,NA,NA
22,21295487,cboettig,2013-07-20T15:44:53Z,2013-07-20T15:44:53Z,"@mbjones Okay, I failed to write this `methodsStep` (which states that `reml` created the EML) correctly:

My R code creates a nod that looks like this:

```xml
<methods>
  <methodsStep>
    <software>
      <license>CC0</license>
      <version>0.0-1</version>
      <implementation>
        <distribution>
          <online>
            <url>https://github.com/ropensci/reml</url>
          </online>
        </distribution>
      </implementation>
    </software>
    <description>An R package for reading, writing, integrating and publishing data
    using the Ecological Metadata Language (EML) format.</description>
  </methodsStep>
</methods> 
```

And the validator complains: 

```
[1] ""cvc-complex-type.2.4.a: Invalid content starting with element 'methodsStep'. The content must match '(((\""\"":methodStep){1-UNBOUNDED},(\""\"":sampling){0-1}),(\""\"":qualityControl){0-UNBOUNDED}){1-UNBOUNDED}'.""
```

Um, does this mean I need `sampling` and `qualityControl` in a methods step?  I'm confused.



",NA,NA,NA
7,21295497,duncantl,2013-07-20T15:45:52Z,2013-07-20T15:45:52Z,"I am looking into to the defineClasses() and defClass() issues.

As for the validation error, I suspect the error message is correct although I can't tell what schema you are using. I imagine you are using the eml-2.1.1 while  the my_eml_data.xml is using the namespace eml... 2.1.0",NA,NA,NA
22,21295658,karthik,2013-07-20T15:56:41Z,2013-07-20T15:56:41Z,"> My R code creates a nod that looks like this

What function generated that metadata?",NA,NA,NA
22,21295786,cboettig,2013-07-20T16:03:09Z,2013-07-20T16:03:09Z,"eml_write, which called eml_dataset, which calls

```
  methodsStep <- newXMLNode(""methodsStep"", parent = methods_node)
  addChildren(methodsStep, eml_R_software(""reml""))
  addChildren(methodsStep,
              newXMLNode(""description"",
                         packageDescription(""reml"", fields=""Description"")))
```

which uses eml_R_software creates that node... (using eml_software)





On Sat, Jul 20, 2013 at 8:56 AM, Karthik Ram <notifications@github.com>wrote:

> My R code creates a nod that looks like this
>
> What function generated that metadata?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/22#issuecomment-21295658>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
22,21296051,mbjones,2013-07-20T16:19:28Z,2013-07-20T16:19:28Z,"@cboettig -- Just pushed a fix -- ""methodsStep"" should have been ""methodStep"".

",NA,NA,NA
22,21296253,cboettig,2013-07-20T16:31:41Z,2013-07-20T16:31:41Z,"thanks!


On Sat, Jul 20, 2013 at 9:19 AM, Matt Jones <notifications@github.com>wrote:

> @cboettig <https://github.com/cboettig> -- Just pushed a fix --
> ""methodsStep"" should have been ""methodStep"".
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/22#issuecomment-21296051>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
33,21296316,mbjones,2013-07-20T16:35:01Z,2013-07-20T16:35:01Z,"@cboettig -- yeah, for searching across documents, I think we really should be using another library like the R dataone library that let's us do full SOLR queries against EML and other metadata standards using any repository that is DataONE enabled.  Querying across documents requires indexing those documents as a collection, and that seems out of scope for reml.  The dataone library might be used like this:

```
library(dataone)

# Initialize a client to interact with DataONE
cli <- D1Client()

# search for data packages and only request versions that are not obsolete
results <- d1SolrQuery(cli, list(q=""increase experiments -obsoletedBy:*"",fl=""identifier,title,author,documents,resourceMap""))
sr <- xmlParse(results)
sr
```

There are hundreds of thousands of EML documents and FGDC documents accessible via the DataONE search index.",NA,NA,NA
22,21296547,cboettig,2013-07-20T16:50:41Z,2013-07-20T16:50:41Z,"oh, validator still unhappy: 

```
[1] ""cvc-complex-type.2.4.a: Invalid content starting with element 'software'. The content must match '((((((\""\"":description),((\""\"":citation)|(\""\"":protocol)){0-UNBOUNDED}),(\""\"":instrumentation){0-UNBOUNDED}),(\""\"":software){0-UNBOUNDED}),(\""\"":subStep){0-UNBOUNDED}),(\""\"":dataSource){0-UNBOUNDED})'.""
```",NA,NA,NA
22,21296656,mbjones,2013-07-20T16:57:55Z,2013-07-20T16:57:55Z,"What does the methodStep snippet look like now?  The error message is just relating the schema rules, which are somewhat easier to grok in this image:

http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-methods.png


",NA,NA,NA
22,21296710,cboettig,2013-07-20T17:01:08Z,2013-07-20T17:01:08Z,"now it is: 


```xml
<methods>
  <methodStep>
    <software>
      <license>CC0</license>
      <version>0.0-1</version>
      <implementation>
        <distribution>
          <online>
            <url>https://github.com/ropensci/reml</url>
          </online>
        </distribution>
      </implementation>
    </software>
    <description>An R package for reading, writing, integrating and publishing data
    using the Ecological Metadata Language (EML) format.</description>
  </methodStep>
</methods> 
```",NA,NA,NA
36,21296722,mbjones,2013-07-20T17:01:53Z,2013-07-20T17:01:53Z,"You should be using eml-2.1.1, but note that there is a lot of content in older versions of EML (2.0.0, 2.0.1, 2.1.0), and that each of them is more or less an extension of the previous versions.  There are some incompatibilities introduced at EML 2.1.0, but you can mostly ignore them as they have to do with how access rules are interpreted, which I think you largely can ignore.  So yeah, produce 2.1.1, but consume any of the versions would be best IMO.  In Morpho, when we find an old version, we forward convert it to the new namespace before allowing people to make changes, so the process of updating a metadata record has the benefit of moving it to the most recent schema version.  I think a similar strategy would be good for `reml`.",NA,NA,NA
22,21296884,mbjones,2013-07-20T17:12:46Z,2013-07-20T17:13:33Z,"Elements need to be in a different order to be valid. In addition, you are missing required fields from the software module, including title, and creator. See:
  http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-software.png

Something like this might validate (I didn't try it, just tried to follow the schema):

```xml
<methods>
  <methodStep>
    <description>An R package for reading, writing, integrating and publishing data
    using the Ecological Metadata Language (EML) format.</description>
    <software>
      <title>reml</title>
      <creator>
            <individualName>
                  <givenName>Carl</givenName><surName>Boettiger</surName>
            </individualName>
      </creator>
      <creator>
            <individualName>
                  <givenName>Karthik</givenName><surName>Ram</surName>
            </individualName>
      </creator>
      <implementation>
        <distribution>
          <online>
            <url>https://github.com/ropensci/reml</url>
          </online>
        </distribution>
      </implementation>
      <license>CC0</license>
      <version>0.0-1</version>
    </software>
  </methodStep>
</methods> 
```",NA,NA,NA
22,21304928,cboettig,2013-07-21T04:20:49Z,2013-07-21T04:20:49Z,"Thanks for clarifying, sorry I haven't got the hang of reading the spec still.  I keep forgetting `resourceGroup` and forgetting to pay attention to node ordering.  Once `XMLSchema` package is fully running, we will be able to automate the creation of corresponding S4 objects to the schema, so it will just be a matter of writing coercion methods (e.g. like `eml_R_software` that can extract information from native R formats (e.g. the R package DESCRIPTION) into the S4 object, which will reduce errors like this!

",NA,NA,NA
22,21304956,karthik,2013-07-21T04:23:29Z,2013-07-21T04:23:29Z,I don't have a complete understanding of the spec either. @mbjones can you suggest some readings that will allow me to get up to speed?,NA,NA,NA
32,21304994,cboettig,2013-07-21T04:28:39Z,2013-07-21T04:28:39Z,"The minimal version of software nodes is now written (`eml_software`), but really needs to be extended to support the full software node: http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-software.png

At least the `eml_R_software` should be re-written as a coercion method (e.g. see `as.eml_person`) and extract and map all package description fields (well, except fields that don't map, like the collate list).  So far we map title (actually we use packagename), authors, version, url, and license.  ",NA,NA,NA
32,21305009,karthik,2013-07-21T04:30:19Z,2013-07-21T04:30:30Z,What does `implementation` refer to (the second check box in the original issue at the top)?,NA,NA,NA
22,21305033,cboettig,2013-07-21T04:34:44Z,2013-07-21T04:34:44Z,"@karthikram The pngs are pretty handy once you get the hang of them, e.g.
http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-software.png   I
think ordering of nodes matters, top down as shown.  dashed lines are
optional nodes. I don't get the symbols with boxes on lines (either linear
ones or stacked).  And of course vector graphic would be eaiser to read
without squinting...

Otherwise I find the descriptions in the 'normative technical documents'
reasonably readable...
http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-software.html

As I commented above, a working XMLSchema will be a big help in automating
a lot of this, and letting us focus on the UI.  But writing a few nodes out
by hand is pretty instructive.


On Sat, Jul 20, 2013 at 9:23 PM, Karthik Ram <notifications@github.com>wrote:

> I don't have a complete understanding of the spec either. @mbjones<https://github.com/mbjones>can you suggest some readings that will allow me to get up to speed?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/22#issuecomment-21304956>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
32,21305104,cboettig,2013-07-21T04:43:36Z,2013-07-21T04:43:36Z,"Implementation is a required child node of a software node, see: http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-software.html 

The description there explains: ""Implementation describes the hardware, operating system resources a package runs on...."" 

The only required child node of an implementation node is the `distribution` node, which can be a variety of things (click through the links -- yes, you can specify carved on stone tablets)  

You'll see my implementation only provides for online distribution, in which case we're required to simply give a url.  e.g. the implementation node I generate for reml looks like:

```xml
         <implementation>
            <distribution>
              <online>
                <url>https://github.com/ropensci/reml</url>
              </online>
            </distribution>
          </implementation>
```",NA,NA,NA
33,21305186,cboettig,2013-07-21T04:56:09Z,2013-07-21T04:56:09Z,"@mbjones This sounds great.  

I think one useful exercise in this spirit would simply be to give a snapshot of what is available in EML across, say, all of DataONE: how many EML files, covering how many publications, how many taxa (or distribution of coverage across taxa), how many attributes, rows of data; what fraction are from LTER sites, fraction that are EML vs FGDC, number of EML files by year, etc.  I think many researchers might be surprised to realize how much is available, which could help motivate further adoption.  ",NA,NA,NA
33,21305256,karthik,2013-07-21T05:07:52Z,2013-07-21T05:07:52Z,This would be particularly important to emphasize for the manuscript to really drive home the point. Ecologist (as with most scientists) are reluctant to invest time in a tool like this without knowing what their time investment is really worth.,NA,NA,NA
22,21310239,mbjones,2013-07-21T13:43:27Z,2013-07-21T13:43:27Z,"The diagrams are the best way to understand the spec, although note that the diagrams do not show XML attributes.  This was a shortcoming of the software used to generate the schemas.  There is a nice explanation of the diagrams here: http://www.diversitycampus.net/projects/tdwg-sdd/minutes/SchemaDocu/SchemaDesignElements.html

I'm not sure what you are referring to with the 'symbols with boxes on lines' comment. Sorry. Getting up to speed on the EML spec (or any other) just takes time -- the EML schema itself is the most useful.  We wrote a paper describing use of the spec a while ago targeted at ecologists, but it doesn't get into the technical details of the spec -- see Fegraus et al. 2005: http://www.mendeley.com/download/public/1825821/4445891665/2d411d3da6a51fecf34b4f0061a68d250c486eaa/dl.pdf

These diagrams were built with XML Spy, an XML editor that can display XML Schema.  There are several others that will produce diagrams.  If you are trying to understand the EML schema, it can be useful to open the eml.xsd schema in one of these editors so that you can explore the schema tree more dynamically -- the images are just static screenshots of the diagrams for certain subtrees.",NA,NA,NA
35,21318627,cboettig,2013-07-21T22:48:12Z,2013-07-21T22:48:12Z,"@mbjones Great question.  @duncantl might have better insight on this, but:

My intuition is that the `dataone` package would list `reml` in DESCRIPTION Suggests list, along with other such schema-specific parser packages.  Likewise, `reml` would list `dataone` in it's Suggests list, along with any other packages that provide publishing platforms (e.g. rfigshare).  

I think this avoids a hard dependency and sidesteps the question of who depends on who (e.g. which package must be installed first).  When we have a function in `reml` that calls the `dataone` API for downloading / uploading, the user could be prompted to install the `dataone` package.  Likewise a user of the `dataone` package would always be able to explore the XML with basic xpath tools as you show above, but could be prompted to load the `reml` package for advanced methods when handling EML files.  

The looser dependency also means that if you don't care about a certain feature (say, publishing to figshare) then you don't need to install the rfigshare package just to use `reml`.  Make sense?  @duncantl two packages can suggest each-other in this way, right?

",NA,NA,NA
7,21318676,cboettig,2013-07-21T22:51:55Z,2013-07-21T22:51:55Z,"Indeed!  We've moved to reml to writing 2.1.1 by default, #36 and now our example generated EML passes `xmlSchemaValidate()` against the 2.1.1 `eml.xsd`, as expected.  

Sounds good -- keep us posted on `defineClasses()`",NA,NA,NA
30,21318773,cboettig,2013-07-21T22:59:07Z,2013-07-21T22:59:07Z,"Good question, I need to get up to speed on the `dataone` R package still; but I think the answer to this is ""yes"", using the approach discussed in issue #35 ",NA,NA,NA
8,21319055,cboettig,2013-07-21T23:17:22Z,2013-07-21T23:17:22Z,"I keep thinking about this integration question and trying to wrap my head around the different kinds of semantics involved here.  Let's see if I got this right:  

We have a vocabulary defined by the EML Schema, which can give semantic meaning to things (e.g. we have a precise notion of the term ""genus"", the units ""gram"" and ""kilogram"", etc), but it is not an ontology (e.g. OWL), so we don't have access to the richer reasoning tools and infrastructure thereof.  

We can use the schema definitions (e.g. 'coverage' nodes) to annotate attributes using `id` and `<reference>` as described in #9, but this is not commonly done.  This would also be weaker than providing ontological definitions of terms, ultimately needed to do the synthesis described at the top of this issue.

So instead, we can annotate EML with the approach @mbjones describes in #5, in which an external XML file provides ontological descriptions of the nodes, as illustrated in [some examples](https://code.ecoinformatics.org/code/semtools/trunk/dev/sms/examples).  It seems like this is the way forward, given the current EML schema.  

@mbjones Are you familiar with the NeXML standard, e.g. as described by [Vos _et al._ 2012](http://dx.doi.org10.1093/sysbio/sys025 ""NeXML: Rich, Extensible, and Verifiable Representation of Comparative Data and Metadata."") 

> All elements in a NeXML document—branches and nodes in trees, cells in a matrix, OTUs, and so on—can be identified and given annotations using a generalized system that allows for simple values as well as complex, structured information such as geo-references, taxon concepts, or character-state descriptions. Moreover, data elements can be declared as instances of a class defined in an ontology, making the semantics of the data themselves computable.

@mbjones It seems like they have a more direct way of accomplishing this goal; e.g. a tighter correspondence between Schema's vocabulary and available ontologies?  Is this at all instructive for us?  There are a lot of shared objectives here -- e.g .attaching geo-references and taxon concepts to nodes -- it seems like a common approach here would be good.  Perhaps your working groups are already talking to each other?
",NA,NA,NA
8,21346552,mbjones,2013-07-22T14:13:35Z,2013-07-22T14:13:56Z,"I have a passing familiarity with NexML, having had to deal with it in Kepler, but what you describe sounds useful.  The key in all of these is to have a solid, well-defined identifier for anything that you want to apply an annotation to.  The EML id attribute is one of these, and is how we implemented the semtools annotations that I described in #5.  Although I said that people don't often apply geospatial and taxon constraints to particular attributes, that is how  we intended for the system to work.  The additionalMetadata `<describes>` element provides a general purpose way of annotating any subtree in an EML document.  I think this is parallel to what NexML provides though their `<Annotated>`  complex type, with its `<about>` attribute pointing at a URI. So, the reason for us to do annotations separately in Semtools is exactly this -- there are many metadata standards, and each has its own way describing entities and attributes.  The external annotation schema that I cited in #5 provides a mechanism to link annotations to ontologies that is flexible enough to apply to multiple different metadata schemas.  It could be inlined inside of an EML additionalMetadata element for ease of use, or it could stand alone as its own independent document. My impression is that NexML assumes these will always be external, as the `<Annotated>` element uses the about URI attribute for the pointer.  

We have not talked with the NexML folks, but sounds like we should.  Do you know that there will be a concentrated emphasis on this via a biodiversity semantics workshop at TDWG this year that Mark Schildhauer is organizing?  We also have some work on this coming up via our Semtools project, so hopefully we can all come to an acceptable shared approach.
",NA,NA,NA
8,21457327,cboettig,2013-07-24T01:00:14Z,2013-07-24T01:00:14Z,"@mbjones From @rvosa I understand that NeXML provides this kind of annotation in  `<meta>` nodes as child nodes to the attributes, e.g. here's an excerpt from a list of `<node>` elements where one has such an annotationL

```xml
                        <node id=""tree2n2"" label=""n2"" otu=""t1""/>
			<node id=""tree2n3"" label=""n3""/>
			<node id=""tree2n4"" about=""#tree2n4"" label=""n4"">
			    <meta 
			    	id=""tree2dict1"" 
			        property=""cdao:has_tag"" 
			        content=""true"" 
			        xsi:type=""nex:LiteralMeta""
			        datatype=""xsd:boolean""/>
			</node>
			<node id=""tree2n5"" label=""n5"" otu=""t3""/>
			<node id=""tree2n6"" label=""n6"" otu=""t2""/>
```
(Or see [richer examples here](https://github.com/shumelchyk/RNeXML/blob/master/tests/examples/treebase-record.xml))

One clever thing about this is that the `<meta>` nodes use RDFa syntax, so that the data can be extracted by any generic RDFa tool, and can leverage ontologies directly.  The external Semtools annotation examples you linked (like [this one](https://code.ecoinformatics.org/code/semtools/trunk/dev/sms/examples/er-2008-ex1-annot.xml)) look like powerful way to go about this.  Curious if they could exploit the same RDFa trick?  

I see there's already a beta schema for the Semtools annotation (`sms-semannot.xsd`); perhaps you could point me to the documentation for this?   I guess we can already generate the annotations for some attributes programmatically, e.g. the standardUnits).  ",NA,NA,NA
8,21467672,rvosa,2013-07-24T07:16:01Z,2013-07-24T07:16:01Z,"Hi Matt, Carl,

nice to be in touch about this, and interesting to see how you guys are dealing with the same challenges. To give a more complete, applied example of the RDFa annotations, have a look at this TreeBASE study dump: https://github.com/rvosa/supertreebase/blob/master/data/treebase/S100.xml

(Carl, this directory holds all of TreeBASE, as you asked.)

What we're trying to do is embed the metadata about the study (publication data, GUIDs for taxa) inside the data file so that it can be extracted with generic tools. To wit, here are the triples that are thus generated:

http://www.w3.org/2012/pyRdfa/extract?uri=https%3A%2F%2Fraw.github.com%2Frvosa%2Fsupertreebase%2Fmaster%2Fdata%2Ftreebase%2FS100.xml&format=turtle&rdfagraph=output&vocab_expansion=false&rdfa_lite=false&embedded_rdf=true&space_preserve=true&vocab_cache=true&vocab_cache_report=false&vocab_cache_refresh=false

Cheers,

Rutger",NA,NA,NA
8,21507327,cboettig,2013-07-24T18:52:51Z,2013-07-24T18:52:51Z,"@mbjones Guess I should learn to use a computer.  I see there's already a lot of information about the semtools approach here:  https://code.ecoinformatics.org/code/semtools/trunk/dev/sms/README.txt

I suppose we can follow a similar approach to morpho of providing a semtools R package that could be used as a 'plugin' with reml.  ",NA,NA,NA
5,21573824,cboettig,2013-07-25T18:19:00Z,2013-07-25T18:19:18Z,"Okay, I'm now thinking that adding RDF to the `additionalMetadata` section and using `describes` references (as discussed above and more in #9) is the best way to go about adding semantic definitions, rather than the relying on the external semtools schema for this (as we considered in issue #8).  When asked about using the semtools schema, Ben makes the case for this approach quite eloquently: 

> While we did use the sms annotation schema in the Semtools project, I can't say that I think you should also use it. I'd be more interested in seeing a ""purer"" semantic approach to storing those types of annotations (e.g., ""this column of this data table is measured in Gram""). Basically, these are all RDF triples. I'm not sure if Shawn Bowers - the one who first drew up the sms annotation schema - is still advocating its use, but it was experimental even in the heyday of Semtools. One of the major issues with this annotation approach is that it is another independent file that describes the EML file. This gets annoying when you try to have tools work with the many files. You could potentially embed the annotation - or any XML - in EML's additionalMetadata section.

I think the really clever thing here is that the `metadata` tag is flexible enough for us to just add RDF directly, as Ben illustrates like this:

```xml
<eml>
…
<dataTable id=""http://some.namespace#myUniqueEntityId1"">
        <attribute id=""http://some.namespace#myUniqueAttributeId1""/>
        <attribute id=""http://some.namespace#myUniqueAttributeId2""/>
</dataTable>
…
<additionalMetadata>
        <describes>http://some.namespace#myUniqueAttributeId1</describes>
        <metadata>
                <!-- RDF stuff here that annotates http://some.namespace#myUniqueAttributeId1 -->
                <rdf:RDF
                        xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#""
                        xmlns:o=""http:/oboe-core#"">
                        <rdf:Description rdf:about=""http://some.namespace#myUniqueAttributeId1"">
                                <o:entity>Air</o:entity>
                                <o:characteristic>Temperature</o:characteristic>
                                <cd:unit>Celsius</cd:unit>
                        </rdf:Description>
                </rdf:RDF>
        </metadata>
</additionalMetadata>
<additionalMetadata>
        <describes>http://some.namespace#myUniqueAttributeId2</describes>
        <metadata>
                <!-- RDF stuff here that annotates http://some.namespace#myUniqueAttributeId2 -->
        </metadata>
</additionalMetadata>
</eml>

```

A few questions: 

- [ ] Would it be preferable to use RDFa (like NeXML does) instead of RDF, which would presumably allow us to extract the RDF data into a pure RDF file using standard tools (e.g. http://www.w3.org/2012/pyRdfa/#distill_by_uri)?  
Or is there a good reason to prefer embedded RDF, as above?

- [ ] Ben points out that one option would be, rather than a separate `additionalMetadata` for each attribute, we could have one `additionalMetadata` referencing the root EML id in `describes`, since the `rdf:Description` node points to the attribute already anyhow.  Any reason to prefer one approach over the other?  

- [ ] Presumably we could generate this automatically for standard units.  We could also generate this automatically for species names, along with the adding the appropriate EML version of `coverage`?  Or would it be better to have  a single `coverage` node with all the taxanomic coverage, etc?  (Basically a question of how other tools are using the coverage nodes.  Since it sounds like they are just using them at aggregate level to identify EML files containing certain coverage, rather than at the attribute level to give semantic meaning to columns, maybe there is no point in doing the latter?  This issue already touched upon in #9 , though undecided.)

- [ ] What namespace do we put the attribute ids under?  (both in the `<rdf:Description rdf:about=""http://some.namespace#myUniqueAttributeId1"">` and in the `describes` nodes?)  

- [ ] Obviously we simply don't have ontological meaning for lots of terms.  For a first pass, I imagine `reml` adding this annotation 'silently' on the above cases where we can probably automatically interpret (or infer from the schema) the semantic meaning.  The harder challenge is thinking how a user might specify additional semantic annotations of elements without expert knowledge of the schema, the relevant ontology, and lots of hand-crafting.  Maybe that's an impossible problem.  ",NA,NA,NA
22,21576291,cboettig,2013-07-25T18:55:17Z,2013-07-25T18:55:17Z,"reml-generated methods node now exists, so I think we can close this issue. `software` and `citation` nodes will be written/improved upon once we get this S4 thing nailed down, and then we can revisit this.  ",NA,NA,NA
32,21576440,cboettig,2013-07-25T18:57:14Z,2013-07-25T18:57:14Z,"As this now provides a working if not fully featured implementation, will close this issue.  A better / more thorough implementation should probably use the S4 approach described in #38, (e.g. will come more-or-less for free once `XMLSchema` methods are ready).  ",NA,NA,NA
38,21576537,cboettig,2013-07-25T18:58:55Z,2013-07-25T18:58:55Z,This issue to be addressed once we close #34 (e.g. can generate the class definitions).,NA,NA,NA
13,21585561,cboettig,2013-07-25T21:20:43Z,2013-07-25T21:20:43Z,"This issue is really several issues, most of which are being addressed elsewhere (so I am closing this issue as redundant)

### Generating semantics (write)

This is being addressed conceptually in #5 and in implementation in #9.  We have to decide in what format and what location the semantic data will appear (e.g. via XML Schema, RDF, RDFa? as a separate annotation file or inline?)  


### Leveraging semantics (read) 

This is posed as the central challenge of issue #8.  Before we can address this it would be nice to have some sense of how the semantic information is coming in (e.g. the write issues above), though ideally we are not dependent on those details.  Certainly a problem for a much later stage in the game.


",NA,NA,NA
38,21592780,cboettig,2013-07-25T23:46:42Z,2013-07-25T23:46:42Z,"Among other things, I think that the S4 approach sidesteps our difficulties with the API construction.  

For instance, currently we define `eml_dataset` struggles to list its arguments as the arguments to it's child nodes and then uses some ugly `do.call` and `if` statements to check if a child node is included

```r
eml_dataset <- function(..., .title = ""Unnamed"", .creator=list(), .contact=list(), .methods=list(), .coverage=list(), .rights = eml$get(""default_rights"")){
  dataset <- newXMLNode(""dataset"")
  title <- newXMLNode(""title"", .title)
  rights <- newXMLNode(""intellectualRights"", .rights)
  addChildren(dataset, title)

  creator <- do.call(eml_person, c(person_type=""creator"", .contact))
  addChildren(dataset, creator)
  contact <- do.call(eml_person, c(person_type=""contact"", .creator)) 
  addChildren(dataset, contact)

  if(!is.empty(.coverage)){
   coverage <- do.call(eml_coverage, .coverage)
   addChildren(dataset, coverage)
  }
  
  ## Methods node
  methods_node <- newXMLNode(""methods"", parent=dataset)
  if(!is.empty(.methods)){
   methods_node <- 
     do.call(eml_methods, 
             c(node = methods_node,
               .methods))
  }
....

 ```

All of this is sidestepped when this creature is instead an S4 object with children defined in slots:

```r
setClass('eml_dataset', 
         representation(title = ""XMLInternalElementNode"",
                        creator = ""XMLInternalElementNode"",
                        rights = ""XMLInternalElementNode"",
                        methods = ""XMLInternalElementNode"",
                        datatable = ""XMLInternalElementNode""))
                                       
```
Which is easy to extend.  (Of course we don't have to write it manually, and the abstract node type can be replaced with detailed node definitions as they are written / once they are generated).  @duncantl What then would the arguments to the `eml_write` or `eml_dataset` functions look like (to maintain a clean API)? Can we just map/match arguments to S4 slots? 
",NA,NA,NA
38,21592928,duncantl,2013-07-25T23:49:41Z,2013-07-25T23:49:41Z,"The XMLSchema package generates the constructor functions for the classes corresponding to eml_dataset, eml_write, etc.  Yes, they provide a map from arguments to slots. They also get the optional slots and default values correct when they are specified in the schema.",NA,NA,NA
38,21593718,duncantl,2013-07-26T00:10:07Z,2013-07-26T00:10:07Z,"Why would the slots of eml_dataset be XMLInternalElementNode? The goal is to map the XML from an eml document to regular R objects and similarly, to map the R objects to an XML document that can be exported. ",NA,NA,NA
38,21593890,cboettig,2013-07-26T00:15:00Z,2013-07-26T00:15:00Z,"Right, that was just me being lazy in the example. I think I comment above
that they would actually be the node classes (eg from  XMLschema)

---
Carl Boettiger
http://carlboettiger.info

sent from mobile device; my apologies for any terseness or typos
On Jul 25, 2013 5:10 PM, ""Duncan Temple Lang"" <notifications@github.com>
wrote:

> Why would the slots of eml_dataset be XMLInternalElementNode? The goal is
> to map the XML from an eml document to regular R objects and similarly, to
> map the R objects to an XML document that can be exported.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/38#issuecomment-21593718>
> .
>",NA,NA,NA
38,21593993,cboettig,2013-07-26T00:17:37Z,2013-07-26T00:17:37Z,"Ah, so we will get the constructor fns automatically too; cool.  Will have
to see what they look like...

---
Carl Boettiger
http://carlboettiger.info

sent from mobile device; my apologies for any terseness or typos
On Jul 25, 2013 4:49 PM, ""Duncan Temple Lang"" <notifications@github.com>
wrote:

> The XMLSchema package generates the constructor functions for the classes
> corresponding to eml_dataset, eml_write, etc. Yes, they provide a map from
> arguments to slots. They also get the optional slots and default values
> correct when they are specified in the schema.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/38#issuecomment-21592928>
> .
>",NA,NA,NA
5,21653902,cboettig,2013-07-26T23:17:54Z,2013-07-26T23:17:54Z,"@mbjones Just brainstorming about adding semantics here, since Ben wasn't enthusiastic about the semtools XSD route.  Would love to hear what you think about this approach when you get back.  

I've just added an [example](https://github.com/ropensci/reml/blob/a1ba71a9b769c1e172e8869a1d5ffcd6258d4ea8/inst/examples/rdfa_example.xml#L179) in which semantic metadata is included using RDFa.  Building on Ben's suggestions, the additionalMetadata node looks like:

```xml
<additionalMetadata>
     <describes>1838</describes>
     <metadata>
      <subject about=""http://some.namespace#1838"" xmlns:o=""http:/oboe-core#""
               xmlns:dc=""http://purl.org/dc/elements/1.1/"" xmlns:dcterms=""http://purl.org/dc/terms/"" 
               xmlns:prism=""http://prismstandard.org/namespaces/1.2/basic/"" 
               xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#"" 
               xmlns:rdfs=""http://www.w3.org/2000/01/rdf-schema#"" 
               xmlns:skos=""http://www.w3.org/2004/02/skos/core#""
               xmlns:xsd=""http://www.w3.org/2001/XMLSchema#"" 
               xmlns:nex=""http://www.nexml.org/2009"">
          <meta property=""o:entity"" content=""Air"" datatype=""xsd:string""/>
          <meta property=""o:characteristic"" content=""Temperature"" datatype=""xsd:string""/>
          <meta property=""o:unit"" content=""Celsius"" datatype=""xsd:string""/>
      </subject>
    </metadata>
  </additionalMetadata>
```


I believe this has a few advantages over the (potentially depricated?) semtools xml annotations or RDF nodes: 

-  A dumb parser (e.g. without any knowledge of the schema) could still extract the triples, in any desired format (RDF, turtle, etc).  For instance, w3c's pyRdfa [gives](http://www.w3.org/2012/pyRdfa/extract?uri=https%3A%2F%2Fgithub.com%2Fropensci%2Freml%2Fraw%2Fmaster%2Finst%2Fexamples%2Frdfa_example.xml&format=turtle&rdfagraph=output&vocab_expansion=false&rdfa_lite=false&embedded_rdf=true&space_preserve=true&vocab_cache=true&vocab_cache_report=false&vocab_cache_refresh=false)

```turtle
@prefix o: <http:/oboe-core#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

<http://some.namespace#1838> o:characteristic ""Temperature""^^xsd:string;
    o:entity ""Air""^^xsd:string;
    o:unit ""Celsius""^^xsd:string .
```

- we have semantics embedded in the EML file in a language natural for the expression of semantic data.  


One concern is that the contents our our additionalMetadata node are not very human-readable in this way. Nonetheless, it is still reasonably easy to understand when we render the EML file as a ""plain text"" format by coercing it into yaml:

```yaml
describes: '1838'
  metadata:
    subject:
      meta:
      - o:entity
      - Air
      - xsd:string
      meta:
      - o:characteristic
      - Temperature
      - xsd:string
      meta:
      - o:unit
      - Celsius
      - xsd:string
      .attrs: http://some.namespace#1838
```
Though perhaps ""Air Temperature Celsius"" would be the preferred human version.  In any event, that can be added directly to the text.  (Yeah, the example EML attribute at 1838 isn't actually about temperature, this is just a quick demo of what adding semantics might be about).  

We still have the design consideration questions above to address.  Semantics could be added automatically for Dublin Core terms (things like title, creators, publication date, etc, and for cases like standard units or taxanomic names (at least when stated in coverage nodes if not in attributes) that we can resolve from the schema logic.  

In the long run, ideally additional functions will allow the user to add arbitrary annotations for EML elements through `reml`.  
",NA,NA,NA
5,21653992,cboettig,2013-07-26T23:20:50Z,2013-07-26T23:20:50Z,"@mbjones One quick related issue: for some reason, my [example file](https://github.com/ropensci/reml/raw/a1ba71a9b769c1e172e8869a1d5ffcd6258d4ea8/inst/examples/rdfa_example.xml) does not validate against the online validator.  I get the error:

```
> doc <- saveXML(xmlParse(""rdfa_example.xml""))
> eml_validate(doc)
$`EML specific tests`
[1] ""Error processing keyrefs: //additionalMetadata/describes : Error in xml document. This EML instance is invalid because referenced id 1838 does not exist in the given keys.""
```

even though there is indeed a node with `id=""1838""`, so I'm not sure what I did wrong.  ",NA,NA,NA
5,22506351,leinfelder,2013-08-12T16:33:54Z,2013-08-12T16:33:54Z,"The EML parser was not actually configured to parse attribute@id values as valid references in the additionalMetadata/describes field. I've fixed this and will deploy it soon. Parsing errors aside, the sample EML+RDF looks pretty workable as it stands, but the more I think about it, you should probably just use a single additionalMetadata/describes block for all the RDF instead of little bits for each attribute. This will be easier for parsing the RDF in one go and, as you mention, the RDF explicitly references the attribute@id values anyway as the subject.",NA,NA,NA
39,23459536,cboettig,2013-08-29T00:36:19Z,2013-08-29T00:36:19Z,"Going with the latter option, which corresponds most closely to the schema structure anyway.  

```
metadata <- 
  list(""river"" = list(""river"",
                      ""River site used for collection"",
                      c(SAC = ""The Sacramento River"", 
                        AM = ""The American River"")),
       ""spp"" = list(""spp"",
                    ""Species common name"", 
                    c(king = ""King Salmon"", 
                      ccho = ""Coho Salmon"")),
       ""stg"" = list(""stg"",
                    ""Life Stage"", 
                    c(parr = ""third life stage"", 
                      smolt = ""fourth life stage"")),
       ""ct""  = list(""ct"",
                    ""count"", 
                    ""number""))

```

the outer list need not be named...

",NA,NA,NA
41,23584140,mbjones,2013-08-30T19:28:23Z,2013-08-30T19:28:23Z,"Yes, they are both defined centrally for the purpose of reuse across the schemas.  So, for example, entityGroup is defined in [eml-entity.xsd](http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-entity.html#EntityGroup), and referenceGroup is defined in [eml-resource.xsd](http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-resource.html#ReferencesGroup).   In XML Schema, an `xs:group` is a model containing a set of elements that are meant to be inserted in multiple places, and so the group definition provides a common way to define the set of elements once.  Similar to ""include"" directives in many languages.  See http://stackoverflow.com/questions/12431031/difference-between-group-and-sequence-in-xml-schema
",NA,NA,NA
41,23594535,cboettig,2013-08-30T23:20:53Z,2013-08-30T23:20:53Z,"@mbjones Thanks! somewhat related: can you explain the thinking with attributes?  I see some elements get no attributes, some just get id, and some get id, system and scope (and maybe there are others I haven't seen). 

What determines this pattern?  And what exactly are system and scope for?  ",NA,NA,NA
19,23682933,cboettig,2013-09-02T23:25:25Z,2013-09-02T23:25:25Z,This is now handled automatically as we add more of the schema to the class definitions.  ,NA,NA,NA
37,23690140,cboettig,2013-09-03T05:01:50Z,2013-10-15T17:36:29Z,"Further write tasks will be filed as their own issues, as they should be part of a separate workflow providing methods for additional annotation rather than essential metadata.  
- [ ] reml method note needs to be added back in based on S4 classes.
- [ ] software class and methods need flushing out first.  ",NA,NA,NA
31,23690159,cboettig,2013-09-03T05:03:04Z,2013-09-03T05:03:04Z,"Optional wizard implemented, see vignette for example.  Can be used independently for assembling `attributeList` metadata given a `data.frame`, or will open up if this metadata is missing in an `eml_write` call.  ",NA,NA,NA
21,23690186,cboettig,2013-09-03T05:04:18Z,2013-09-03T05:04:18Z,"Currently assumes local files are in the working directory.  

No longer assuming online or offline endpoints, just uses what it is given.  `eml_publish` step will add online endpoints when available.  ",NA,NA,NA
9,23690254,cboettig,2013-09-03T05:07:16Z,2013-09-04T04:18:31Z,"Coverage classes are now defined.  

- [x] Need to implement some user-friendly constructor files (potentially with wizard cues for missing required definitions)...
- [ ] Need to add examples and tests making use of these
- [ ] Need to think out and implement functions that can automatically fill these in when possible (e.g. from species names in attributeList, aided by `taxize`.  
- [ ] Longer-term: the semantics integration discussed above.  ",NA,NA,NA
11,23690281,karthik,2013-09-03T05:08:40Z,2013-09-03T05:08:40Z,This might be a good time to implement code review. Once you're done with a function you could assign me or someone else to review and check off. Not a priority but maybe a good practice for us to follow as rOpenSci devs.,NA,NA,NA
11,23690354,cboettig,2013-09-03T05:11:37Z,2013-09-03T05:11:37Z,"Great idea.

Note that none of the user-facing functions in reml are actually finished,
(i.e. documented and fully tested), partly because they will support
additional parts of the schema that we haven't implemented yet.  We might
have two levels of this review -- a basic one for testing purposes, and a
more careful review for user-facing functions once version 0.1 has been
pushed to CRAN...


On Mon, Sep 2, 2013 at 10:08 PM, Karthik Ram <notifications@github.com>wrote:

> This might be a good time to implement code review. Once you're done with
> a function you could assign me or someone else to review and check off. Not
> a priority but maybe a good practice for us to follow as rOpenSci devs.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/11#issuecomment-23690281>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
43,23729033,mbjones,2013-09-03T17:04:46Z,2013-09-03T17:04:46Z,"@cboettig `/eml/dataset/pubDate` is meant to hold the [date of publication of a data set](http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-resource.html#pubDate), and is meant to be included in a a citation. See http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-dataset.png ",NA,NA,NA
43,23729731,cboettig,2013-09-03T17:12:32Z,2013-09-03T17:12:32Z,"thanks, not sure how I missed that!  While we're at it, can you clarify the meaning of `system` and `scope` attributes (#41)  in some elements? (and what determines if an element gets no attributes, an `id` attribute, or all three?)",NA,NA,NA
41,23732622,mbjones,2013-09-03T17:47:21Z,2013-09-03T17:47:48Z,"`scope` defines the space within which the identifier should be interpreted, and can be either 'document', which means the ID has no meaning outside of the current document, or 'system', in which case the id should be interpreted as being from and within the namespace of the system identified by the `system` attribute.  `system` defines the URI (hopefully, or at least the name) of the system from which the identifier was drawn and within which it should be unique.  For example, for DOIs, you might set it to 'http://doi.org'.  If `scope` is set to 'document', then the identifier can be used within the document as a pointer, but has no interpretation outside of the document in some larger system.  This can be used, for example, to put an ID on a `<creator>` elelment that is referenced later within the same document.

In general, id, system, and scope should be present on any element that allow 'id', and we allowed 'id' on any element which had a major potential for reuse or external reference.  We discussed allowing ID on every element, but then you end up with pathological cases like `<fieldDelimiter id=""doi:10.6788/xyxyx"">,</fieldDelimiter>`, which we wanted to avoid because it makes parsing the EML very difficult (and starts to look an awful lot like full-blown RDF).  As it is, many EML parsing applications fail to properly parse and interpret `references` fields, so we didn't want to make it harder.  This decision is certainly debatable as to what is the right way to go.

@cboettig Can you give examples of where the `id` attribute is used without `system` and `scope`?



",NA,NA,NA
41,23735018,cboettig,2013-09-03T18:19:32Z,2013-09-03T18:19:32Z,"Great, that makes sense.  Also makes sense that scope and system attributes
should be available whenever you have an id, so I think they are just
missing from the documentation occassionally, e.g.
http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-attribute.html#AttributeListType




On Tue, Sep 3, 2013 at 10:47 AM, Matt Jones <notifications@github.com>wrote:

> scope defines the space within which the identifier should be
> interpreted, and can be either 'document', which means the ID has no
> meaning outside of the current document, or 'system', in which case the id
> should be interpreted as being from and within the namespace of the system
> identified by the system attribute. system defines the URI (hopefully, or
> at least the name) of the system from which the identifier was drawn and
> within which it should be unique. For example, for DOIs, you might set it
> to 'http://doi.org'. If scope is set to 'document, then the identifier
> can be used within the document as a pointer, but has no interpretation
> outside of the document in some larger system. This can be used, for
> example, to put an ID on a` elelment that is referenced later within the
> same document.
>
> In general, id, system, and scope should be present on any element that
> allow 'id', and we allowed 'id' on any element which had a major potential
> for reuse or external reference. We discussed allowing ID on every element,
> but then you end up with pathological cases like <fieldDelimiter
> id=""doi:10.6788/xyxyx"">,</fieldDelimiter>, which we wanted to avoid
> because it makes parsing the EML very difficult (and starts to look an
> awful lot like full-blown RDF). As it is, many EML parsing applications
> fail to properly parse and interpret references fields, so we didn't want
> to make it harder. This decision is certainly debatable as to what is the
> right way to go.
>
> @cboettig <https://github.com/cboettig> Can you give examples of where
> the id attribute is used without system and scope?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/41#issuecomment-23732622>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
41,23740089,mbjones,2013-09-03T19:30:16Z,2013-09-03T19:30:16Z,"They are not missing from the documentation, as the documentation is automatically generated from the schema.  I checked the schema, and they are indeed missing in several places.  Now I'm questioning whether we had a reason for that (e.g., those id's were only meant to have document scope), or if it was a mistake.  It would require some sleuthing through old email list archives to determine if it was intentional.  Probably not a high priority for me unless it causes problems and needs revision in the spec.
",NA,NA,NA
7,23745613,cboettig,2013-09-03T20:48:37Z,2013-09-03T20:48:37Z,"@mbjones is there an external URL I can use to validate against?  (Just for the schema files, I know we can use http://knb.ecoinformatics.org/emlparser/ but have to figure out what went wrong in the `RHTMLForm` function first).  

Currently I have a local copy of the schema downloaded that I use, but a more portable solution would be better.  ",NA,NA,NA
7,23753570,mbjones,2013-09-03T22:51:40Z,2013-09-03T22:51:40Z,"@cboettig Not quite sure what you are asking.  The parsing service can be called at:

http://knb.ecoinformatics.org/emlparser/parse 

as long as you do an HTTP POST and provide the proper parameters.  For example, with `curl` you could do:

```
curl -F action=textparse -F doctext=@/Users/jones/Desktop/eml-sample.xml http://knb.ecoinformatics.org/emlparser/parse
```",NA,NA,NA
7,23753755,cboettig,2013-09-03T22:55:23Z,2013-09-03T22:55:23Z,"Is there a canonical URL for for eml.xsd?
I only see the option to download the xsd files as a tarball, not browse
them as web files....

---
Carl Boettiger
http://carlboettiger.info

sent from mobile device; my apologies for any terseness or typos
On Sep 3, 2013 3:51 PM, ""Matt Jones"" <notifications@github.com> wrote:

> @cboettig <https://github.com/cboettig> Not quite sure what you are
> asking. The parsing service can be called at:
>
> http://knb.ecoinformatics.org/emlparser/parse
>
> as long as you do an HTTP POST and provide the proper parameters. For
> example, with curl you could do:
>
> curl -F action=textparse -F doctext=@/Users/jones/Desktop/eml-sample.xml http://knb.ecoinformatics.org/emlparser/parse
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/7#issuecomment-23753570>
> .
>",NA,NA,NA
7,23754409,mbjones,2013-09-03T23:10:00Z,2013-09-03T23:10:00Z,"Ah.  Now I see.  No, we do not provide one, because it is a security hole for applications to use an external schema for validation (similar to a SQL injection attack, this is an [XML Injection](http://projects.webappsec.org/w/page/13247004/XML%20Injection), also called [XML External Entity XXE Processing] (https://www.owasp.org/index.php/XML_External_Entity_\(XXE\)_Processing). Although our copy may be secure now, if many applications point at it, then it becomes an attractive and central point of attack, and if our host is compromised, then all apps that point at our schema URL would potentially be compromised as well. Compromises can lead to reading sensitive data on your computer (such as files like /etc/passwd), injection of malicious content into your application, and other maladies.  So, we try to make it hard for people to be insecure.  In general, trusting xsi:schemaLocation is allowing a third party to inject data into your process -- you are better served by downloading the schema, inspecting it, and if it is trustworthy, pointing at your local copy for validation.  ",NA,NA,NA
43,23763942,cboettig,2013-09-04T03:33:43Z,2013-09-04T03:33:43Z,Default `pubDate` added to dataset prototype.  ,NA,NA,NA
10,23764283,cboettig,2013-09-04T03:46:35Z,2013-09-04T03:46:35Z,"Now approaching this using `eml_config`, which will allow the user to create S4 objects that can be passed in for frequently used values.  These objects can then be loaded from the custom environment hash using: 

```coffee
get(""defaultElementName"", envir=remlCache)
```

- [ ] Need to add a method to write out and load in cache between R sessions.  Where is a good place to write to for this?  `system.file`?

Still expanding this function.  Commonly reused elements we will want to create and provide in this function:

- [x] intellectualRights element
- [x] default creator element
- [x] default contact element
- [ ] multiple saved taxonomic coverage elements (for researcher working on common set of species)
- [ ] default geographic coverage (multiple) (researcher working with common set of field sites)
- [ ] default software element (commonly used software).  
- [ ] ...

will continue to amend list as we think of more.  feel free to add suggestions.  ",NA,NA,NA
7,23788783,hlapp,2013-09-04T13:29:46Z,2013-09-04T13:29:46Z,"@mbjones I'm not sure I follow the logic. Wouldn't that mean even more so by extension that webpages shouldn't include .js from anywhere, shouldn't include CSS from anywhere, etc, not even from the originating website because that would make it a target for being compromised? W3C schemas do give the schema location, and xs:imports even require it. For example, here's the PROV schema: http://www.w3.org/ns/prov-core.xsd Are you suggesting they are giving a bad example?",NA,NA,NA
44,23809266,cboettig,2013-09-04T17:45:03Z,2013-09-04T17:45:03Z,Thanks. Yeah I should refrain from pushing things that can't install.... Given that we're teaching with this I should probably start a development branch rather than breaking master all the time!,NA,NA,NA
44,23809437,karthik,2013-09-04T17:47:11Z,2013-09-04T17:47:11Z,"Not to worry. Not going to update the package on the AMI for the coming week.

On Wed, Sep 4, 2013 at 10:45 AM, Carl Boettiger <notifications@github.com>
wrote:

> Thanks. Yeah I should refrain from pushing things that can't install.... Given that we're teaching with this I should probably start a development branch rather than breaking master all the time!
> ---
> Reply to this email directly or view it on GitHub:
> https://github.com/ropensci/reml/issues/44#issuecomment-23809266",NA,NA,NA
44,23809972,emhart,2013-09-04T17:55:03Z,2013-09-04T17:55:03Z,I'm also looking into it since my life is now a flood of meetings about metadata here at neon and there's some small consensus that we might go with eml and I'm pointing people towards the package.  Nice to have a working branch that I can show off here.,NA,NA,NA
44,23810979,cboettig,2013-09-04T18:09:14Z,2013-09-04T18:09:14Z,"Okay, I now have a `devel` branch (also pushed to this github repo) that I'll mess around with, and I'll keep master stable (Installing and passing the test suite at least).  Will try and flush out documentation on master.  

The current master branch (1ee0bc33485f3a900ac5bfefc974456439bfee11) now passes all tests and installs (needs more documentation etc to pass Check....).  I'm developing on `devel`.  At this point reml does most of the basic necessities: just closed the next milestone now that we can add coverage metadata.  We can read in, write out, and publish basic EML files.  


Note that the top-level API might still need to change a bit as we support more optional fields and need ways the user can modify them...

Great to hear about interest in the package (at least among us!).  Will push towards a stable version on CRAN in the near future (that is, freezing the user-facing functions API).  File issues and bugs and requests!  

",NA,NA,NA
44,23811088,karthik,2013-09-04T18:10:43Z,2013-09-04T18:10:43Z,@cboettig What is the `API` here? Are you referring to the internal plumbing?,NA,NA,NA
44,23812290,cboettig,2013-09-04T18:26:28Z,2013-09-04T18:26:28Z,"@karthikram Correct, I use the term API to refer to any function exported by the NAMESPACE.  It's the ""API"" because exported functions are designed to be available to other packages (as well as the end-user, obviously), and hence are the interface for both users and applications and should be stable once released (or otherwise justify at least a change to the major number in the semantic versioning).  ",NA,NA,NA
7,23832013,mbjones,2013-09-04T22:50:46Z,2013-09-04T22:50:46Z,"Yes, indeed -- you should only include javascript from a highly trusted source.  That is even more of an issue than XML.  Even when you don't intentionally include untrusted JS code, people develop clever XSRF and related attacks just to inject JS into your pages.  Its a bad thing.  I inspect any JS I include, lock it down as a local copy, and don't rely on external copies, as I would be trusting the security and goodwill of that host.  

The W3C does know about these XML injection issues, and it influenced their web architecture documents.  The W3C TAG [discussed these issues](http://lists.w3.org/Archives/Public/www-tag/2002Oct/0301.html), which was expressed in the web architecture principle of [Reference does not imply dereference](http://www.w3.org/TR/webarch/#implied-dereference); although the security implications are mentioned, they are glossed over in that document.  What they are saying is that just because someone provides an xsi:schemaLocation in their document is not an indication that you should dereference that in your parsing and validation. If that were so, then every document author would have the ability to inject potentially harmful content into your process.  Rather, xsi:schemaLocation is defined as a 'hint' to help someone locate a schema for a namespace that is unknown, but blindly importing them is certainly an exploitable security hole.  I first learned of these issues from a talk in 2000 by David Meggison from the W3C XML Working Group, but they persist today (and are actually easier, as there are now more injection vectors).  The recommended practice to avoid XEE attacks is to download and inspect DTDs and schemas yourself, and then set up a catalog for mapping namespaces to the vetted local schema copy for validation, thereby avoiding potential injection attacks.  XML (and SGML before it) catalogs are a common technology, and every XML and XSLT engine I have seen support them in their APIs.  We use them in Metacat, and simply register each schema we wish to support with its associated xsd or dtd that we have inspected and stored locally.  This is pretty simple and avoids user-driven content injection.

Our experience was that when we provided a resolvable copy of the schema, we started seeing many EML documents pointing at it (ok), and people automatically dereferencing it (bad).  So we took it down.  I think its reasonable to argue that [in principle we should have a resolvable copy of the schema](http://www.w3.org/TR/webarch/#pr-namespace-documents), but in practice it lead to bad dereferencing practices that we wanted to curtail in our community.  For example, I think @cboettig was not aware that its a potential security hole to directly dereference the EML xsd, and would not have found that out if we had placed eml.xsd at a resolvable location.  So, with this in mind, should we put up a resolvable copy?

Related pieces:
    https://www.owasp.org/images/5/5d/XML_Exteral_Entity_Attack.pdf
    http://www.soatothecloud.com/2008/08/dont-follow-that-schemalocation.html
    http://www.slideshare.net/qqlan/bh-ready-v4
    http://www.securityfocus.com/archive/1/297846/30/0/threaded

    
",NA,NA,NA
7,23833568,cboettig,2013-09-04T23:22:00Z,2013-09-04T23:22:00Z,@mbjones @hlapp Thanks both for the input and discussion here.  A bit over my head but I'm trying to follow along.  @mbjones Stupid question: the attack requires that the attacker alter the schema file that lives at the URL given?  ,NA,NA,NA
7,23838412,mbjones,2013-09-05T01:30:03Z,2013-09-05T01:30:03Z,"Yes, the attacker must manipulate one of the information sets that will be injected into the parsing process,  Within the document itself, these include external entities that are defined, and any of the multiple external files that can be included by reference to a 3rd party URI.  In the specific case of the namespace, the xsi:schemaLocation will point at a schema document, and the attacker would need to modify that document, which would potentially compromise multitudes of computers if they all point at that single schema file. ",NA,NA,NA
46,23838516,mbjones,2013-09-05T01:33:29Z,2013-09-05T01:33:29Z,"@cboettig One potential source of validation errors that you may not have considered is the use of illegal XML characters in the user input.  Before you write out the XML, all illegal XML chanracters need to be escaped.  Does your S4 class handle this escaping automatically when moving data in and out of R data structures?",NA,NA,NA
7,23842825,hlapp,2013-09-05T04:13:38Z,2013-09-05T04:13:38Z,"@mbjones I know about the semantics of xsi:schemaLocation, and why it is only a hint for how to obtain the schema definition, but not required to be dereferencable. What I was asking is whether W3C is giving a bad example by providing in its own XSD documents xsi:schemaLocation and xsi:import URIs that actually do dereference, to the correct schema document, even though as you say it's not required. I'm also sure that lots of people do dereference their schemaLocation URIs, and yet they haven't found this undesirable.

I also didn't suggest that one include JS, or XSD for that matter, from arbitrary and untrusted sources. What I did say is that by your logic JS included on an NCEAS-served website that's loaded from the NCEAS server that serves the website is bad and not to be trusted, because NCEAS servers could get compromised and hacked. While that is of course a possibility, and that machines get hacked every day is a fact, surely I shouldn't conclude from that not to visit NCEAS websites in my browser, but download them first via cURL and then inspect them by hand for malicious content?

I don't find anything wrong or hazardous with trusting sites and URI locations provided by well-known institutions (such as, for example, the W3C), and I (and I don't think I'm alone with this) expect that these institutions will strive to apply best sysadmin practices to prevent such compromise. We certainly do so at NESCent, and we take security and compromise detection very seriously, and I would expect NCEAS to do no less. So I'm sorry, but I can't see the case for erecting barriers to developers who want to develop applications with your schemas. The W3C certainly doesn't. ",NA,NA,NA
46,23877957,cboettig,2013-09-05T15:49:10Z,2013-09-05T15:49:10Z,"Yes, it looks like the R XML library automatically escapes these characters.  (Noticed this somewhat by accident in my  example from the README: https://github.com/ropensci/reml/blob/70c1f8b2747515ae32b770007c84c905f1fda3d3/inst/doc/reml_example.xml 

I added an html-marked up link for intellectual rights, which you will see escaped there.  (How would you suggest that section be marked up properly to include a link to the relevant license?  Or should I just stick in the whole license text?",NA,NA,NA
7,23888602,duncantl,2013-09-05T18:00:13Z,2013-09-05T18:00:13Z,"The eml_validate() function is now working again. The problem seems to have been that the format of the HTML changed from using h2 to h4 for the relevant headers.  
So nothing to do with RHTMLForms, just how we process the HTML response.
Ideally, the validator would allow us to request the response as XML or JSON and give it to us without the HTML formatting.",NA,NA,NA
7,23892270,mbjones,2013-09-05T18:50:00Z,2013-09-05T18:50:00Z,"That would be a good change.  We didn't originally design the validator that way, but it would be fairly easy to change it to output more structured data on request.",NA,NA,NA
42,23965322,cboettig,2013-09-06T19:58:48Z,2013-09-06T19:58:48Z,"Fixed by ddfe3efcad5fbe74525ccafe077b64adfeffd5ac

May be adjusted later to use an XML-based reply from the form when that is implemented:
https://projects.ecoinformatics.org/ecoinfo/issues/6079",NA,NA,NA
47,24538606,mbjones,2013-09-16T19:45:37Z,2013-09-16T19:45:37Z,"This is great.  For the other metadata, I would ensure we have all of the metadata attached from [eml-attribute](http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-attribute.png) that are required to properly interpret the variables.  In particular, I think that we should have:

- name, label, and description
- for numerical quantites, units, precision, number type, bounds
- for categorical quantities, definitions of enumerated codes
- for datetime values, ISO 8601 format string and precision

I think we should take DataONE's concept of a 'DataPackage' that can contain multiple data.frame (and other)  objects and extend it to be the container for the high-level metadata about the package (creator, contact, title, abstract, methods, etc).  That way, this information is outside of the data.frame objects, but is still readily accessible.  We should talk some more about the design of this and the dataone library to make them as synergistic as possible and eliminate overlap where possible.


",NA,NA,NA
47,24540927,cboettig,2013-09-16T20:17:49Z,2013-09-16T20:17:49Z,"@mbjones Thanks!  

Yes, I agree entirely about getting more of the `eml-attribute` level metadata in, and struggle a bit to find an approach that is both concise/non-redundant and complete.  I'd love input on proposed function calls / API structure through which the user would specify this information.  Here's a few sticking points I've hit, particularly with `numeric` and `dateTime`.  



* __Numerical quantities__: On one hand, we want to leverage R's native types -- e.g. numeric vs. integer, rather than providing an alternative and potentially inconsistent way to declare type.  On the other hand, these don't map trivially onto EML, in which numbers are `ratio` vs `interval` and can be `whole`, `integer`, `real`, `complex`.  What do we do here?

* __Datetime__ R has several datetime classes: base R provides __Date__, along with __POSIXlt__ and __POSIXct__, not to mention packages like `chron`.  None of these formats have a concept of precision, which is particularly frustrating if you want a column in, i.e. `YYYY` format.  Making it a `Date` with a given formatting string: `as.Date(""2012"", ""%Y"")` adds today's month and day arbitrarily.  We could advise users to define all dates as ""character"" class instead (most may already do this...), but this goes against the ""use native types"" objective.  

Currently character types will be assigned `nominal/nonEnumeratedDomain/textDomain`. Do we provide a mechanism to allow a column to be ""character"" class while still be encoded as a ""dateTime""?  Related, I recall that dateTime in EML can also have much more fuzzy formats -- presumably things like geological Epochs, etc.  I assume these would appear as (ordered?) factors in the `data.frame`, ""Holocene"", etc, but no idea how to go about handling this case such that we write out a `dateTime` node and not a `nominal` under `measurementScale`.  

Currently `reml` expects dateTime to be in one of those three formats, and for the user to just give the format string in the `unit.defs` list.  When writing to csv I use the `format(date, format_string)` to write a character string of the correct structure.  An optional precision could be added to the slot in `unit.defs`.  



* __categorical variables__ The current implementation handles this better, where the user gives code and definition as a named string as already illustrated above.  R has the notion of factors and ordered factors, which map cleanly onto `nominal/enumeratedDomain` and `ordinal/enumeratedDomain`  (I see ordinal can have a `textDomain instead  in the Schema?  Not sure what that would mean....) .   The only non-ideal part of this approach is that it is slightly redundant: the user must write out the codes explicitly (though the wizard helper function will display each code and prompt for the definition if not given).  I should probably at least add an automated check that the codes match the factor levels....


--------------------

the DataPackage idea is interesting -- how is it different from an EML `dataset`?  (I think `dataset` can have multiple `dataTables`?  I may still need to add support for that in `reml`...)  Of course, we already have R classes corresponding to EML concepts of `dataset`, `dataTable`, etc.  It would be relatively trivial to make them inherit `data.frame` methods.  I don't know of a native R concept for a collection of data.frames though (I mean, other than as a list of data.frames).  We could simply add methods such that `dataset[[1]]` would return the first `dataTable` (or equivalent creature, like raster, etc), which would inherit the appropriate R type (e.g. data.frame)...

",NA,NA,NA
46,26354483,cboettig,2013-10-15T17:24:02Z,2013-10-15T17:24:02Z,"I think we aim for a two-fold strategy: (1) mostly to provide constructor functions for which it is difficult to make invalid EML, while still supporting direct construction for advanced users, and then (2) wrap the validation check in the ""publish"" functions (with toggle off option), but not wrap it in the regular ""write"" functions.  We also expose the validation function for end-users to run it themselves if they wish.  (Re-tagging question as ""publish"" instead of ""write"").  

To Do:

- [ ] Add validation check to `publish` functions",NA,NA,NA
20,26354697,cboettig,2013-10-15T17:26:31Z,2013-11-12T04:18:00Z,"- [x] Create publish method for KNB using `dataone` package.  

See notes http://carlboettiger.info/2013/10/10/notes.html on CILogin.  ",NA,NA,NA
47,26355063,cboettig,2013-10-15T17:31:03Z,2013-10-15T17:31:03Z,"We now have support for the unit metadata (attributes) Matt highlights: 

-   name, label, and description
-   for numerical quantites, units, precision, number type, bounds
-   for categorical quantities, definitions of enumerated codes
-   for datetime values, ISO 8601 format string and precision

See [Ex 3](https://github.com/ropensci/reml/blob/master/inst/examples/ex3.R)
",NA,NA,NA
4,26355958,cboettig,2013-10-15T17:41:23Z,2013-10-15T17:41:23Z,"Not sure that it makes sense to have a `publish(""file.xml"", repo=""github"")` in the same sense as `publish(""file.xml"", repo=""KNB"")` or `publish(""file.xml"", repo=""figshare"")`.  Github isn't indexed in the same way as these data repositories -- not clear where the data would be published to on Github anyway.  

In particular, the other repositories have their own notion of metadata, so it makes sense to have a function that can map EML metadata into, say, the required figshare title, category, tag, author, and description boxes. The same is not true for Github.  Users can always use `eml_write` and put their EML where-ever they want.  ",NA,NA,NA
7,26356242,cboettig,2013-10-15T17:44:40Z,2013-10-15T17:44:40Z,"Since we have a working `eml_validate` function at this point, I think we can close this issue.  See #46 on workflow for validation.  ",NA,NA,NA
48,26366634,karthik,2013-10-15T19:54:12Z,2013-10-15T19:54:12Z,We could use this issue to highlight other rOpenSci packages. Download data that cover these different types of coverage and document those. Also what other major issues need to be worked on before [this milestone](https://github.com/ropensci/reml/issues?milestone=7&state=open)?,NA,NA,NA
48,26370488,emhart,2013-10-15T20:44:13Z,2013-10-15T20:44:13Z,"I think this is a great idea and something to incorporate into spocc (née occdat) Right now it's written with S4 classes, but we can write the dataset with EML that automatically incorporates those elements using reml.  I'll open an issue on the other package.",NA,NA,NA
50,26372498,cboettig,2013-10-15T21:09:43Z,2013-10-15T21:09:43Z,"Consider adding support for `listOf` `XMLInternalNode` elements to the `<metadata>` element. Currently this can be accomplished very crudely:

 
```coffee
rdfa <- '<subject about=""http://some.namespace#1838"" xmlns:o=""http:/oboe-core#"">
             <meta property=""o:entity"" content=""Air"" datatype=""xsd:string""/>
             <meta property=""o:characteristic"" content=""Temperature"" datatype=""xsd:string""/>
             <meta property=""o:unit"" content=""Celsius"" datatype=""xsd:string""/>
            </subject>'
eml_write(dat, additionalMetadata = new(""additionalMetadata"", metadata = rdfa))
```
",NA,NA,NA
47,26439402,mbjones,2013-10-16T17:26:06Z,2013-10-16T17:26:06Z,Fantastic!,NA,NA,NA
52,26487047,SimonHStats,2013-10-17T08:31:29Z,2013-10-17T08:31:29Z,"Oh christ, I've put this in the Reml thread. Is there a way to migrate. I'm really sorry, I'm quite new to Github.",NA,NA,NA
53,26764327,emhart,2013-10-21T22:58:51Z,2013-10-21T22:58:51Z,"I've been using NetCDF4 in R, you just need to use the [ncdf4 package](http://cran.r-project.org/web/packages/ncdf4/index.html).  That said it seems like netcdf would be a long shot, as most ecologists have never heard of it before, let alone how to use it.  Also is there an easy way to use XSLT to do the mappings?  ",NA,NA,NA
53,26765612,cboettig,2013-10-21T23:22:12Z,2013-10-21T23:22:12Z,"@emhart Thanks for the ncdf4 package, good to know that exists.  I agree entirely about the low uptake among ecologists, but I think it's issues just like this that make an unnecessary barrier between environmental scientists and ecologists; who obviously have large conceptual overlap.  (NOAA provides some very good fish population dynamics in netcdf, which might be useful to ecologists, and ecologists wanting to send data to enviro sci collaborators might prefer netcdf to EML?)  Hopefully by providing more tools that allow them to consume each-other's data formats we can decrease barriers slightly.  

netcdf4 might also be a good choice merely for performance reasons with large data.  

Yeah, good thought about XSLT. It would certainly be nice to define mappings in a way that wasn't R dependent; and to re-use existing mappings.  @mbjones is that how the mapping to BDP is done?  

@emhart The XSLT approach has been done elsewhere  -- for instance, NeXML defines XSLT mappings to render NeXML into CDAO/RDF format: https://github.com/nexml/nexml/blob/master/xslt/nexml2cdao.xsl#L293 However, this example highlights some of the weaknesses of that approach as well, at least as far as converting into linked data RDF formats -- the CDAO URIs there are largely outdated, and some developers advocate maintaining a simpler mapping, such as a simple JSON table.  See https://github.com/nexml/nexml/issues/5 for some reasons not to do this with XSLT. 





",NA,NA,NA
53,26766927,cboettig,2013-10-21T23:49:10Z,2013-10-21T23:49:10Z,"Whoops.  In case you thought this issue sounded familiar... see: #15 

@mbjones links several XSL stylesheets.  @emhart feel free to add some R function wrappers based on these stylesheets, or I'll get to it one of these days.  My intuition would be to create a directory such as `inst/xsl` and have the functions work from the local files?  Or maybe better to pull the XSL from an online home?  

@mbjones it would be great if you could track down the ISO19115 mappings.  and would love to hear any thoughts on eml to/from netcdf question.  
",NA,NA,NA
53,26767333,cboettig,2013-10-21T23:57:17Z,2013-10-21T23:57:17Z,@emhart The Sxslt package may be useful in implementing this. See documentation: http://www.omegahat.org/Sxslt/ and github home: https://github.com/omegahat/Sxslt,NA,NA,NA
53,26864442,cboettig,2013-10-22T23:36:18Z,2013-10-22T23:36:18Z,"@duncantl @mbjones Not managing to apply this [eml2tonbii.xsl](https://github.com/ropensci/reml/blob/master/inst/xsl/eml2tonbii.xsl) using [my example EML](https://github.com/ropensci/reml/blob/master/inst/doc/reml_example.xml) file using @duncantl 's Sxslt package for some reason (installed from www.omegahat.org, because I couldn't get the github.com/omegahat/Sxslt to install):

Not sure if this is a problem with the XSL file or the Sxslt wrapper?  

```coffee
library(Sxslt)
xsl <- ""inst/xsl/eml2tonbii.xsl""
eml <- ""inst/doc/reml_example.xml""
xsltApplyStyleSheet(eml, xsl)
```


```
xmlXPathCompOpEval: function nodeset not found
XPath error : Unregistered function
xmlXPathCompiledEval: 1 objects left on the stack.
$doc
<?xml version=""1.0"" encoding=""ISO-8859-1""?>
<metadata>
  <idinfo>
    <citation>
      <citeinfo>
        <origin/>
      </citeinfo>
    </citation>
  </idinfo>
</metadata>
 

```",NA,NA,NA
53,26865805,duncantl,2013-10-22T23:47:16Z,2013-10-22T23:58:04Z,"On the train and I have a brief moment before exiting. Apologies for the brevity.

It looks like the xsl uses functions that are defined in the xalan namespace.
Some of these may be now in the exslt namespace (i.e. extended xsl) available in libxslt. Some may not.
Specifically  xalan:nodeset().  This most likely corresponds to exslt:node-set().

",NA,NA,NA
53,26881329,mbjones,2013-10-23T05:23:58Z,2013-10-23T05:23:58Z,"I think Duncan's right about the xalan dependency -- that rings a bell.  Not sure if we could generalize that to use a more commonly implemented function so we can avoid the xslt processor dependency.  

GBIF did the work on the EML to ISO 19139 conversion stylesheet.  It's a start, and I'm sure the whole community would rejoice if we made the conversion script more complete.  We haven't used this in our software yet, but here's an email thread with some background on the converter:
  http://lists.nceas.ucsb.edu/ecoinformatics/pipermail/eml-dev/2010-October/001859.html
And General info:
  http://rs.gbif.org/schema/eml-gbif-profile/dev/
And the XSLT file itself:
  http://rs.gbif.org/schema/eml-gbif-profile/dev/eml2iso19139.xsl",NA,NA,NA
53,26922916,cboettig,2013-10-23T17:00:21Z,2013-10-23T17:00:38Z,"@mbjones Cool!  That stylesheet works with the EML 2.1.0 test file (from Harvard Forest LTER), but not the 2.1.1 EML I'm writing with reml (executes but returns empty): 


```coffee
> library(Sxslt)
> a = xsltApplyStyleSheet(""inst/examples/hf205.xml"", ""inst/xsl/eml2iso19139.xsl"")
```
works!   [output XML here](https://github.com/ropensci/reml/blob/8873df377687d33c0613cc7ff46b1bf257032c63/inst/examples/hf205_iso19139.xml).  Not sure how I'd validate that `hf205_iso19139.xsl`; a couple lines look questionable (plain text at start and end, missing contact info).  

This example with our `reml` generated EML fails:  

```coffee
> a = xsltApplyStyleSheet(""inst/doc/reml_example.xml"", ""inst/xsl/eml2iso19139.xsl"")
> a$doc
<?xml version=""1.0""?>
```

@duncantl Not quite clear what needs to be done about the xalan to get the other stylesheets working.  ",NA,NA,NA
20,27333057,cboettig,2013-10-29T19:02:55Z,2013-10-29T19:02:55Z,"@mbjones Just implemented the [knb method](https://github.com/ropensci/reml/blob/devel/R/eml_knb.R) on the development branch, following your example from https://github.com/mbjones/opensci_r_esa_2013/issues/4.  Seems something on the server side is down though, KNB isn't resolving metadata files at the moment: e.g. even in a browser this doi gives me an error: http://dx.doi.org/10.5063/AA/wolkovich.30.1 resolves to this page with an error:
https://knb.ecoinformatics.org/knb/d1/mn/v1/object/doi:10.5063%2FAA%2Fwolkovich.30.1 


Also, quick question on the ids:

- is `id.mta` (the id of the metadata D1Object in https://github.com/mbjones/opensci_r_esa_2013/issues/4) the same as `id.pkg` (and also the same as `packageId` attribute on the head `eml` node?) If not, which one is the `packageId` and where does the other ID live inside the EML? 
- Also in https://github.com/mbjones/opensci_r_esa_2013/issues/4, you show uploading the EML both by passing the filename, ""text.xml"", and later by first using a ""readLines"" and passing the text.  Does either approach work or is the `readLines` step necessary, given the name of an xml file?  
",NA,NA,NA
20,27344135,cboettig,2013-10-29T21:14:10Z,2013-10-29T21:14:10Z,"@mbjones KNB seems to be back up and the function is generally working:


I've just assumed that the EML `packageId` attribute should correspond to the metadata of the eml D1Object itself, while the `pkg.id` passed to the `new(""dataPackage""`` call should be something different (I just append `_package` to the eml packageId).  Please correct me if this is wrong or ill-advised!!

The example shown in [test_knb.R](https://github.com/ropensci/reml/blob/devel/inst/tests/test_knb.R) generates these IDs for the EML, the CSV, and the package, respectively, and it appears each of them resolve in the browser:  

* eml: [urn:uuid:7d01d639-0e89-4d75-9d88-e334f11b8bad](https://knb.ecoinformatics.org/knb/d1/mn/v1/object/urn:uuid:7d01d639-0e89-4d75-9d88-e334f11b8bad)
* csv: [urn:uuid:931dea76-2f84-4736-9875-4beade1d62a8](https://knb.ecoinformatics.org/knb/d1/mn/v1/object/urn:uuid:931dea76-2f84-4736-9875-4beade1d62a8)
* package: [urn:uuid:7d01d639-0e89-4d75-9d88-e334f11b8bad_package](https://knb.ecoinformatics.org/knb/d1/mn/v1/object/urn:uuid:7d01d639-0e89-4d75-9d88-e334f11b8bad_package)

Yay!


```coffee
> a = getD1Object(cli, pid[[""csv""]])
checkServerTrusted - RSA
checkServerTrusted - RSA
   === Trying Location: urn:node:KNB (https://knb.ecoinformatics.org/knb/d1/mn/v1/object/urn:uuid:931dea76-2f84-4736-9875-4beade1d62a8)
checkServerTrusted - RSA
@@ D1Object-class:R initialize as something else
@@ D1Object-class:R initialize with jobjRef
> asDataFrame(a)
theData is textConnectionconnection
  river                      spp   stg  ct        day
1   SAC Oncorhynchus tshawytscha smolt 293 2013-09-01
2   SAC Oncorhynchus tshawytscha  parr 410 2013-09-01
3    AM     Oncorhynchus kisutch smolt 210 2013-09-02
```

- [ ] Um, I'm not sure what the equivalent of `asDataFrame` is for the XML file.  For `reml` at least I think we might prefer a wrapper to `getD1Object` / `getD1Package` so that we can read in EML-annotated data in the same way from KNB as we do with local files in the `eml_read` function.  


I think I still need some advice on how to write an appropriate test case.  The server detects that my files have already been uploaded based on matching metadata:     

```
 * SystemMetadata indicates that this object was already created (uploaded Tue Oct 29 13:12:14 PDT 2013). Skipping create.
```

even when the test has generated a new ID, which I guess is good, though it's not obvious to me what's happening.  Am I creating multiple ids that point to the same file this way?  What's the best way to go about writing tests so that (a) they don't trigger this error, and (b) I don't end up writing a lot of junk to KNB?  


",NA,NA,NA
20,27356238,mbjones,2013-10-30T00:27:51Z,2013-10-30T00:27:51Z,"That's great, @cboettig!  Glad to see its working, and sorry you hit the server while we were migrating it from one IP address to another.  Should be good to go now.

About `packageId`, we have always used the ID of the EML document there, and use that as a surrogate for the package.  There are arguments that this isn't right given our new `DataPackage` model, but history is trumping novelty for the time being for us.  Your way of generating IDs for the packages sounds great and matches ours.  

I need to turn down the debug output from the R Client -- kinda annoying that it spits out all of that garbage.  Need to work on that, so I entered a [ticket](https://redmine.dataone.org/issues/4143).

We don't have a good viewer for XML in the `dataone` package -- I agree it would be great to automatically create a `reml` instance from it so it is more manageable in R.  I'm open to suggestions on how to do that.  As I've mentioned, I don't think asDataFrame() belongs in the `dataone` package anyways.

About your error saying that the object was already created -- there is something wrong there.  It's perfectly legitimate to upload two identical copies of an object that differ only by identifier.  There may be a bug lurking here. Can you describe more precisely which part of the code triggers this message?

Regarding testing, it would be best to not upload test content to the KNB (and to archive anything that isn't real science data so it doesn't show up in searches).  We run a variety of testing servers that you can use instead of the KNB.  They, of course, are not stable like the KNB because we clear content and reinstall them fairly regularly, but they serve the purpose of allowing you to generate a lot of test data without swamping the view normal scientists see in the KNB.  We provide an [overview of the environments](http://mule1.dataone.org/OperationDocs/environments.html) which is good background but the details of the nodes are now wrong. You can find out what nodes are available in each environment using the nodes service:
* Production: https://cn.dataone.org/cn/v1/node
* Staging: https://cn-stage.test.dataone.org/cn/v1/node
* Sandbox: https://cn-sandbox.test.dataone.org/cn/v1/node
* Dev: https://cn-dev.test.dataone.org/cn/v1/node
",NA,NA,NA
20,27361777,cboettig,2013-10-30T02:57:12Z,2013-10-30T02:57:12Z,@mbjones very good.  Can I select these environments from the dataone R client?  From the docs its not clear if sandbox or staging would be preferred for unit testing?  ,NA,NA,NA
20,27444667,mbjones,2013-10-30T22:18:16Z,2013-10-30T22:18:16Z,"@cboettig Yes, you can do so when setting up the client.  Don't be surprised if the test environments are sometimes not working, as they go up and down a lot.  Nevertheless, here's how you would use the 'DEV' environment.
```coffee
# Initialize a client to interact with DataONE
mn_nodeid <- ""urn:node:mnDemo5""           # MN for DEV env
cli <- D1Client(""DEV"", mn_nodeid)
```

And I updated a full example here:
https://github.com/mbjones/opensci_r_esa_2013/blob/master/dataone-r/dataone-write-pkg.R",NA,NA,NA
20,27506948,cboettig,2013-10-31T17:26:29Z,2013-10-31T17:26:29Z,"I get the error

    Error: The provided mnNodeid value is invalid for the DataONE
environment

with both

    mn_nodeid <- ""urn:node:mnDemo5"" # MN for DEV env
    cli <- D1Client(""DEV"", mn_nodeid)

and

    mn_nodeid <- ""urn:node:mnSandboxUCSB1
    cli <- D1Client(""SANDBOX"", mn_nodeid)

is that just because the environments are down?








On Wed, Oct 30, 2013 at 3:18 PM, Matt Jones <notifications@github.com>wrote:

> @cboettig <https://github.com/cboettig> Yes, you can do so when setting
> up the client. Don't be surprised if the test environments are sometimes
> not working, as they go up and down a lot. Nevertheless, here's how you
> would use the 'DEV' environment.
>
> # Initialize a client to interact with DataONEmn_nodeid <- ""urn:node:mnDemo5""           # MN for DEV envcli <- D1Client(""DEV"", mn_nodeid)
>
> And I updated a full example here:
>
> https://github.com/mbjones/opensci_r_esa_2013/blob/master/dataone-r/dataone-write-pkg.R
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/20#issuecomment-27444667>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
20,27514472,mbjones,2013-10-31T18:50:07Z,2013-10-31T18:50:07Z,"I got that error yesterday too.  I quit R Studio and relaunched, and then it worked fine.  I just tried it now and its working for me.  The problem seems to be because R loads Java classes statically via a JNI interface, so I'm thinking that the library is caching the old environment and therefore not seeing the the MN you request.  There's no way in R to cause the Java library to reload. I can consistently change environments as long as I restart R in between trying to switch environments with the D1Client.  Can you try restarting your R/Java environment and trying again? I opened an issue for this: https://redmine.dataone.org/issues/4151",NA,NA,NA
20,27516017,cboettig,2013-10-31T19:08:47Z,2013-10-31T19:08:47Z,"Cool, restarting R worked.  I can run the upload function successfully, but
get an error on `getD1Object` on the resulting id for, say, the uploaded
csv file:

     csv <- getD1Object(cli, pid[[""csv""]])

With message and trace:

checkServerTrusted - RSA
Error in .jcall(""RJavaTools"", ""Ljava/lang/Object;"", ""invokeMethod"", cl,  :
  org.dataone.service.exceptions.NotFound: No system metadata could be
found for given PID: urn:uuid:1ae5ec40-0fcf-49ce-aae8-fe71669e7594

Enter a frame number, or 0 to exit

1: getD1Object(cli, pid[[""csv""]])
2: getD1Object(cli, pid[[""csv""]])
3: .local(x, identifier, ...)
4: J(""org/dataone/client/D1Object"")$download(jPid)
5: .jrcall(x@name, name, ...)
6: .jcall(""RJavaTools"", ""Ljava/lang/Object;"", ""invokeMethod"", cl, .jcast(if
(i
7: .jcheck(silent = FALSE)




On Thu, Oct 31, 2013 at 11:50 AM, Matt Jones <notifications@github.com>wrote:

> I got that error yesterday too. I quit R Studio and relaunched, and then
> it worked fine. I just tried it now and its working for me. The problem
> seems to be because R loads Java classes statically via a JNI interface, so
> I'm thinking that the library is caching the old environment and therefore
> not seeing the the MN you request. There's no way in R to cause the Java
> library to reload. I can consistently change environments as long as I
> restart R in between trying to switch environments with the D1Client. Can
> you try restarting your R/Java environment and trying again? I opened an
> issue for this: https://redmine.dataone.org/issues/4151
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/20#issuecomment-27514472>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
20,27517154,mbjones,2013-10-31T19:22:57Z,2013-10-31T19:22:57Z,"Great! Did you wait the required time for synchronization to occur to DataONE before looking for the object?  Usually about 3-4 minutes?  Each MN decides how long the interval is between sync calls with DataONE; Metacat defaults to 3 minutes.  So, no object will show up on DataONE until the sync has occurred after the create.

The R client depends on the CN to find an object (for various good reasons), but we could also make it possible to directly look on the MN for the created object, which is available immediately after create().  Its probably a good change.",NA,NA,NA
20,27520950,cboettig,2013-10-31T19:49:37Z,2013-10-31T19:49:56Z,"I didn't wait.  Working now.  I can add a `sleep` command into the unit test I suppose.  I'm just using `getD1Object` in the unit test as way of confirming that the data was successfully uploaded in the first place, so perhaps there's a more sensible query I can use anyhow. 

So, looks like we now have working publish method for the KNB.  A few issues still to attend to:

- [ ] The function should probably be updating the EML file to include the download URL for the CSV and EML file itself, right?  I gather the full URL can be inferred from the identifier once the D1Client and mn_node have been specified?  

- [ ] The function assumes the certificate is already available in /tmp/, and thus  I haven't wrapped any of the authentication steps; e.g.

```coffee
cm <- CertificateManager()
downloadCert(cm)
getCertExpires(cm)
```

even though we're dynamically loading `dataone` library inside the function call. This is the same way we handle `figshare` -- neither package is strictly a dependency and is instead only on the ""suggests"" list.  The documentation simply directs the user to install those packages and consult there docs on how to set up authentication.  The only difference is that for figshare this is more of a 'set and forget' process, where you copy some API keys from the web client into your .Rprofile once and then you're set.  It looks like the dataone certificates expire in 24 hrs(!?), so it's more of a burden there. 

- [ ] Might consider adding some user prompts about details like waiting the 3 minutes before querying the objects (not that we provide any query methods from reml yet anyway -- but that's a separate issue -- we could just read in the EML file directly from it's published URL).


Other feedback on the function is welcome!  See the [function definition](https://github.com/ropensci/reml/blob/f9346d3b7716b4f5298d411248353907defc350d/R/eml_knb.R) and the [test example](https://github.com/ropensci/reml/blob/10e99d6ae86ab581f4cb5520aa6f232efc4ce381/inst/tests/test_knb.R) (version stable links, devel branch). 
",NA,NA,NA
54,27529013,mbjones,2013-10-31T21:07:52Z,2013-10-31T21:07:52Z,"@cboettig I am planning that access directives in the EML are going to be deprecated in the next version, so long as the community agrees.  When we originally created EML, it made sense to add access control rules, but over the years it we've found that only Metacat pays any attention to them.  Partly that is because it becomes painful to have to version a metadata file just to change external properties such as access control, so most repositories manage access independently of the metadata file itself.  So, in DataONE, we have followed suit and moved the access rules to SystemMetadata, and don't really pay any attention to the access rules in the EML (except during special import cases where they populate SystemMetadata for backwards compatibility).  Calls to the `dataone` `setPublicAccess()` method handle this.  This has worked well, and has the benefit of supporting multiple metadata standards (e.g., EML, ISO, FGDC, etc.).  So, I wouldn't invest a lot in serializing access rules into EML.",NA,NA,NA
20,27531135,mbjones,2013-10-31T21:36:30Z,2013-10-31T21:36:30Z,"The download URL may change over time, and we would like to protect against that so the metadata remains useful over time.  This can be due to changes in the URL that is used for services on a node, or because a node is unavailable temporarily but replica copies may be available on other nodes, or because a node is defunct and a replica copy of an object on another node has become the authoritative copy. DataONE tracks multiple copies of objects, and knows which are available at any given moment.  DataONE provides its `resolve()` service to report the current locations of objects.  The `dataone` R package uses the resolve service to find out which nodes contain a copy of an object, and then downloads a copy from one of those, rather than a static URL.  (This is why you currently have to wait for the object to be reported to the DataONE Coordinating Node, because that is what allows the resolve() service to know about the object.)

So, a couple things to consider:

* Although you can predict the download URL for a node using its reported baseUrl (e.g., https://knb.ecoinformatics.org/knb/d1/mn/v1/object/doi:10.5063%2FAA%2Fnceas.290.8), the baseUrl is almost guaranteed to change over the course of a few years.
* The DataONE `resolve()` service should represent a stable URL for an object.  For example, for the DOI above, resolve run from a browser will redirect to the same knb URL above, and for a user agent requesting XML, will provide a list of locations for that object.  So, for https://cn.dataone.org/cn/v1/resolve/doi:10.5063%2FAA%2Fnceas.290.8, a browser will return the EML document via a redirect, while through curl you'll see the resolve response.

```xml
$ curl -H ""Accept: text/xml"" https://cn.dataone.org/cn/v1/resolve/doi:10.5063%2FAA%2Fnceas.290.8

<?xml version=""1.0"" encoding=""UTF-8""?>
<d1:objectLocationList xmlns:d1=""http://ns.dataone.org/service/types/v1"">
  <identifier>doi:10.5063/AA/nceas.290.8</identifier>
  <objectLocation>
    <nodeIdentifier>urn:node:CN</nodeIdentifier>
    <baseURL>https://cn.dataone.org/cn</baseURL>
    <version>v1</version>
    <url>https://cn.dataone.org/cn/v1/object/doi:10.5063%2FAA%2Fnceas.290.8</url>
  </objectLocation>
  <objectLocation>
    <nodeIdentifier>urn:node:KNB</nodeIdentifier>
    <baseURL>https://knb.ecoinformatics.org/knb/d1/mn</baseURL>
    <version>v1</version>
    <url>https://knb.ecoinformatics.org/knb/d1/mn/v1/object/doi:10.5063%2FAA%2Fnceas.290.8</url>
  </objectLocation>
</d1:objectLocationList>
```

So, you might consider adding the Resolve URI for the online/url field in EML as that should over time be the most stable URL for an object (notwithstanding issues such as DataONE failure, which is a possibility with any repository, but less likely for DataONE given our institutional diversity). ",NA,NA,NA
54,27532849,cboettig,2013-10-31T21:59:29Z,2013-10-31T21:59:29Z,"Sounds great.  Makes sense and simplifies things for `reml`, so I think we will just ignore `access` nodes and can safely close this issue. ",NA,NA,NA
20,27535112,cboettig,2013-10-31T22:37:33Z,2013-10-31T22:37:33Z,"So when publishing to the KNB via the `dataone` package, would it then be advisable to encode the resolve URL as the download URL for each object? 

Or is it in fact preferable to omit the download URLs entirely for content on the KNB, since the file should be accessed by its identifier rather than a download URL?  

e.g. would it be advisable to have the `<physical><distribution>`  state something like: 

```xml
<online>
  <url function=""download"">""https://cn.dataone.org/cn/v1/resolve/urn:uuid:931dea76-2f84-4736-9875-4beade1d62a8"" </url>
</online>
```
(and then presumably we would substitute in  https://cn-dev.test.dataone.org/ etc for the test case using the dev server?), or should we avoid writing such a url?




On the reading end, sounds like we should teach `eml_read` to accept identifiers rather than URLs, e.g. you would do something like:

```coffee
eml_read(""doi:10.5063%2FAA%2Fnceas.290.8"")
```

(Though we can also make it such that forwarding URL links also work, e.g. 

```coffee
eml_read(""https://cn.dataone.org/cn/v1/resolve/doi:10.5063%2FAA%2Fnceas.290.8"")
```

would work as long as that URL also resolves...",NA,NA,NA
20,27552404,mbjones,2013-11-01T08:14:05Z,2013-11-01T08:14:05Z,"Yes, I think it would be advisable to provide the CN resolve URL for an object in its `online/url` field as you propose.  This will help clients that do not know that these objects are on the DataONE network and accessible by DataONE services.  Clients who do know will be able to use the id attribute to find the identifier for use in DataONE service calls. Others will find the URL handy.  And yeah, it's fine to use cn-dev for testing URLs.

So I think `eml_read` should be able to take an identifier, and it could easily delegate to the `dataone` library to handle all of the MN.get() calls.  You could, for example, use something like this, with of course error checking:

```coffee
eml_read <- function(pid) {
    # Set up a D1Client class and stuff ahead of time
    cli <- D1Client()
    # then get the metadata object from dataone package, which handles the resolve and getting the data bytes
    obj0 <- getD1Object(cli, pid)    # Be sure to check for errors before proceeding
    formatId <- getFormatId(obj0)
    metadata <- xmlParse(getData(obj0))
    # if the formatId is some form of EML, send it off for further parsing in reml
}
```",NA,NA,NA
56,27576200,cboettig,2013-11-01T15:57:44Z,2013-11-01T15:57:44Z,"Thanks, I was about to ask how I should handle that.  Um, a bit stuck: 

After logging in to the KNB and going to https://knb.ecoinformatics.org/m/#data/search/Boettiger, I don't see any button to archive these.  (Also, not obvious why this list isn't comprehensive, but that's a different question I guess).

Using curl would be a nice way to go, but I guess I'm not parsing the documentation correctly.  For one, it's not clear to me what the base URL I should use is?  I try 

```bash
curl -E /tmp/x509up_u1000 https://cn.dataone.org/cn/v1/archive/urn%3Auuid%3A07a5f00b-78d6-4f8e-a214-3526daabc65c
```

And just get the following error xml:

```xml
<?xml version=""1.0"" encoding=""UTF-8""?>
<error detailCode=""500"" errorCode=""500"" name=""ServiceFailure"">
    <description>Internal Server Error: The server encountered an unexpected condition which prevented it from fulfilling the request.</description>
</error>
```

Any hints as to what I've done wrong? ",NA,NA,NA
56,27584686,mbjones,2013-11-01T17:44:20Z,2013-11-01T17:44:20Z,"The list isn't comprehensive because the web search only lists the EML documents, not the associated identifiers for CSV files and ORE packages.  I could figure out a SOLR search for these, but its probably just easier to look at the ORE package associated with each EML document.

Yeah, sorry, we don't have a web-based way of archiving content.  You have to call the rest service, or use a client tool that supports it.  We have a [ticket for this feature](https://redmine.dataone.org/issues/3428) for the R client. In the meantime you have to use curl.

Your command is just about right, but the problem is you are calling it on the CN, not on the KNB.  You can only make changes (including archive) an object on the authoritativeNode for an object, in this case the KNB.

```bash
curl -E /tmp/x509up_u1000 https://knb.ecoinformatics.org/knb/d1/mn/v1/archive/urn%3Auuid%3A07a5f00b-78d6-4f8e-a214-3526daabc65c
```",NA,NA,NA
56,27590979,cboettig,2013-11-01T18:51:20Z,2013-11-01T18:51:20Z,"@mbjones that all makes sense.  For some reason though I'm still getting an error with my curl command. 

```coffee
curl -E /tmp/x509up_u1000 https://knb.ecoinformatics.org/knb/d1/mn/v1/archive/urn%3Auuid%3A90d455cd-3066-423a-a526-c3fb114a31b8
```


```xml
<?xml version=""1.0"" encoding=""UTF-8""?>
<error detailCode=""0000"" errorCode=""500"" name=""ServiceFailure"">
    <description>Unknown error, status = false</description>
</error>
```

the [file with that id exists](https://knb.ecoinformatics.org/m/#view/urn:uuid:90d455cd-3066-423a-a526-c3fb114a31b8) and the key with the R console...",NA,NA,NA
56,27593896,mbjones,2013-11-01T19:26:48Z,2013-11-01T19:26:48Z,"Oh, sorry, I spaced that this needs to be a PUT operation, so you need to add that to the curl command:
```bash
curl -E /tmp/x509up_u1000 -X PUT https://knb.ecoinformatics.org/knb/d1/mn/v1/archive/urn%3Auuid%3A07a5f00b-78d6-4f8e-a214-3526daabc65c
```",NA,NA,NA
56,27594540,cboettig,2013-11-01T19:35:26Z,2013-11-01T19:35:26Z,"ah of course, and that part was in the documentation.  working now.


On Fri, Nov 1, 2013 at 12:26 PM, Matt Jones <notifications@github.com>wrote:

> Oh, sorry, I spaced that this needs to be a PUT operation, so you need to
> add that to the curl command:
>
> curl -E /tmp/x509up_u1000 -X PUT https://knb.ecoinformatics.org/knb/d1/mn/v1/archive/urn%3Auuid%3A07a5f00b-78d6-4f8e-a214-3526daabc65c
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/56#issuecomment-27593896>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
56,27601075,cboettig,2013-11-01T21:06:46Z,2013-11-01T21:06:46Z,"Okay, I've archived all the test data that appeared in https://knb.ecoinformatics.org/m/#data/search/Boettiger 

Thanks to the slick REST API you have in place, this is really easy from within R as well:

```coffee
httr::PUT(paste0(""https://knb.ecoinformatics.org/knb/d1/mn/v1/archive/"", 
                         ""urn:uuid:0bf9cf50-6b6b-45ad-b22c-ffd0ee2a21c3""), 
               config=config(sslcert = ""/tmp/x509up_u1000""))
```
and easy to wrap into a function if necessary. 


I'm looking at getting a list of other files (e.g. the non-EML files) I'd still need to archive (just a few csv files).  The documentation on how to do queries is a bit unclear to me.  Looks like I need to specify a query engine, and it looks like I can only get a list of possible query engines by making a query (i.e. not from the online documentation). http://mule1.dataone.org/ArchitectureDocs-current/apis/MN_APIs.html#MNQuery.getQueryEngineDescription

```bash
curl https://knb.ecoinformatics.org/knb/d1/mn/v1/query
```
seems to suggest that `solr` and `querypath` are possible engines, though I'm not sure quite how they work.  Trying: 

```bash
curl https://knb.ecoinformatics.org/knb/d1/mn/v1/query/querypath/Boettiger
```

gives me a huge XML file with lots of stuff that obviously isn't me...  

@mbjones Thanks again for the walk-throughs with this! Knowing my way around the REST API will definitely be helpful!





",NA,NA,NA
57,27606513,mbjones,2013-11-01T22:37:16Z,2013-11-01T22:37:16Z,"The official EML discussion list is eml-dev@ecoinformatics.org.  You can subscribe at http://lists.nceas.ucsb.edu/ecoinformatics/mailman/listinfo/eml-dev.  The list is very quiet these days, as EML has been fairly stable.  But there are occasional bursts of activity, and occasional discussions.  Most everyone subscribed has a pretty solid knowledge of EML.
",NA,NA,NA
56,27608560,mbjones,2013-11-01T23:24:22Z,2013-11-01T23:24:22Z,"Each node can implement different query engines, each with their own query syntax.  Metacat implements `pathquery` (which is deprecated) and `solr`, which is preferred.  Once you choose an engine, you can get a list of the fields it supports by:
```bash
https://cn.dataone.org/cn/v1/query/solr/
```
This will list all of the fields in the index. Then you can make a query using SOLR syntax like this:
```bash
https://cn.dataone.org/cn/v1/query/solr/?q=datasource:*KNB&fl=identifier,datasource,formatType,formatId&rows=20&wt=xml
```
The solr q parameter lists the query criteria in name:value pairs, and the fl parameter lists the fields you want returned.  wt is the serialization format, one of `xml`, `json`, and `csv`.",NA,NA,NA
56,27608694,cboettig,2013-11-01T23:27:24Z,2013-11-01T23:27:24Z,"Very cool

---
Carl Boettiger
http://carlboettiger.info

sent from mobile device; my apologies for any terseness or typos
On Nov 1, 2013 4:24 PM, ""Matt Jones"" <notifications@github.com> wrote:

> Each node can implement different query engines, each with their own query
> syntax. Metacat implements pathquery (which is deprecated) and solr,
> which is preferred. Once you choose an engine, you can get a list of the
> fields it supports by:
>
> https://cn.dataone.org/cn/v1/query/solr/
>
> This will list all of the fields in the index. Then you can make a query
> using SOLR syntax like this:
>
> https://cn.dataone.org/cn/v1/query/solr/?q=datasource:*KNB&fl=identifier,datasource,formatType,formatId&rows=20&wt=xml
>
> The solr q parameter lists the query criteria in name:value pairs, and the
> fl parameter lists the fields you want returned. wt is the serialization
> format, one of xml, json, and csv.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/56#issuecomment-27608560>
> .
>",NA,NA,NA
56,27644556,cboettig,2013-11-03T13:27:43Z,2013-11-03T13:27:43Z,"Hmm,  https://cn.dataone.org/cn/v1/query/solr/ seems to include documentation of actual datasets rather than explaining the solr query options?  

This page: https://mule1.dataone.org/ArchitectureDocs-current/design/SearchMetadata.html#attribute-descriptions-and-notes seems to give some possible queries. Seems like I can query on ""values extracted from Science Metadata"" listed there (such as `eastBoundCoord`, values derived from the resource map, or system metadata)

- [ ] Can I query multiple fields?  Trying to this way didn't work: 

https://cn.dataone.org/cn/v1/query/solr/?q=identifier:*doi,author:*Wolkovich&fl=identifier,formatType,authorLastName&rows=60&wt=xml

------------------

As far as this issue goes, querying for me is now blank: 

https://cn.dataone.org/cn/v1/query/solr/?q=authorLastName:Boettiger&fl=identifier,authorLastName&rows=60&wt=xml 

so we can close this issue?  
",NA,NA,NA
53,27650601,cboettig,2013-11-03T18:32:12Z,2013-11-03T18:32:12Z,"This Issue is a duplicate of #15, so closing now.  ",NA,NA,NA
15,27650720,cboettig,2013-11-03T18:37:14Z,2013-11-03T18:37:14Z,"- [ ] Conversion from Biological Data Profile to EML:  Added stylesheet: bdp2eml.xsl, yet not tested.  
- [ ] Conversion from EML to ISO19139: Stylesheet added.  Conversion working with EML 2.1.0.  reml extended to optionally write in this legacy format, but ideally the stylesheet should be updated to acknowledge 2.1.1. 

```coffee
library(Sxslt)
a = xsltApplyStyleSheet(""inst/examples/hf205.xml"", ""inst/xsl/eml2iso19139.xsl"")
```

- [ ] Conversion from EML to BDP: added eml2tonbii.xsl, not working with `Sxslt` library yet.  (xalan issue, see #53).  

- [ ] Conversion from ESRI to EML: not tested.  ",NA,NA,NA
55,27657715,cboettig,2013-11-03T23:25:48Z,2013-11-03T23:25:48Z,"The test illustrates this working nicely, using the remote download. For fun, the test also shows an access call to the original EML using the DataONE REST API via the canonical identifier (which should be more stable), rather than the Harvard Forest url.  


```coffee
  ## Create a subdirectory to make sure we can read when data is not in the working directory
  dir.create(""tmp"")

  ## Let's get the example dataset from the DataONE REST API just for fun
  require(httr)
  id <- ""knb-lter-hfr.205.4""
  base <- ""https://cn.dataone.org/cn/v1""
  url <- paste(base, ""object"", id, sep=""/"")
  file <- content(GET(url))

  ## Write the file out again just so we can test our reading local paths
  require(XML)
  saveXML(file, ""tmp/hf205.xml"")

  ## Here's the actual test
  met <- eml_read(""tmp/hf205.xml"")
  dat <- dataTable(met) # Includes call to extract that must download the CSV file from the address given in the EML
  expect_is(dat, ""data.frame"")
```
",NA,NA,NA
56,27658020,cboettig,2013-11-03T23:39:24Z,2013-11-03T23:39:24Z,"@mbjones two more questions in addition to the little ones above.  In general it makes sense to archive (remove from search index) rather than delete data (for provenance/historical preservation).  But I do see that there exists a [delete method](http://mule1.dataone.org/ArchitectureDocs-current/apis/MN_APIs.html#MNStorage.delete) as well, which is also ""tier 3"".  What then is the use case for the `delete` method?  

Related, I'm not quite clear on the rules for the base URL.  Do I need the ""Authoritative Node"" url, such as https://knb.ecoinformatics.org/knb/d1/mn/v1/,  for all authenticated operations (>= Tier 2?)  Or just for some, like archiving?  Likewise, could you explain the mapping between the base URL and the two arguments of `D1Client`? ",NA,NA,NA
56,27660044,cboettig,2013-11-04T01:15:07Z,2013-11-04T01:15:07Z,"@mbjones sorry, I see you already answered my questions via the links in #20 

I see I can find out the URLs for all the testing levels from the links you provide: 

-    Production: https://cn.dataone.org/cn/v1/node
-    Staging: https://cn-stage.test.dataone.org/cn/v1/node
-    Sandbox: https://cn-sandbox.test.dataone.org/cn/v1/node
-    Dev: https://cn-dev.test.dataone.org/cn/v1/node

Also I see the documentation does mention that `delete` is really for the central node to manage replicates.  It seems like the kind of thing I shouldn't have the authentication to do, so I was surprised to see it in ""Tier 3"", but that may be me just reading in to much into the tier designation.  

I also see that for multiple queries, etc, I can just consult the generic `solr` documentation, e.g. I can add ""filter queries"" to get all the objects that have dois by Wolkovich with something like: 

https://cn.dataone.org/cn/v1/query/solr/?q=identifier:*doi&fq=author:*Wolkovich&fl=identifier,formatType,authorLastName&rows=60&wt=xml

Very cool.  Anyway, assuming I've deleted all the test data.  Closing this issue (as it has become my tangent for understanding the dataone REST API anyway...)",NA,NA,NA
20,27665259,cboettig,2013-11-04T05:31:16Z,2013-11-04T05:31:16Z,I've created a little R repository to play with the REST API further: https://github.com/ropensci/rdataone/issues/1,NA,NA,NA
15,27825728,mbjones,2013-11-05T23:51:18Z,2013-11-05T23:51:18Z,"@cboettig Note that we often have the issue of needing to convert from older versions of EML to newer ones.  In Morpho, we've decided to always serialize the newest version, but to be able to read all of them.  If a user wants to edit an EML document, we force them to forward-convert to the latest EML version, which reduces our maintenance load and makes it easier for tools to work together.  To do that, we have XSLT stylesheets to convert from one version of EML to another:

https://code.ecoinformatics.org/code/eml/trunk/style/eml201to210.xsl
https://code.ecoinformatics.org/code/eml/trunk/style/eml201to211.xsl
https://code.ecoinformatics.org/code/eml/trunk/style/eml210to211.xsl

You may find these useful. 
",NA,NA,NA
15,27841147,cboettig,2013-11-06T03:56:25Z,2013-11-06T03:56:25Z,"Awesome.  Given that reml / our S4 structure is built on 2.1.1, would you recommend that `eml_read` attempt to detect the version and apply the conversion into 2.1.1 if necessary/possible before parsing?  Or is that likely to introduce more headache than gain?  In my limited experiments I haven't had trouble reading 2.1.0 into reml, which currently doesn't have the full 2.1.1 schema implemented yet anyhow...",NA,NA,NA
20,28266947,cboettig,2013-11-12T04:25:10Z,2013-11-12T04:25:10Z,"Identifiers are generated for both external csv data and the eml, and `eml_read` can access the data based on these identifiers.  We may eventually revisit this issue if we move to the REST-based KNB implementation and end up needing to call the functions slightly differently, but the top-level api for `publish_eml` with KNB as the destination repository will probably remain unchanged, so I think we can now close this.  ",NA,NA,NA
23,28267254,cboettig,2013-11-12T04:34:35Z,2013-11-12T04:34:35Z,"reml currently generates a uuid identifier when writing out the XML.  When publishing to the KNB, this identifier is used by default, and currently we don't have a mechanism in the API to request a DOI.  (Likewise, when publishing to `figshare` the identifier in `packageId` remains the one generated and the DOI isn't added.  Not sure what we should do when a user writes out an EML document and it receives a uuid for its packageId, and then is given a DOI on upload. 

- [ ] Presumably we keep both identifiers?  How do we structure that in the EML (both for the metadata file / packageId, and for the csv file)?  Is there something that will indicate that the DOI is the canonical id?  

- [ ] What should the workflow to request a DOI look like?  (In figshare, this happens whenever a document is declared public.  Presumably we don't want to enforce that behavior for KNB though, so I suppose we need a different option for getting a DOI through the KNB?  Unsure how best to do this while keeping a consistent API for uploading to the different repositories (as @mbjones advises above).  ",NA,NA,NA
10,28267417,cboettig,2013-11-12T04:39:16Z,2013-11-12T04:39:16Z,"Anyone ( @mbjones @duncantl @all)  have advice on a good strategy to cache package settings between R sessions? 

We want a user to be able to enter their contact information once and have R remember for future use.  We could write this just to the working directory, which has advantages (e.g. could configure different working directories differently), or we could write to some global scratch.  My intuition would be to create a hidden/dotfile in the user's home directory; not sure if that would work for windows users, etc?

",NA,NA,NA
10,28267531,karthik,2013-11-12T04:43:30Z,2013-11-12T04:44:01Z,"Isn’t this exactly what we’d use the .rprofile for? For e.g.  I keep this in mine for `devtools` package creation:

```
options(devtools.desc.author = ""'Karthik Ram <karthik.ram@gmail.com> [aut, cre]'"")
options(devtools.desc.license = ""CC0”)
```

Can’t we do something similar?



 ",NA,NA,NA
10,28268714,cboettig,2013-11-12T05:09:33Z,2013-11-12T05:09:33Z,"@karthik Sorry, I should have been clearer -- was thinking about an interactive cache of R objects.  Typically we've done that by asking a user to edit `.rprofile` by hand.  Sure, we could `writeLines()` to .rprofile, but that is poor style, since this could potentially be a large chunk of information (okay, not that big) that isn't needed by every R session.  I'd rather load a cache only when `reml` is loaded.  

Thinking this out a bit more, I'm leaning towards doing away with saved configurations as I think it is more likely to create errors and not be flexible enough.  For instance, consider the use-case of coverage metadata.  While it would be annoying to have to specify the complete geographic coverage for a site you work on all the time, you don't exactly want that coverage metadata be loaded automatically either.  You'd rather write the coverage to a file once, and then just copy it over when you need it later, e.g.


```coffee
eml <- read.eml(""my_earlier_eml.xml"")
sagehen <- geographic_coverage(eml) 
karthik <- creator(eml)
```

The user that wanted to save the eml element for later reuse in the standard way (e.g .`save(sagehen, ""sagehen.rda"")`), or just extract it from an eml as above.  Then it can be written into future EML files explicitly but without going through the tedium of constructing the geographic coverage node, e.g. 

```coffee
write.eml(dat, ..., creator = karthik, geographic_coverage = sagehen)
```

In general this assigning variable names to reused elements is probably more natural and flexible than trying to remember previous values and re-using them automatically.  Sound good?  

",NA,NA,NA
10,28336156,cboettig,2013-11-12T21:40:35Z,2013-11-12T21:40:35Z,"Working along these lines, I think we'll drop `eml_config` and illustrate element reuse as above instead. This would better fit the use-case of groups like Harvard Forest, who currently generate EML using a mix of a generic XML editor (Oxygen) for metadata above the `entity` level and Morpho for attribute details.  

All arguments to `eml_write` take either text-based arguments or existing S4 objects as shown above.  See examples in #48.  ",NA,NA,NA
48,28337907,cboettig,2013-11-12T22:00:08Z,2013-11-12T22:00:08Z,"

### Writing coverage nodes

A coverage node for the dataset can be generated either using the constructor function `eml_coverage`, for commonly specified formats, or using the richer but lower-level constructors, `new(""coverage"", ...)`, `new(""geographicCoverage, ...)`, etc.  `eml_write` technically takes the S4 object as an argument, though users may find it easier to: 

1. call the constructor inline in the function call: 

```coffee
eml_write(dat, metadat, 
               coverage = 
               eml_coverage(""Sarracenia purpurea"", 
                                     dates = c(""2012-06-01"", ""2012-06-01""),
                                     geographic_description = ""Harvard Forest Greenhouse, Tom Swamp Tract (Harvard Forest)"",
                                     c(north=42.55,  south=42.42 , east=-72.10, west=-72.29, min_alt=160, max_alt = 330)))
```

2. or: the same could be accomplished, along with more complex definitions allowed in EML, with the manual constructors.  For instance, generating the same temporalCoverage node above can be done by calling the constructor, and then writing into the desired slots:

```coffee
temporal = new(""temporalCoverage"")
temporal@rangeOfDates@beginDate@calendarDate = ""2012-06-01""
temporal@rangeOfDates@endDate@calendarDate = ""2012-06-01""
```

We can then use `slotNames` to see what other options are available.  For instance, `slotNames(temporal@rangeOfDates@beginDates)` shows us that we can have a ""time"" as well as a ""calendarDate"".  This is more flexible, but requires a bit more exploring or familiarity with S4 structures than using the simple `eml_coverage` function shown above (along with it's associated documentation).  _Not sure if it is worth extending our `eml_coverage` function to allow a user to construct arbitrary coverage nodes without needing `@` references_


3. We could also extract a coverage node from an existing file as shown below, and simply pass that to `write_eml` to reuse it in a new file.  

### Extract coverage nodes

Not sure what the preferred format for extracting coverage metadata from an EML file is.  Currently, we have an accessor method to extract coverage node:

```coffee
eml <- eml_read(""my_eml.xml"") 
coverage <- coverage(eml)
```

This is equivalent to `eml@dataset@coverage`, so it is really just a convenience function.  We do define a pretty-print method based on yaml, showing only non-empty fields:

```coffee
> coverage
coverage(eml)
geographicCoverage:
  geographicDescription: Harvard Forest Greenhouse, Tom Swamp Tract (Harvard Forest)
  boundingCoordinates:
    westBoundingCoordinate: '-72.29'
    eastBoundingCoordinate: '-72.1'
    northBoundingCoordinate: '42.55'
    southBoundingCoordinate: '42.42'
    boundingAltitudes:
      altitudeMinimum: '160'
      altitudeMaximum: '330'
      altitudeUnits: meter
temporalCoverage:
  rangeOfDates:
    beginDate:
      calendarDate: '2012-06-01'
    endDate:
      calendarDate: '2013-12-31'
taxonomicCoverage:
  taxonomicClassification:
    taxonRankName: genus
    taxonRankValue: Sarracenia
    taxonomicClassification:
      taxonRankName: species
      taxonRankValue: purpurea 

```

Because it may also be useful to have a list format used by `eml_config`, currently this coverage element can simply be coerced into that list format: 

```coffee
> as(coverage, ""list"")
$scientific_names
[1] ""Sarracenia purpurea""

$dates
[1] ""2012-06-01"" ""2012-06-01""

$geographic_description
[1] ""Harvard Forest Greenhouse, Tom Swamp Tract (Harvard Forest)""

$NSEWbox
  north   south    east    west min_alt max_alt 
  42.55   42.42  -72.10  -72.29  160.00  330.00 

```

Note that this is not a generic list conversion, but rather always into these four summary elements for convenience. Users wanting the full structure should probably subset from the S4 class directly.  These are the very same short-hand arguments used by the constructor function `eml_coverage`, e.g. one can do: 

```coffee
cov_list <- as(coverage, ""list"")
S4 <- do.call(eml_coverage, cov_list)
```

which returns the original S4 version of the coverage element.  

Open for feedback on interface choices here.  I'm always divided on how to write these helper functions so that they are intuitive but still flexible, instead of making the user manually navigate the data structure with all the `@`s.  
",NA,NA,NA
15,29182613,mbjones,2013-11-25T07:45:30Z,2013-11-25T07:45:30Z,"You'll probably find that 2.0.x versions are essentially compatible, and 2.1.x versions are essentially compatible (only requiring a namespace change to the newer namespace), but that moving from 2.0.x to 2.1.x will require document transformations and may result in validation errors in some cases where the schema became more strict.  An overview of the differences is available here:
  http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-210info.html
",NA,NA,NA
58,29661924,mbjones,2013-12-02T21:56:14Z,2013-12-02T21:56:14Z,Me too.  I started doing this for the `dataone` package as I refactor it -- its a simple change.,NA,NA,NA
58,29661993,karthik,2013-12-02T21:57:03Z,2013-12-02T21:57:03Z,:+1: ,NA,NA,NA
58,29662155,sckott,2013-12-02T21:58:25Z,2013-12-02T21:58:25Z,See also related in `RNeXML` https://github.com/ropensci/RNeXML/issues/47,NA,NA,NA
50,29671049,cboettig,2013-12-02T23:56:37Z,2013-12-02T23:56:37Z,"additionalMetadata now takes only XMLInternalElementNode class content.  The above example becomes: 


```coffee
require(XML)
rdfa <- newXMLNode(""subject"", 
                        attrs=c(about=""#1838""), 
                        namespaceDefinitions = c(o=""http:/oboe-core#""), 
                        .children = list(
             newXMLNode(""meta"", attrs = c(property=""o:entity"", content=""Air"", datatype=""xsd:string"")),
             newXMLNode(""meta"", attrs = c(property=""o:characteristic"", content=""Temperature"", datatype=""xsd:string"")),
             newXMLNode(""meta"", attrs = c(property=""o:unit"", content=""Celsius"", datatype=""xsd:string""))
            ))

eml <- eml(dat, creator=""Carl Boettiger <cboettig@ropensci.org>"", additionalMetadata = new(""additionalMetadata"", metadata = rdfa))
```


",NA,NA,NA
59,29733057,cboettig,2013-12-03T17:45:50Z,2013-12-03T17:45:50Z,"@cpfaff Indeed there are.  Given an XML schema, in principle we can programmatically construct the S4 classes and methods for serializing and parsing that we are instead writing by hand in this repo.  This is the goal of Duncan's [XMLSchema](https://github.com/omegahat/XMLSchema) package.  (A similar concept is possible for SOAP-based APIs, which provide an internet exchange protocol in addition, see the [SSOAP package](https://github.com/omegahat/SSOAP)).  

The package works on simple examples, but xml schemas like EML can get pretty complex so it isn't working out of the box for EML (see #34), hence we're writing the schemas out by hand.  There are of course some advantages to hand-crafting, and at any rate the user-end functions still need to be written.

There are a handful of XML Schemas that are more directly related to EML and indeed are frequently used for representing biodiversity information.  For these schema, the Ecoinformatics working groups from NCEAS and collaborators have already written XSLT stylesheets, which can be used to convert between different XML schema. In `reml` we provide the ability to convert between EML and some of these other schema by wrapping an R function around these stylesheets, see #15 & #53. ",NA,NA,NA
33,29740169,cboettig,2013-12-03T19:02:43Z,2013-12-03T19:02:43Z,"Closing this issue since SOLR queries are part of `dataone` and external to `reml`.  

We can nonetheless include a sample solr query in the example documentation showing uploading to the KNB, illustrating that the EML metadata submitted becomes a query-able part of the dataone general index.  After all, that fact alone is a huge advantage of using EML (+KNB) and important to bring home to folks. (vs using any other metadata standard or depositing on Dryad or Figshare where only much more minimal metadata is exposed).  

",NA,NA,NA
60,29740766,emhart,2013-12-03T19:09:04Z,2013-12-03T19:09:04Z,Is the idea here that we embed the semantics in the meta tags?  Or would be be able reference the semantics elsewhere in a separate file?,NA,NA,NA
60,29744867,cboettig,2013-12-03T19:52:50Z,2013-12-03T20:38:26Z,"@emhart Both.  The above gets embedded in the EML in an `additionalMetadata` element.  Then we can parse it as XML content when we `read.eml`, or we can pipe the whole darned EML file through an RDFa distiller, and out will come an RDF version of this metadata (in whatever format we want: N3, turtle, etc, but we'll use RDF-XML to illustrate).  We can then explore that with whatever semantic tools we have handy to chew on RDF.  For instance, I illustrate both XML parsing and RDF SPARQL queries on a minimal EML file in this example/test:

https://github.com/ropensci/reml/blob/0ac91203026779f3a89d5cc42470faaab879a82a/inst/tests/test_semantics.R
(**updated link**, fixed first xpath query)

enjoy!",NA,NA,NA
60,29744983,mbjones,2013-12-03T19:54:05Z,2013-12-03T19:54:05Z,"@emhart The semantics are implied due to the namespace association.  The ""o:"" prefix is linked to the OBOE namespace, which defines the semantics of the properties.

@cboettig Regarding your second point, we have an XSLT as part of Metacat that can do a minimal EML->RDF translation, which was used in some early work on supporting LSIDs.  Its minimal, but a decent starting point.
",NA,NA,NA
60,29745093,mbjones,2013-12-03T19:55:11Z,2013-12-03T19:55:11Z,"@emhart After re-reading your comment, I think I misinterpreted your question.",NA,NA,NA
60,29746590,cboettig,2013-12-03T20:12:48Z,2013-12-03T20:12:48Z,"@mbjones awesome.  yeah, minimal is fine, it might be a nice proof-of-concept to include along with our other stylesheets and see what users find most useful. Can you link me to the XSLT?  With an increasing number of resources being available in RDF that particular case becomes more compelling.  Doesn't dataone have an associated triplestore?  At this stage I imagine the SOLR queries are more useful, but who knows.  

Also, @emhart might have mentioned to you that he and I have had some discussions about his work at NEON and in providing EML for some of their data products.  It sounds like a semantically enhanced EML could be particularly promising in that case.

I'm still getting my head wrapped around SPARQL queries but the ability to do these from R, as in my example linked above, is a nice touch.  
",NA,NA,NA
61,29773067,mbjones,2013-12-04T02:38:52Z,2013-12-04T02:38:52Z,"It's a recursive schema, its completely legal, and if you are building elements by parsing the schema, the parser has to be smart enough to notice that it's recursive and employ a stopping rule.  I'm not sure how to handle it in R -- does S4 not support recursion?",NA,NA,NA
61,29773411,cboettig,2013-12-04T02:44:54Z,2013-12-04T02:44:54Z,"@mbjones Perhaps it's not a problem, at least when the class definitions are separated by one class (as is the case with protocols).  R will warn if a class is used in representation/slots definition before it is defined, but I guess we might just be able to ignore such a warning.  The taxonomic classification is recursive already: see https://github.com/ropensci/reml/blob/master/R/taxonomicCoverage.R#L41-L45 and I guess that works.  Just wanted to put in a note in case this was potentially problematic for R or I had misunderstood the schema.  

@duncantl Are there any gotcha's to worry about with S4 classes containing themselves as slot types; e.g. [like this](https://github.com/ropensci/reml/blob/master/R/taxonomicCoverage.R#L41-L45) ?",NA,NA,NA
60,29778441,mbjones,2013-12-04T04:22:34Z,2013-12-04T04:22:34Z,"@cboettig As I said, its rudimentary, and is no longer maintained, so its out of date wrt the current EML version.  But you can find the XSLT here:
  https://code.ecoinformatics.org/code/metacat/trunk/lib/lsid_conf/eml-2.0.1.xslt

",NA,NA,NA
27,29922811,cpfaff,2013-12-05T18:18:51Z,2013-12-05T18:18:51Z,"I am dealing with the literature module at the moment and stumbeled upon somthing where I need input. The edited book citation type equals to the book citation type with the difference that there are some editors that edited the book whreas the chapters can have different authorships. The documentation says that the editors need to go into the ""creators"" field. But the book citation type does not have a ""creators"" field. Do I miss something?",NA,NA,NA
27,29924024,cboettig,2013-12-05T18:33:42Z,2013-12-05T18:33:42Z,"@cpfaff Every citation type inherits the `ResourceGroup`, which provides `creator` (and title, and lots of other stuff)

For an `editedBook`, you would probably put the creator `responsibleParty` elements with different `positionName`, either ""editor"" or ""author"" accordingly.  

I wouldn't worry about that at this stage.  We first want to create the S4 class equivalent of the schema.  For that purpose, editedBook should just inherit the book class:

```coffee
setClass(""editedBook"", contains=""book"")
```

And make sure `book` contains `resourceGroup`, as well as it's unique slots.  Point me to your code soon so I can give some feedback before you get too deep.  


Once we have the S4 classes in place, we will write methods that map other citation objects into these types.  For instance, R already has a native `citation` class (based on Bibtex).  It won't map perfectly into EML, but we'll do our best to make a reasonable mapping.  (That stage is where we actually will worry about where we put editors vs authors, etc)
",NA,NA,NA
27,29930303,cpfaff,2013-12-05T19:40:36Z,2013-12-05T19:40:36Z,@cboettig Ok thanks that sounds good. [Here](https://github.com/cpfaff/reml-1/blob/master/R/literature.R) is what I did today. Not much but a beginning. ,NA,NA,NA
27,29932626,cboettig,2013-12-05T20:04:19Z,2013-12-05T20:04:19Z,"@cpfaff Nice work, thanks for the link. 

You may know this already, but you'll want all of the citation classes to inherit `resourceGroup`. This is a bit annoying as the `resourceGroup` slots should be listed before the type-specific slots, since order matters in the schema.  e.g. you might think you can just add `""contains=""resourceGroup""` to the class definition, but this gets the wrong ordering, with those slots coming last (e.g. in the order given by `slotNames(new(""article""))`.  So you have to do something like this:  

```coffee
    setClass(""A"", slots = c(slot1 = ""character"", slot2=""character""))
    setClass(""B_slots"", slots = c(Bslot1 = ""character"", Bslot2= ""character""))
    setClass(""B"", contains =c(""A"", ""B""))
```

e.g. for article: 


```coffee
setClass(""article_slots"",
         slots = c(journal = ""journal"",
                   volume = ""volume"",
                   issue = ""issue"",
                   pageRange = ""pageRange"",
                   publisher = ""publisher"",
                   publicationPlace = ""publicationPlace"",
                   ISSN = ""ISSN""))
setClass(""article"", contains = c(""resourceGroup"", ""article_slots"")
```

not super elegant, I know.  I'm looking for a better way to handle ordering but this is pretty simple.  



",NA,NA,NA
62,29939717,emhart,2013-12-05T21:26:10Z,2013-12-05T23:23:43Z,"I have the bandwidth to take this on if you want @cboettig.  We have a ton of metadata lodged in our ATBD (Algorithm Theoretical Basis Documents) word  files.  I would love a way to get info out, beyond just methods without someone copying and pasting.  ",NA,NA,NA
62,29940701,cboettig,2013-12-05T21:37:14Z,2013-12-05T21:37:14Z,"@emhart that would be awesome.  I've just figured out how to get the text:

````coffee
library(RWordXML)
f <- wordDoc(""inst/examples/methods.docx"")
doc <- methods[[getDocument(f)]]
xpathSApply(doc, ""//w:t"", xmlValue)
```


Way cool.  

Yeah, I think there's a real opportunity here.  Ideally we could extract other metadata from Word XML too.  For instance, Harvard Forest actually requires their users to submit a metadata in a Word template, which some poor bloke converts by hand to EML using a combination of Morpho and a commercial XML editing program (Oxygen). Ideally we could get all the metadata from their template, not just methods text as I pose in this issue.  

For instance: 

```coffee
creator(f)
```

will tell you I'm the creator of that word docx.  ",NA,NA,NA
27,29972285,cpfaff,2013-12-06T08:47:40Z,2013-12-06T08:47:40Z,Ok I see. Thanks!,NA,NA,NA
27,29977549,cpfaff,2013-12-06T10:16:21Z,2013-12-06T10:19:24Z,"What to do best in the following case. Publisher in article comes from responsibleParty. Should I do it this way
where I include responsible party as inheritance:

```coffee
setClass(""article_slots"",
         slots = c(journal = ""journal"",
                   volume = ""volume"",
                   issue = ""issue"",
                   pageRange = ""pageRange"",
                   publicationPlace = ""publicationPlace"",
                   ISSN = ""ISSN"")
         )

setClass(""article"",
         contains = c(""resourceGroup"",
                            ""responsibleParty""
                            ""article_slots"")
         )
```

Or rather put the responsibleParty class in place for publisher as publisher is no where else mentioned and the slots needs to be named like this as otherwise we miss it.


```coffee
setClass(""article_slots"",
         slots = c(journal = ""journal"",
                   volume = ""volume"",
                   issue = ""issue"",
                   pageRange = ""pageRange"",
                   publisher = ""responsibleParty"", 
                   publicationPlace = ""publicationPlace"",
                   ISSN = ""ISSN"")
         )

setClass(""article"",
         contains = c(""resourceGroup"",
                            ""article_slots"")
         )
```

I would preferably choose the second one but I am not exactly sure",NA,NA,NA
27,30010509,cboettig,2013-12-06T17:03:02Z,2013-12-06T17:03:02Z,"@cpfaff I think you're overthinking this actually.  

You want the slot names to match exactly with the elements, so article must contain a slot `publisher` of class `publisher`. That class should then simply `responsibleParty` (and nothing else).    You'll see that in fact I've already defined the `publisher` class in this way, because it is used elsewhere: https://github.com/ropensci/reml/blob/master/R/dataset.R#L5  (yeah, dataset.R is not a good home for this, but I just put it there because I first needed this class while writing the dataset slots.)

Does that make sense?

Your second case would work if we took the node name from the parent slot, but we take it from the object itself because that is more modular (we don't need to know the parent).  So, your second case would result in EML that had a `<responsibleParty>` node where it should have a `<publisher>` node.  

It may look silly or verbose to have classes defined that are equivalent to responsibleParty, but it's actually very tidy this way. There are lots of kinds of responsibleParty, but we want them to have there own name. 

",NA,NA,NA
27,30010863,cboettig,2013-12-06T17:07:40Z,2013-12-06T17:07:40Z,"p.s. Officially your classes should all have `referencesGroup` at the end of their inheritance, and have `id_scope` for the attribute data, even though we might not be using these slots:

```coffee
setClass(""article"",
         contains = c(""id_scope"",
                            ""resourceGroup"",
                            ""article_slots"",
                            ""referencesGroup""))
```",NA,NA,NA
27,30019812,cpfaff,2013-12-06T19:02:56Z,2013-12-06T19:02:56Z,Thanks @cboettig yes that makes sense. If this is a good place I do not know at the moment. Maybe there will be a better home later on. But makes sense as you describe it to be where it is at the moment and I can use it from there. Thanks for the explanations. ,NA,NA,NA
60,30025276,cboettig,2013-12-06T20:10:45Z,2013-12-06T20:10:45Z,"Okay, decided I might learn some really basic XSLT by writing a style file to pull some standard dublin core-type terms from /eml/dataset and provide them as RDF. No idea if this implementation would really be best-practice, but fun learning exercise anyhow.  


- [XSL file](https://github.com/ropensci/reml/blob/master/inst/xsl/eml211_to_rdf.xsl)
- [example input](https://github.com/ropensci/reml/blob/master/inst/examples/hf205.xml)
- [example output](https://github.com/ropensci/reml/blob/master/inst/examples/hf205_rdf.xml)

```coffee
require(Sxslt)
infile <- system.file(""examples"", ""hf205.xml"", package = ""reml"")
xsltApplyStyleSheet(infile, ""inst/xsl/eml211_to_rdf.xsl"")
```

- [ ] Not clear how I can get the `dc:` namespace definition to appear in one of the parent nodes, XSLT adds it to each `dc:` prefixed element explicitly.  
- [ ] Not sure why my xsl isn't getting the packageId for the about attribute.  

Could easily be extended and could no doubt be improved upon.",NA,NA,NA
64,30025640,cboettig,2013-12-06T20:15:52Z,2013-12-06T20:15:52Z,"Indeed!  Also, I think a lot of the interface needs updating/extending to be more flexible.  The current way `write.eml` is defined with fixed fields for `title`, `description` etc is too rigid, and will be awkward to extend.  


I've been thinking about an example workflow for writing a complete EML document, including pulling in bigger text sections (e.g. Methods) from MS Word/openoffice.  My working mockup is here:

https://github.com/ropensci/reml/blob/master/inst/examples/hf205.R

based on a real EML file deposited by Harvard Forest.  I would love feedback on what parts look good and what parts look horrendous.  

In general I hope we can maintain a super simple interface where: `write.eml(data.set(dat, col.defs=stuff, unit.defs=stuff))` is sufficient to get a minimally valid EML, while also keeping it easy to add much richer information.  



",NA,NA,NA
60,30027384,mbjones,2013-12-06T20:39:41Z,2013-12-06T20:39:41Z,"FYI, we also have a minimal EML -> DC XSLT that is used to produce DC RDF for our Metacat OAI-PMH implementation.  See https://code.ecoinformatics.org/code/metacat/trunk/lib/oaipmh/

Might be useful to you for comparison.",NA,NA,NA
60,30035105,cboettig,2013-12-06T22:27:36Z,2013-12-06T22:27:36Z,"@mbjones those are beautiful! Yes, nice to how that's done.  I think these stylesheets could be potentially useful to reml users seeing to do some triples extraction and manipulation of a bunch of xml with the `rrdf` package, for instance, so I'll include them in the xsl collection.

Ultimately would be nice to include things beyond the Dublin Core to where we can do more with the semantics (I realize that's outside of the OAI-PMH use case for the XSLT you link, but just as an extension).  For instance, it might be nice to add taxonomicCoverage in terms such as the VTO. Then one could construct a sparql query to say things like ""give me all datasets covering frogs"".  

",NA,NA,NA
63,30036146,cboettig,2013-12-06T22:43:45Z,2013-12-06T22:43:45Z,"yeah, that's a good point.  Of course it's not actually a name collision with an existing package, but could still be potentially confusing.  How about just calling the package `EML` (in the spirit of the `XML` package)? Other suggestions?

@karthik @sckott thoughts? what do you guys do about github URLs when you rename packages?",NA,NA,NA
63,30036312,karthik,2013-12-06T22:46:09Z,2013-12-06T22:46:09Z,"Good question Carl. It can be confusing in some cases since people look for `github.com/ropensci/<package_name>` and can get thrown off. If we do change early on (as in this case), let's change both. 

That said, there is no reml on [CRAN](http://cran.r-project.org/web/packages/reml/index.html)
",NA,NA,NA
63,30036408,karthik,2013-12-06T22:47:37Z,2013-12-06T22:47:37Z,"Additional note: I've brought this up earlier but we should consider stopping the use of a `r` prefix in front of packages. I mean, we develop packages in R. The only situations where this would be important is when a data provider might frown upon use of their brand (e..g Dropbox). A package called gbif sounds official when it is not. ",NA,NA,NA
63,30038319,cboettig,2013-12-06T23:19:39Z,2013-12-06T23:19:39Z,"@karthik valid point.  I agree that the prefix is kinda silly. 

I will note that the ambiguity can be annoying in other contexts though.  For instance, it was much more convenient to write a paper about the rfishbase interface to fishbase than it was the treebase interface to treebase. Fixed-width font and alternative capitalizations only go so far (particularly in spoken language).  Perhaps there is a better way to handle this (e.g. always use the word ""package"" after the package name?)  ",NA,NA,NA
65,30047632,mbjones,2013-12-07T04:39:31Z,2013-12-07T04:39:31Z,"@cboettig Those files are part of the Metacat release, which has been historically released under the GPL license, as indicated  in the file headers. A full copy of the GPL is at the top of the Metacat source tree, and included in all release packages.  For other software we have built, we have moved to using an Apache 2 license because it imposes fewer restrictions and because it includes a patent clause that is useful to opening source code,  I've contemplated re-licensing Metacat via Apache 2 License, but haven't overcome the activation energy to do so as I haven't had a compelling need.  If the GPL is too restrictive for you, let's discuss whether Apache 2 would be a better fit.

I don't think we provide clear instructions on how to cite Metacat.  We should, sorry about being remiss. I'll have to think about that. One stopgap is to cite our original Metacat paper (doi:10.1109/4236.957896), available for download here: http://knb.ecoinformatics.org/distributed-data-ic-final.pdf
",NA,NA,NA
65,30047649,karthik,2013-12-07T04:41:01Z,2013-12-07T04:41:01Z,"> whereas reml currently declares CC0 license.

Regardless of other license restrictions we should remove the CC0 and go with a more appropriate license for the package.",NA,NA,NA
63,30047653,karthik,2013-12-07T04:41:25Z,2013-12-07T04:41:25Z,"> Perhaps there is a better way to handle this (e.g. always use the word ""package"" after the package name?)

Sounds good to me. and makes sense as well.",NA,NA,NA
65,30048198,cboettig,2013-12-07T05:23:44Z,2013-12-07T05:23:44Z,"@mbjones we will definitely cite the Metacat paper in the manuscript (I think I have it in there) and the function documentation.  

@karthik I'm happy to consider a new license for the package overall, but I do not believe that is impossible to distribute the R functions etc under CC0 if we so choose.  R packages distribute data and documentation (vignettes, etc) neither of which is appropriately covered by a GPL or other dedicated software license.  

If we distribute XSL files in inst/xsl, they can likewise appear under a separate license from the code (some packages bother to do this by declaring License: file in the DESCRIPTION and then explain which parts are covered by which terms in the LICENSE file.   ",NA,NA,NA
27,30082893,cpfaff,2013-12-08T14:43:54Z,2013-12-09T08:00:54Z,"I thought it would be a good idea to validate the initialization of the citation types just to make sure the required fields are in place on initialiization of the object. How can I do this best? What I have at the moment is (but
does not work like this): 

```coffee
setClass(""article"",
         contains = c(""id_scope"",
                      ""resourceGroup"",
                      ""article_slots"",
                      ""referencesGroup""
                      ),
         validity = function(object){
               if(all.equal(object@journal, character(0))){ # check that non empty
                     return(""Journal is reqired"")
               }else{TRUE}
         })
```

After that I will do the mapping to bibtex. I thought to do this with one method per each citation type as signature that operates on Rs `toBibtex` and after that create a method that operates on `citation` or `bibentry` for signature `eml`. Depending on the citation type provided in the `eml` the right function with right BibTeX mapping will be called to generate the bibtex representation. But this would require many manual decision on which citation type to call in the citation(eml) method. Is there a more elegant way to do this? Or am I on the right track so far?",NA,NA,NA
63,30086366,cpfaff,2013-12-08T17:36:00Z,2013-12-08T17:36:00Z,+1 for the `EML` idea. ,NA,NA,NA
59,30087805,cpfaff,2013-12-08T18:34:50Z,2013-12-08T18:35:06Z,"OK thanks, that is very intresting. I read it and just forgot to answer. ",NA,NA,NA
27,30110350,cpfaff,2013-12-09T07:39:47Z,2013-12-09T07:50:50Z,"Well ok. The `bibentry` feels more native that you mention in your first post. 

```coffee
setMethod(""bibentry"",
          ""article"",
          function(object, bibtype){
              entry =  bibentry(
                author = object@creator
                title = object@title,
                journal = object@journal,
                year = object@pubDate,
                ....)
              entry

          }
 
          )
```",NA,NA,NA
27,30141907,cboettig,2013-12-09T15:40:35Z,2013-12-09T15:40:35Z,"@cpfaff Nice. Yes, if you map to `bibentry`, we get the mapping to Bibtex for free, as well as other formats. (For instance, we can add a citation by DOI then using the `knitcitations::cite` function.  

You've written `setMethod` above, (creates a function named ""bibentry"" that takes an ""article"" as it's signature), but this looks like a ""Coercion"" method (changes types) so I think it is best written as a `setAs` instead.   Obviously we'd want coercions both ways, from `article` to `bibentry` and the reverse.  Make sense?  

",NA,NA,NA
27,30145669,cpfaff,2013-12-09T16:18:36Z,2013-12-09T16:18:53Z,@cboettig Ok good then I am set up to go on and hopefully finish this within the next few days. What do you think about checking that required fields in bibtypes are in place on initialization? I added a question in the post before. As what is displayed there does not really work and I am not sure why it now works like this. Maybe it is completely senseless to to this checking but if it is good to have there I would like to know how to best do this.,NA,NA,NA
27,30149143,cboettig,2013-12-09T16:54:20Z,2013-12-09T16:54:20Z,"@cpfaff yeah I meant to comment on the validation step.  I haven't taken a
close look at your validation code, but in general I do not think it is
necessary for us to write validation methods, since:

a) We can already validate against the schema itself using eml_validate(),
so there is no need to also validate the S4
b) While validation is important and a great strength of using EML, it
doesn't really make sense from the user's perspective to have the function
fail because they are trying to read in some EML that isn't technically
valid (e.g. missing some required field, etc).  In such cases, it seems the
software should try and do it's best with what it has (maybe with a
warning) rather than fail anyhow.

For more discussion of this, see past issues:
https://github.com/ropensci/reml/issues/7 and
https://github.com/ropensci/reml/issues/46 and feel free to weigh in on
those threads.


On Mon, Dec 9, 2013 at 8:18 AM, cpfaff <notifications@github.com> wrote:

> @cboettig <https://github.com/cboettig> Ok good then I am set up to go on
> and hopefully finish this within the next few days. What do you think about
> about checking that required fields in bibtypes are in place on
> initialization? I added a question in the post before. As what is displayed
> there does not really work and I am not sure why it now works like this.
> Maybe it is completely senseless to to this checking but if it is good to
> have there I would like to know how to best do this.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/27#issuecomment-30145669>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
70,30150593,cboettig,2013-12-09T17:08:46Z,2013-12-09T17:08:46Z,"@cpfaff Thanks -- still looking over this but a good start I think.  Some general feedback first:

You have good commit messages and your commit can merge automatically, so nice work.  A few critiques:   While I'm generally a fan of commit early and often, it's a lot more work for me to review a pull request of 13 different commits spanning dozens of files.  Some good goals for pull requests:

- [ ] Try to avoid changing files that are not related to the feature added (whitespace, etc).  If you do change them by accident in your repo you can always revert them back and squash those commits.  
- [ ] Try to limit the number of commits in the request by squashing commits that don't actually change the feature or just fix a typo.  This makes it faster for me to review the changes. 


Do you want to take a go at squashing down your commits and reverting the whitespace changes?  (If our text editors are writing whitespace differently we should resolve that, we don't want every commit to change the whitespace back and forth...)",NA,NA,NA
70,30151886,cpfaff,2013-12-09T17:22:29Z,2013-12-09T17:22:29Z,"@cboettig 

Ok good points. I can tell vim not to remove whitespaces so this will not happen again. Or you all could tell your editors to remove trailing ws. I just have it in place as this unnecessary whitespace drives me nuts somties juming to the end of a line in vim and landing in the blue . ;-)

I have never done the squasing thing but can try this in the future. I am not sure if it is problematic to rewrite the history when I have pushed all this to gitub already. If it is not a problem then I can try this with my changes before you merge it.",NA,NA,NA
63,30155165,cboettig,2013-12-09T17:57:25Z,2013-12-09T17:57:25Z,"All right, unless any objections we will migrate to the package name EML.  I'll wait until we have merged @cpfaff's branch with the literature module and my current `devel` branch to master. Then I'll start the github repo for EML and push my existing branches with complete history there.  

I suggest we keep `github.com/ropensci/reml` for historical reasons/avoid breaking any existing links.  @karthik any suggestions on how best to handle this?  

Ugh, I don't suppose there is an easy way to migrate our issues tracker? ",NA,NA,NA
63,30155406,karthik,2013-12-09T17:59:48Z,2013-12-09T17:59:48Z,Actually I'll suggest that we just rename this repo to `EML`. Right now we haven't advertised the repo much and the almost no one has actually directly installed the package (from the workshops). So we should rename existing repo to `EML`. This will migrate everything including issues and history. Create a new repo called `rEML` and just have a short readme saying it's now been renamed `EML`. ,NA,NA,NA
69,30165532,cboettig,2013-12-09T19:36:47Z,2013-12-09T19:36:47Z,"Now implemented on `devel` branch.  @cpfaff Can you check that you can successfully merge the current `devel` branch with your tree before sending a pull request to master (or just issue your pull request against the `devel` branch)?  

Let me know if you hit any conflicts you want a hand resolving.  The devel branch changes the file organization and avoids all warning messages on install now, but it does move around a lot of stuff.  (It also resolved a few outstanding issues in fixing #68)",NA,NA,NA
67,30166303,cboettig,2013-12-09T19:45:25Z,2013-12-09T19:45:25Z,"Implemented for `dataTable`.  see `grep 'setClass(""ListOf' *.R` for remaining types.  ",NA,NA,NA
70,30174080,cpfaff,2013-12-09T21:15:12Z,2013-12-09T21:15:12Z,Well I find it hard to get this working with squashing. Needs a bit more training with git. ,NA,NA,NA
70,30174742,cboettig,2013-12-09T21:22:34Z,2013-12-09T21:22:34Z,"okay, no problem; it's a big set of commits on which to squash. I can go
ahead and merge this pull request for now, and see if you can get the hang
of squashing or avoiding the non-feature relevant commits next time.


On Mon, Dec 9, 2013 at 1:15 PM, cpfaff <notifications@github.com> wrote:

> Well I find it hard to get this working with squashing. Needs a bit more
> training with git.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/pull/70#issuecomment-30174080>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
70,30175154,cpfaff,2013-12-09T21:27:10Z,2013-12-09T21:27:10Z,Thx.,NA,NA,NA
27,30181404,cboettig,2013-12-09T22:38:35Z,2013-12-09T22:38:35Z,"@cpfaff Okay, integrated. go ahead and pull from master to update your branch.  Had to make a few minor changes: 

- I updated the organization of file types, so I had to update your `@include` directives (which were correct based on the old version, so nice work).  
- You had a few of your classes defined with `Type` as part of the class name, e.g. citation = `citationType`.  Technically that makes sense, but because (as I mentioned earlier) we base the EML node name on the class name instead of the slot name (because that way we can write a node without knowing who it's parent is), we would get the wrong name for the node (e.g. we want `<citation>` not `<citationType>`).  I just dropped the `Type` parts of all these names.  

Anyway, nice work.  Before we get any deeper on the literature module, we'll want some test functions for this.  If you aren't familiar with writing unit tests with `testthat`, read up a bit and check out the tests in `inst/tests`.  (You can run them with, e.g. `test_file(""inst/tests/test_data.set.R"")`, or run (almost*) all of them with `test_dir(""inst/tests"")`.   

Some of the tests use optional libraries you may not have installed. (The functions may try to install these, but no guarantees).  

Once we have a test suite that covers most of the class definitions (read an EML file with these types, write an EML file, and validate the file you write against eml_validate), you'll be in good shape to extend the module futher (bibtex etc).  


* tests whose name doesn't start with `test_` are not run by `test_dir`.  This is handy to exclude tests like knb_test.R, which needs user intervention to get the dataone certificate...",NA,NA,NA
27,30204177,cpfaff,2013-12-10T07:13:00Z,2013-12-10T07:31:55Z,"@cboettig 

Thanks for the integration work and fixing the type in names. I have already a bit experience in writing tests and have all the libraries in place. Tried this with our ""rbefdata"" package and most of the times it worked good. So I will try to set them up before I go deeper into the citation module. Further questions are highly likely to arise in the process. ;-)

> a) We can already validate against the schema itself using eml_validate(), so there is no need to also validate > the S4

Ok I already thought about this, so I will just remove the one consitency check I have (see commit above). But just in case I would like to do this. How would I check consitency in this situation as the example above does not work to check if the slot has an empty string.",NA,NA,NA
27,30209098,cpfaff,2013-12-10T09:03:31Z,2013-12-10T09:12:11Z,Here comes the first questions. There is no citation in place in the eml file that is used to test reading eml. Can I add one manually or do we better use another eml file with citation? Many of the tests fail for me at the moment as something is wrong with my set up I think. For example the `system.file()` call gives me back an empty string and no file path which will make the following stepts fail as well if they require the eml file to be read in. Will have to check this out first before I can start writing tests. ,NA,NA,NA
27,30241153,cboettig,2013-12-10T16:13:41Z,2013-12-10T16:13:41Z,"@cpfaff Right, you'll need to create an example EML file with some citation entries.  You can either find a real example (e.g. on the KNB), or just create one using your module.  

Re: `system.file`, make sure you have already installed the current version of `reml` first? (e.g. `install()` from devtools or `R CMD INSTALL`?)   ",NA,NA,NA
27,30243170,cpfaff,2013-12-10T16:33:29Z,2013-12-10T16:38:00Z,@cboettig Thanks. Meanwhile already a step further. The tests work now and I am playing around with an example file with citation. Added the citation to `eml` class below `dataset` and it nicely reads and writes my citation elements. That is so cool. But I still need to write the tests. ,NA,NA,NA
27,30313357,cpfaff,2013-12-11T11:37:40Z,2013-12-11T11:58:28Z,"I think I am stuck a bit and have a few questions on how to exactly go on with the litrature module besides the testing that still needs to be done. Actually I like the idea of test driven development but find it hard to first write the tests and then the code. So I was just going on a bit further in code that I have something to test. The first question is now on how to best or actually where exactly to integrate the literature module into the rest of the classes. Citations can appear in varios different places and this confuses me a bit. 

The first thing I did was to add the citation on the top most level  in the `eml` class which worked fine as I have written yesterday: 

```coffee
setClass(""eml"",
         slots = c(packageId   = ""character"",
                        system      = ""character"",
                        scope       = ""character"",
                        dataset     = ""dataset"", 
                        citation    = ""citation"",
#                        software    = ""software"",
#                        protocol    = ""protocol"",
                        additionalMetadata = ""ListOfadditionalMetadata"",
                        namespaces = ""character"",
                        dirname = ""character""),
         # slots 'namespaces' and 'dirnames' are for internal use
         # only and not written as XML child elements.
         prototype = prototype(namespaces = eml_namespaces))
```

Now I tried to figure out in which classes to add the `citation = ""citation""` as well, so it can be recognized when we turn the eml into an s4 representation and vice versa, but I am not sure where to look this up best. I did not find this in the `eml` html text documentation of the literature module or maybe I just overlooked it. Maybe I have to look this up in another way like for example derive it from the graphics that are provided in the documentation. If so I am not exactly sure how.",NA,NA,NA
27,30342726,cboettig,2013-12-11T17:39:24Z,2013-12-11T17:39:24Z,"@cpfaff Good question.  `citation` is used throughout the schema. For classes we haven't yet implemented, (e.g. `protocol`), we'll just be able to include it when we write the rest of the class definition (you'll see it in `protocol/proceduralStep/citation`)).  For classes that we have implemented, ideally I would have made some note in class definition where I was unable to complete the definition without the module, but I haven't always done so (particularly in the beginning).   


Yeah, I do a mix of browsing the documentation and browsing the images, but neither is a good way to find all the places the ""CitationType"" is used.  For that, you might just want to grep against the .xsd definitions directly:

```xml
 grep CitationType *.xsd
eml-attribute.xsd:                      <xs:element name=""citation"" type=""cit:CitationType"">
eml-coverage.xsd:            <xs:element name=""timeScaleCitation"" type=""cit:CitationType"" minOccurs=""0"" maxOccurs=""unbounded"">
eml-coverage.xsd:                    <xs:element name=""classificationSystemCitation"" type=""cit:CitationType"">
eml-coverage.xsd:              <xs:element name=""identificationReference"" type=""cit:CitationType"" minOccurs=""0"" maxOccurs=""unbounded"">
eml-literature.xsd:  <xs:element name=""citation"" type=""CitationType"">
eml-literature.xsd:  <xs:complexType name=""CitationType"">
eml-methods.xsd:            <xs:element name=""citation"" type=""cit:CitationType"" minOccurs=""0"" maxOccurs=""unbounded"">
eml-methods.xsd:          <xs:element name=""citation"" type=""cit:CitationType"">
eml-physical.xsd:                    <xs:element name=""citation"" type=""cit:CitationType"" minOccurs=""0"">
eml-project.xsd:                    <xs:element name=""citation"" type=""cit:CitationType"" minOccurs=""0"" maxOccurs=""unbounded"">
eml-project.xsd:              <xs:element name=""citation"" type=""cit:CitationType"" minOccurs=""0"">
eml-project.xsd:              <xs:element name=""citation"" type=""cit:CitationType"" minOccurs=""0"">
eml.xsd:          <xs:element name=""citation"" type=""cit:CitationType"">
```

And we see everywhere it is used.  Notice that sometimes the element name is `citation` while elsewhere we have to define a new class because the element name is actually something like `identificationReference`.  So that's probably the best answer to your question.  



You can search for a list of places where `citation` has already been used, e.g. at the command line in the R directory:

    grep '""citation""' *.R

shows:

```
cboettig@strata:~/Documents/code/ropensci/reml/R$ grep '""citation""' *
coverage.R:         slots = c(classificationSystemCitation = ""citation"",
coverage.R:                        identificationReference = ""citation"",
eml.R:#                        citation    = ""citation"",
literature.R:setClass(""citation"",
literature.R:         slots = c(citation = ""citation"")
literature.R:# setMethod(""citation"", ""eml"",
literature.R:         # slots = c(classificationSystemCitation = ""citation"",
```

Here we see that I've used it in `coverage`, and also that I've made a mistake -- I should have defined corresponding classes that inherit `citation` (recall class names determine element names).  This will create a node named `citation` when we want a node named `identificationReference`, e.g.:

```coffee
t <- new(""taxonomicSystem"")
t@identificationReference@article@title = ""the title""  # So that the entry isn't empty
> reml:::S4Toeml(t)
  <taxonomicSystem>
    <citation>
      <article scope=""document"">
        <title>the title</title>
      </article>
    </citation>
  </taxonomicSystem>
```

So that needs to be fixed.   Note in testing this I saw a few errors you'll want to address:

- [ ] You haven't defined the `XMLElementNode` coercions methods for `citation` class itself.  (Pull from master because I just added this case).  

- [ ] You've copied a mistake I made earlier in a few places:  You've defined both coersions to use `emlToS4`.   When going from the S4 class to EML, we actually want the method S4Toeml.  (This error wasn't actually causing any mistakes because I wasn't calling the coercion method in the recursion, I was calling S4Toeml directly.  I've now fixed this to rely on the defined coercion method though).  

So, your coercions should look like:

```coffee
setAs(""article"",
      ""XMLInternalElementNode"",
      function(from) S4Toeml(from)
      )

setAs(""XMLInternalElementNode"",
      ""article"",
      function(from) emlToS4(from)
      )
```

Again, I've fixed `article` but not the rest, so pull from master and then send me a pull request with these fixed up. Sorry for the confusion.  





",NA,NA,NA
27,30343878,cpfaff,2013-12-11T17:53:33Z,2013-12-11T19:16:44Z,Thanks very much for the detailed answer which was quite helpful. The grepping against the .xsd is a good idea. I noticed the mistake already myself and fixed it for all coercions in the module yesterday in the evening. I also already had the coercions for `citation` in place but I did not pushed the commits yet. Will pull the master and resolve the conflicts if any before I send you a small pull request.,NA,NA,NA
72,30346539,cpfaff,2013-12-11T18:21:42Z,2013-12-11T18:21:42Z,@cboettig Already joined the list quite a while ago but thanks for the heads up to the thread on this as I did not follow the discussions regulary.,NA,NA,NA
73,30348353,cpfaff,2013-12-11T18:34:35Z,2013-12-11T18:34:35Z,Sorry for this very unclean commit message. Was the first squash next one will be better.,NA,NA,NA
73,30349077,cboettig,2013-12-11T18:40:23Z,2013-12-11T18:40:23Z,"@cpfaff Thanks; and good work on the squash, that was a much better pull request.  It's much easier to review changes to just 2 files over just 1 commit.  I added my own commit message to the merge commit anyhow.  ",NA,NA,NA
27,30358723,cpfaff,2013-12-11T20:11:24Z,2013-12-11T20:30:10Z,"Ok seems that code boxes do not work in commit messages in reference thread, bummer. That would be so cool.


",NA,NA,NA
74,30364121,cboettig,2013-12-11T21:08:26Z,2013-12-11T21:08:26Z,"@cpfaff The author case is actually easier than you might think because we already have the coercion methods defined to go from ListOfcreator to R's `person` type.  Check this out: 

```coffee
eml <- eml_read(system.file(""examples"", ""hf205.xml"", package=""reml""))
cre <- eml@dataset@creator  # a ListOfcreator S4 object
bibentry(bibtype='article', author=as(cre, ""person""), title = ""test"", journal=""test"", year=""2013"")
```

so just add `as(eml@dataset@creator, ""person"")` and that will handle getting all the authors full names correctly into the bibentry type.  (The reverse coercion will likewise be handy when you write the method to go from bibentry to `citation` class).


",NA,NA,NA
74,30364791,cpfaff,2013-12-11T21:16:43Z,2013-12-11T21:16:43Z,"Cool, thanks. Will try this tomorrow.",NA,NA,NA
75,30443336,cboettig,2013-12-12T17:30:50Z,2013-12-12T17:30:50Z,"@cpfaff Looks good.  A few thoughts:

For bibentry -> article, you need a `eml_citation@creator = new(""ListOfcreator"" ` instead of `new(""creator"")`.  Recall that any time an EML element can be used more than once, the slot name actually has class `ListOf<class>` instead of class `<class>`.  

One way to construct this is:

```coffee
eml_citation@creator = new(""ListOfcreator"", lapply(from$author, as, ""creator""))
```

(Note that we initialize S4 objects that inherit from list class by just giving the list as an unnamed argument)


Also, I just noticed that you've actually gotten a bit carried away in defining the subclasses like `volume`.  For classes that are not complexTypes (in the schema, e.g. the slot class is a base class like ""character""), we get the element from the slotName (not the class, since that would create nonsense like `<volume><character>1</character></volume>`) 

So you really don't need all the definitions like `setClass(""volume"")`.  Your article class should just declare that `volume` is of type `character`.  Sorry I didn't catch this before.

Even though I think your more verbose code still works for the read/write from EML, please fix this because it makes initializing the classes unnecessarily cumbersome.  E.g. in your coercion, you have to write `eml_citation@volume = new(""volume"", volume = ""1"")`, instead of just `eml_citation@volume = ""1""`.  


",NA,NA,NA
75,30496987,cpfaff,2013-12-13T09:40:35Z,2013-12-13T09:40:35Z,"@cboettig Ok, thanks I alredy thought about this but then just did not do it. Now I have refactored everything and it really feels more native especially in the coerction methods. See latest pull request.",NA,NA,NA
77,30537089,cpfaff,2013-12-13T19:37:49Z,2013-12-13T19:37:49Z,No problem. But I it is very repetitive code. I was just wondering if there is a good way on how to dry this up a bit. But no clue yet.,NA,NA,NA
77,30538072,cboettig,2013-12-13T19:49:31Z,2013-12-13T19:49:31Z,"Yeah, I was trying to think of a way we could at least avoid writing the coercions separately for each class, since they share so much in common, there certainly are a few shortcuts to avoid repetition.  In general though this part of implementing the schema in R is doomed to be a bit repetitive so I wouldn't worry too much about it.  In an ideal world we could generate a lot of this code programmatically -- indeed that should eventually be possible when the XMLSchema package is done.  At some level that's the ""right"" way to do this, so meanwhile not worth fretting too much about it.  

I guess you're working on unit tests for the module now?  ",NA,NA,NA
77,30542740,cpfaff,2013-12-13T20:53:42Z,2013-12-13T20:55:54Z,"I will finish up all the coercions for the citation types first, Then the module is quite complete. After that I wil focus on the tests. 

About the repitition: I agree that implementing the schema is kind of doomed to repitition. One thing that would help a bit but not that much. The inner part of the to person coercion is always the same. So this could be extracted into a function.

```coffee
setAs(""ListOfrecipient"", ""person"", function(from){
      l <- lapply(from, as, ""person"")
      out <- NULL
      for(p in l)
        out <- c(out, p)
      class(out) = ""person""
      out
})
```",NA,NA,NA
77,30543064,cboettig,2013-12-13T20:58:51Z,2013-12-13T20:58:51Z,"Yeah, we may be able to do one better by using callNextMethod (on both the
coercion and the concatenation functions).  Properly speaking, when we have
a class like ""recipient"" that inherits from ""responsibleParty"", it should
inherit the methods as well.  Haven't tried it on coercions before, but
should be as simple as:

setMethod(""c"", signature(""recipient""), function(x, ...) callNextMethod())

setAs(""ListOfrecipient"", ""person"", function(from) callNextMethod())



On Fri, Dec 13, 2013 at 12:53 PM, cpfaff <notifications@github.com> wrote:

> I will finish up all the coercions for the citation types first, Then the
> module is quite complete. After that I wil focus on the tests.
>
> About the repitition: I agree that implementing the schema is kind of
> doomed to repitition. One thing that would help a but but nor that much.
> The inner part of the to person coercion is always the same. So this could
> be extracted into a function.
>
> setAs(""ListOfrecipient"", ""person"", function(from){
>       l <- lapply(from, as, ""person"")
>       out <- NULL
>       for(p in l)
>         out <- c(out, p)
>       class(out) = ""person""
>       out})
>
>
>    - out
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/pull/77#issuecomment-30542740>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
77,30562523,cpfaff,2013-12-14T08:03:15Z,2013-12-14T08:03:15Z,I do not understand how exactly the callNextMethod() works yet. But inheriting not only the slots but also the methods from a super class sounds great to me. ,NA,NA,NA
27,30570870,cpfaff,2013-12-14T13:42:33Z,2013-12-14T13:48:26Z,"A question. There is something that confuses me a bit and where I need some input. We have the creator from resource group which takes a `ListOfcreator`. In edited books the editors should be listed in the creator field. But as said already the slots awaits a `ListOfCreators`. And so the separate person type of editors with a `ListOfeditor` cannot be placed in there but should. How to best realize this. Defining two types for slots like this does not work
as it generates creator1 and creator2 for the different types

```coffee
 ""creator"" = c(""ListOfcreator"", ""ListOfeditor""),
```",NA,NA,NA
27,30571508,cpfaff,2013-12-14T13:54:21Z,2013-12-14T14:21:52Z,"Well just found the solution with class unions. That is cool:

```coffee
setClassUnion(""ListOfcreatorOreditor"", c(""ListOfcreator"", ""ListOfeditor""))
...
""creator"" = ""ListOfcreatorOreditor"",
```",NA,NA,NA
79,30618229,cboettig,2013-12-15T19:36:53Z,2013-12-15T19:36:53Z,Merged.  ,NA,NA,NA
76,30770830,emhart,2013-12-17T17:21:14Z,2013-12-17T17:21:34Z,Interesting that you want to do this in the `reml` package. I know we have an issue to do this within `spocc`.  But maybe it'd be more efficient to just build this into `reml` instead.,NA,NA,NA
80,30778079,cboettig,2013-12-17T18:43:23Z,2013-12-17T18:43:23Z,"@cpfaff Just a heads up that I'll be traveling for the next two weeks with
little access to email or github

---
Carl Boettiger
http://carlboettiger.info

sent from mobile device; my apologies for any terseness or typos
On Dec 16, 2013 9:49 PM, ""cpfaff"" <notifications@github.com> wrote:

> o Address to character
> o More coerctions of bibliography types
> ------------------------------
> You can merge this Pull Request by running
>
>   git pull https://github.com/cpfaff/reml-1 litrature
>
> Or view, comment on, or merge it at:
>
>   https://github.com/ropensci/reml/pull/80
> Commit Summary
>
>    - Address to character
>    - Adds more coercions to literature module
>
> File Changes
>
>    - *M* R/literature.R<https://github.com/ropensci/reml/pull/80/files#diff-0>(88)
>    - *M* R/party.R <https://github.com/ropensci/reml/pull/80/files#diff-1>(13)
>
> Patch Links:
>
>    - https://github.com/ropensci/reml/pull/80.patch
>    - https://github.com/ropensci/reml/pull/80.diff
>
>",NA,NA,NA
80,30778208,karthik,2013-12-17T18:44:49Z,2013-12-17T18:44:49Z,Thanks for the note Carl. It will allow me some time to catch up as well. I can handle other requests that arise during this time (if urgent).,NA,NA,NA
62,30782077,emhart,2013-12-17T19:27:20Z,2013-12-17T19:27:20Z,"I was trying to work on this, and found that I can't install `RwordXML`.  All I could find was a 2 year old version on Duncan's github that fails on an `install_github` call.  If it's not a on CRAN though, seems like we can't use the library if we want widespread adoption.",NA,NA,NA
80,30830212,cpfaff,2013-12-18T10:14:54Z,2013-12-18T10:14:54Z,Ok thanks  for the note,NA,NA,NA
80,30841637,cpfaff,2013-12-18T13:35:27Z,2013-12-18T13:35:27Z,@cboettig or @karthik could anybody merge my commits above?,NA,NA,NA
80,30864022,karthik,2013-12-18T17:52:27Z,2013-12-18T17:52:27Z,"Hi @cpfaff will do. Let me merge locally first before doing that. 
Will do by later today.",NA,NA,NA
80,30885621,cpfaff,2013-12-18T22:03:33Z,2013-12-18T22:03:33Z,ok thanks,NA,NA,NA
62,30971122,cboettig,2013-12-19T22:06:02Z,2013-12-19T22:06:02Z,"@emhart I think I just had to clone and `R CMD INSTALL `. May need to install dependencies that way too.  Anyway, if it is useful we can probably persuade Duncan to put it on CRAN.  Regardless, it would be a 'suggests' instead of 'imports' to keep the dependency footprint small.  Thus users who don't need this functionality don't have to have this package.  We do this already with several packages; see DESCRIPTION.",NA,NA,NA
82,31439149,cboettig,2014-01-02T06:22:08Z,2014-01-02T06:22:08Z,"Hi Daniel,

Thanks for the report and sorry for the rough start.  We're still in pre-alpha but like you say, it's always frustrating to hit errors in the basic examples.

As you see from your error messages, all you're missing is the uuid and RHTMLForms packages -- with those the example should run (that's why I didn't catch the errors earlier).  The reason these packages are not in IMPORTS is that they were not both on CRAN when we started.  `uuid` is now, and RHTMLForms can be installed from Github (see the [Testing and Development](https://github.com/ropensci/reml/#testing-and-development) section which unfortunately appeared at the very bottom of the README....)

Still, this is also somewhat of a bug since both `uuid` identifiers and the `eml_validate()` should probably be able to work around these dependencies (allowing us to keep a small dependency footprint) [1]

Hope this helps and looking forward to your further feedback,

- Carl

[1]: By using a simpler id scheme when uuid isn't available, and possibly by just using the XMLSchemaValidate when RHTMLForms isn't available.  You probably don't care about that at this stage so this footnote is really to the other devs.  I've fixed this for uuid (one function was calling uuid directly instead of `reml_id()` which provides alternative identifier mechanisms when uuid isn't available).  Bypassing RHTMLForms when not available is more questionable, as schema validation doesn't include the extra checks provided by the EML online validator, and would require the user download the schema first since no online endpoint is available -- all things discussed extensively in #7 ",NA,NA,NA
82,31502862,dfalster,2014-01-03T03:09:09Z,2014-01-03T03:09:09Z,"Hi Carl,

thanks for the helpful and prompt reply. Sorry, I didn't realise the package was in such an early stage of development. 

Installing uuid does indeed fix the first of the error,s bu the second (warning) remains. Also the new eml file does not pass the validation. Here is my output. 

```
> eml_config(creator=""Daniel Falster <my@email.com>"")
> eml_write(dat, file = ""reml_example.xml"")
Loading required package: uuid
[1] ""reml_example.xml""
Warning message:
In `[<-.data.frame`(`*tmp*`, , value = list(river = c(2L, 2L, 1L :
  Setting class(x) to NULL;   result will no longer be an S4 object
>  eml_validate(""reml_example.xml"")
Loading required package: RHTMLForms
Loading required package: RCurl
Loading required package: bitops
$`XML specific tests`
[1] ""cvc-complex-type.3.1: Value 'document' of attribute 'scope' of element 'eml:eml' is not valid with repect to the corresponding attribute use.""
```

No need to reply further  - I'll give you a few months for development to continue. 

A more general suggestion: Why not set up a test to make sure the examples run smoothly on a new machine? I have now encountered similar problems with several packages from ropensci, i.e. that the tutorials or examples don't work because of some package dependency issue which appears when someone tries to run the code on a new machine.

Keep up the great work!",NA,NA,NA
85,31958519,cboettig,2014-01-09T17:54:03Z,2014-01-09T17:54:03Z,"Um, see notes inline on the commit and send an updated pull request when ready. 
",NA,NA,NA
85,31966887,cpfaff,2014-01-09T19:27:44Z,2014-01-09T19:27:44Z,Ok thanks for the input. I wil improve this tomorrow!,NA,NA,NA
85,32187430,cpfaff,2014-01-13T16:51:49Z,2014-01-13T16:51:49Z,Hm. The comments have been deleted on squasing the commits and forcing the push.,NA,NA,NA
85,32269727,cpfaff,2014-01-14T14:44:21Z,2014-01-14T14:44:21Z,Please review and merge. There is still work to be done for the detail decision on the book edited book and chapter coercion but this will come in the next few days.,NA,NA,NA
85,32519270,cboettig,2014-01-16T19:15:47Z,2014-01-16T19:15:47Z,"@cpfaff Oh, minor comment: the `README.md` file is generated by the `Makefile` in `inst/doc` using `README.Rmd`.  So make any edits to README.Rmd and run `make readme` (at the bash shell), rather than editing `README.md` directly.  Make sense?",NA,NA,NA
85,32520932,cpfaff,2014-01-16T19:33:28Z,2014-01-16T19:33:28Z,@cboettig Good to know. Thanks. Sure that makes sense!,NA,NA,NA
89,32668835,cboettig,2014-01-18T00:30:24Z,2014-01-18T00:30:24Z,"@cpfaff I see one of the literature unit tests failing 

```
1. Error: We auto coerce audioVisual to bibentry -------------------------------
object 'an_audioVisual' not found
```",NA,NA,NA
87,32807664,cboettig,2014-01-20T23:33:17Z,2014-01-20T23:33:17Z,"@karthik I'm looking over this but no idea where to start.  I see the [.travis.yml](https://github.com/ropensci/reml/blob/master/.travis.yml) in the directory.  I thought I used to see the ""build-failing"" icon at the top of the README but do not see it at the moment.  The link above complains that it cannot find .travis.yml, so I'm confused.  I was getting the ""build-failing"" warnings in pull requests.  ",NA,NA,NA
87,32807712,karthik,2014-01-20T23:34:04Z,2014-01-20T23:34:04Z,"Will respond shortly on the Carl. 



On January 20, 2014 at 3:33:17 PM, Carl Boettiger (notifications@github.com) wrote:

@karthik I'm looking over this but no idea where to start. I see the .travis.yml in the directory. I thought I used to see the ""build-failing"" icon at the top of the README but do not see it at the moment. The link above complains that it cannot find .travis.yml, so I'm confused. I was getting the ""build-failing"" warnings in pull requests.

—
Reply to this email directly or view it on GitHub.",NA,NA,NA
86,32808082,cboettig,2014-01-20T23:40:03Z,2014-01-20T23:40:03Z,"@cpfaff I added this `ListOfcitation` definition to the literature module in dca167faa408f2fc07c8306f81fdacb3ff455dc9 

I've also added this to places in the coverage module that allow zero or more citations, 0e69cfc

Closes this issue.  ",NA,NA,NA
27,32808942,cboettig,2014-01-20T23:55:03Z,2014-01-20T23:55:03Z,"Looks like we have the literature module mostly in place, great work @cpfaff .  This issue also raises the idea of adding and extracting a ""canonical"" citation (e.g. what people should cite when using the data).  We have the function `eml_get(eml, ""citation_info"")` (or the method `citation_info`) which extracts a citation from an EML form based on creators, title, year, and publisher data from the dataset, but that's not quite ideal.  Looking for something more standard that could house a journal article the dataset creators wanted cited, etc, see https://projects.ecoinformatics.org/ecoinfo/issues/6283  As this issue has already covered a lot of stuff an has almost 30 comments, will open this ""canonical citation"" as a new issue.  I think we can close this one.  ",NA,NA,NA
66,32814020,cboettig,2014-01-21T01:39:46Z,2014-01-21T01:39:46Z,"Implemented, see [Advanced_writing_of_EML.md](https://github.com/ropensci/reml/blob/master/inst/doc/Advanced_writing_of_EML.md) vignette.",NA,NA,NA
48,32814066,cboettig,2014-01-21T01:40:51Z,2014-01-21T01:40:51Z,Coverage nodes are implemented.  Basic writing of a coverage node is shown in the [Advanced writing EML vignette](https://github.com/ropensci/reml/blob/master/inst/doc/Advanced_writing_of_EML.md),NA,NA,NA
23,32814407,cboettig,2014-01-21T01:48:26Z,2014-01-21T01:48:26Z,"@mbjones Quick questions on reserving a DOI through KNB: Looks like from the [documentation](http://mule1.dataone.org/ArchitectureDocs-current/apis/MN_APIs.html#MNStorage.generateIdentifier) that identifiers other than uuid are not yet available through the dataone API? Is that still current?  Is  this method implemented in either version of the dataone R package?  

A related question is I'm reluctant to test or document an example of reserving a DOI since we don't want to consume DOIs needlessly.  Does KNB or dataone have guidelines in place for when / what kind of data to request a DOI for, as opposed to another identifier?  What happens if I reserve a DOI using the development/testing servers instead?  ",NA,NA,NA
64,32814559,cboettig,2014-01-21T01:51:51Z,2014-01-21T01:52:15Z,"@karthik Let me know if there's any demos I can help check / update.  The Readme and [Advanced Writing](https://github.com/ropensci/reml/blob/master/inst/doc/Advanced_writing_of_EML.md) tutorials show both the simple and fancy ways of generating EML, hopefully they are a sufficient template to update the demos.  

The package relies on the `data.set` object (data.frame with metadata attached) as the prefered mechanism for unit and column metadata, as opposed to the method we used earlier in the demos where a data.frame and unit/column metadata where handled separately.  ",NA,NA,NA
49,32814650,cboettig,2014-01-21T01:53:39Z,2014-01-21T01:53:39Z,This will be inherent in any of the integration examples.  ,NA,NA,NA
87,32814976,karthik,2014-01-21T02:01:04Z,2014-01-21T02:01:04Z,I believe the build button was botched during all the README merges with Claas. I'll add it back in shortly. The build will continue to fail if it cannot install all the Imports listed in the description file. We can add additional lines to the yaml to install from GitHub but right now we can't install from source or from Omegahat. ,NA,NA,NA
87,32814998,karthik,2014-01-21T02:01:36Z,2014-01-21T02:01:36Z,I'll also check to make sure the build wasn't turned off on Travis. I see that you are moving this over to EML. Should I wait?,NA,NA,NA
87,32815191,cboettig,2014-01-21T02:05:42Z,2014-01-21T02:05:42Z,"Cool, I'll clean up the imports issues.

Good question about the transition.  Actually shouldn't matter.  I'm going
to keep ropensci/reml online, since it has all of our issues history
(unless / until someone is clever enough with the Github API to copy all
the issues over programmatically??)  and I can sync my local copy with
either/both.  Ugh, migration is kinda a nuisance but better to bite the
bullet sooner than later .


On Mon, Jan 20, 2014 at 6:01 PM, Karthik Ram <notifications@github.com>wrote:

> I'll also check to make sure the build wasn't turned off on Travis. I see
> that you are moving this over to EML. Should I wait?
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/87#issuecomment-32814998>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
87,32815315,cboettig,2014-01-21T02:08:50Z,2014-01-21T02:08:50Z,"Oh, I overwrite the README.md, so I probably destroyed the travis link in
the README file. whoops.  Need to add it to inst/doc/README.Rmd instead.
 What does Travis actually check?  R CMD check?


On Mon, Jan 20, 2014 at 6:05 PM, Carl Boettiger <cboettig@gmail.com> wrote:

> Cool, I'll clean up the imports issues.
>
> Good question about the transition.  Actually shouldn't matter.  I'm going
> to keep ropensci/reml online, since it has all of our issues history
> (unless / until someone is clever enough with the Github API to copy all
> the issues over programmatically??)  and I can sync my local copy with
> either/both.  Ugh, migration is kinda a nuisance but better to bite the
> bullet sooner than later .
>
>
> On Mon, Jan 20, 2014 at 6:01 PM, Karthik Ram <notifications@github.com>wrote:
>
>> I'll also check to make sure the build wasn't turned off on Travis. I see
>> that you are moving this over to EML. Should I wait?
>>
>> —
>> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/87#issuecomment-32814998>
>> .
>>
>
>
>
> --
> Carl Boettiger
> UC Santa Cruz
> http://carlboettiger.info/
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
87,32815530,karthik,2014-01-21T02:14:06Z,2014-01-21T02:14:06Z,Not sure why you didn't just rename the repo itself rather than create a new one. That way you could have kept everything in the same place. We would just need to update our remotes (or simply clone again).,NA,NA,NA
87,32815558,karthik,2014-01-21T02:14:56Z,2014-01-21T02:14:56Z,"Yes, Travis runs R CMD CHECK. You can see the same log files as you would get when you run locally on the Travis page for the repo.",NA,NA,NA
87,32815736,cboettig,2014-01-21T02:18:43Z,2014-01-21T02:18:43Z,"I can do that?? I guess it breaks links but that is a much much better idea
all the same. If I delete the EML repo I just created I should be able to
do that then, yes?


On Mon, Jan 20, 2014 at 6:14 PM, Karthik Ram <notifications@github.com>wrote:

> Yes, Travis runs R CMD CHECK. You can see the same log files as you would
> get when you run locally on the Travis page for the repo.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/reml/issues/87#issuecomment-32815558>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
87,32815789,karthik,2014-01-21T02:20:19Z,2014-01-21T02:20:19Z,"yes, just delete EML first.
Then go to settings for rEML and just rename (first setting). That's it. Everything will stay the same. Some links will break but that's not a big deal for now. You can create a rEML repo and say everything's at EML now. ",NA,NA,NA
87,32815849,cboettig,2014-01-21T02:21:55Z,2014-01-21T02:21:55Z,"brilliant, done.


On Mon, Jan 20, 2014 at 6:20 PM, Karthik Ram <notifications@github.com>wrote:

> yes, just delete EML first.
> Then go to settings for rEML and just rename (first setting). That's it.
> Everything will stay the same. Some links will break but that's not a big
> deal for now. You can create a rEML repo and say everything's at EML now.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/EML/issues/87#issuecomment-32815789>
> .
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
92,32874239,ivanhanigan,2014-01-21T12:56:59Z,2014-01-21T12:56:59Z,Thanks for the great work!  Hopefully the script makes sense.  Let me know any thing I can do.,NA,NA,NA
93,32874656,ivanhanigan,2014-01-21T12:57:12Z,2014-01-21T12:57:12Z,Thanks for the great work!  Hopefully the script makes sense.  Let me know any thing I can do.,NA,NA,NA
93,32985460,mbjones,2014-01-22T01:33:49Z,2014-01-22T01:33:49Z,"Morpho 1.8 knows nothing about EML 2.1.1, which is what reml produces (IIRC).  So you would have to do a back-conversion from EML 2.1.1 to EML 2.1.0 which is what the most recent version that Morpho 1.8.x can read.   The main difference between these versions is the addition of new internationalized language fields, so if you don't use these, you should be able to simply change the namespace to eml 2.1.0 of your output document from REML and it should still be a valid EML 2.1.0 document and hopefully it will then open in Morpho 1.8.x.

I'm really happy to see you combining these tools and making best use of each of their features (authoring for Morpho, automation for REML).  We should think more about how to make this combination seamless.",NA,NA,NA
23,32986264,mbjones,2014-01-22T01:48:42Z,2014-01-22T01:48:42Z,"@cboettig Each MN can support different identifier types. If the MN does not support the id type you want, you'll get a Exceptions.InvalidRequest back.  The KNB supports two identifier types: UUID and DOI.

You can reserve IDs on the development servers with impunity -- they come from the test DOI server and are not preserved.  

On the KNB production server, please don't reserve any DOIs for testing.  For real data packages, you can reserve DOIs and use them from whatever parts of the data package you'd like, but we've gotten pushback from DataCite for issuing lots of DOIs for the various components and revisions contained in a data package.  So, more recently we have focused on publishing a DOI for the metadata document and using UUIDs for all of the components (data files, resource maps, etc), as the metadata document is the main visible entrypoint for the data package and is presumably what should be cited and linked to).  But we leave this decision up to client tools as the service lets you generate and use DOIs how you wish.",NA,NA,NA
93,32994078,ivanhanigan,2014-01-22T05:15:10Z,2014-01-22T05:34:17Z,"Thanks!  I feel bad that I'm not entering all the variable level details, but some of the datasets from social science have hundreds (even thousands) of variables and so I just want to do a quick pass to identify basic type, then I can come back to the most interesting and give more detail.

Toward automating the variable description I've worked up:

* this code https://github.com/ivanhanigan/EML/blob/devel/R/eml_boilerplate.R 
* and this test https://github.com/ivanhanigan/EML/blob/devel/tests/test-eml_boilerplate.r

Which fails to validate
    eml_validate(""mulgara.xml"")
    # $`XML specific tests`
    #[1] ""cvc-complex-type.2.4.b: The content of element 'attribute' is not complete. It must match '((((((((((\"" 
    #\"":attributeName),(\""\"":attributeLabel){0-UNBOUNDED}),(\""\"":attributeDefinition)),(\""\"":storageType)
    #{0-UNBOUNDED}),(\""\"":measurementScale)),(\""\"":missingValueCode){0-UNBOUNDED}),(\""\"":accuracy){0-1}),
    #(\""\"":coverage){0-1}),(\""\"":methods){0-1})|(\""\"":references))'.""
",NA,NA,NA
64,33076785,karthik,2014-01-22T22:35:57Z,2014-01-22T22:35:57Z,"@cboettig 
Hi there. This sounds great. I'll write up a couple of demos once I've had a chance to scan through the update. I'll post an update to this thread or simply start a new pull request.
",NA,NA,NA
94,33410275,cboettig,2014-01-27T19:19:05Z,2014-01-27T19:19:05Z,"@cpfaff Can you send me a cleaner pull request?  This can't be automatically merged and changes a lot of different files.  I think we got out of sync somewhere.  You might want to squash or roll back the first 6 commits and then add just the fixed typo and the start of the project module.  

Also, haven't quite gotten Travis deployed online yet, but I've tidied up the documentation, etc, so that you should be able to confirm a clean R CMD check result before sending a pull request? ",NA,NA,NA
94,33410420,karthik,2014-01-27T19:20:28Z,2014-01-27T19:20:28Z,"Travis also works on multiple branches. So it might also work to merge into a dev branch (or keep multiple ones), sync with master, then merge after all checks pass. Travis (once working) should also show whether or not a pull request will pass (inline, on this page) before you automatically accept one.",NA,NA,NA
95,33421430,cboettig,2014-01-27T20:53:20Z,2014-01-27T20:53:20Z,"Apparently Travis cannot install (see [build-failed log](https://travis-ci.org/ropensci/EML/builds/17722702)) the `dataone` package even though [it is on CRAN](http://cran.r-project.org/web/packages/dataone/) (probably the Java environment requirements).  Oh well, guess we can drop it from the automated suggests list and we can tell check to skip these tests, since we cannot do KNB authentication over Travis anyway. (KNB use will still be documented in the vignettes etc, and hopefully the REST-based R client will solve this...).  ",NA,NA,NA
95,33421995,karthik,2014-01-27T20:59:03Z,2014-01-27T20:59:03Z,"Yep, although I'll see if we can workaround that. Java is a dep for most non R use-cases for Travis.",NA,NA,NA
95,33424582,mbjones,2014-01-27T21:25:16Z,2014-01-27T21:25:16Z,"Yeah, java itself probably isn't the challenge -- its the JNI bridge needed for rJava to compile that is the issue.  I started down the path of using `httr` for rdataone and its working well.  I just need to work out the XMLSchema bug with the dataone schemas, and from that point forward I think the implementation will go quickly.  So I wouldn't invest a lot of time in getting rJava working in Travis.",NA,NA,NA
95,33429707,cboettig,2014-01-27T22:16:40Z,2014-01-27T22:16:40Z,Travis build / checks passing now.,NA,NA,NA
93,33433073,cboettig,2014-01-27T22:52:48Z,2014-01-27T22:52:48Z,"@ivanhanigan Thanks for the interest, bug reports and automation example, and sorry for the hiccups while we're still in active development phase.  Your examples help us see how users will approach these functions.

As you say, being able to script the annoying and slow parts is precisely what we are aiming for. 

You are getting that (admittedly opaque -- blame the xmllint C libraries) validation error message because you haven't defined the unit definitions appropriately for the factor you've labeled ""Treat"".  Factors need a named character vector defining the levels (see the README), while you have just given it a character vector `c(""0"", ""1"")` with no labels.  Just editing your boilerplate to define these codes, e.g. `c(""no treatment"" = ""0"", ""treatment"" = ""1"")`, should cause the R package to write the complete metadata for the factor (nonNumericDomain) so that EML validates.   I'll try and add some better error handling in to catch this case before validation... 


 I see your point about wanting to provide good basic top-level metadata, while not being bothered to write out definitions for 100s of columns or factor levels.  Some metadata is still better than no metadata.  I would personally recommend in such cases not trying to ""hack"" the validation by duplicating names instead of descriptions though.  In many cases, I think scripts can still be used to generate informative unit-level metadata, following a similar strategy as you show here.  

However, if writing out this unit-level metadata really isn't on your agenda, but you still want to take advantage of the higher-level metadata, I would suggest the best practice is just to omit the dataTable level documentation all-together.   EML doesn't insist on having a `dataTable` object at all.  You can write as much or as little other higher-level metadata as you want then (creators, coverage, abstract, methods, citation etc), and just write the file out to a csv and stick it somewhere (such as figshare).  You can mention the location of said CSV by linking it from the `eml/dataset/distribution` node.  Anyway, that's how I'd handle adding partial metadata. @mbjones or the [eml-discuss](http://lists.nceas.ucsb.edu/ecoinformatics/mailman/listinfo/eml-dev) would have greater insight on how best to handle this case.  

 I'll try and add an example of that in one of the vignettes we're working on (see drafts in https://github.com/ropensci/EML/tree/master/inst/doc/vignettes).  


Sorry for the slow reply.  Feel free to close the issue if this answers your question, and free to open new issues as needed.  Again thanks for your time and input!",NA,NA,NA
92,33438258,cboettig,2014-01-28T00:05:33Z,2014-01-28T00:05:33Z,"@ivanhanigan Thanks for the bug report; yes the script makes sense, thanks.  

Sorry about the double-entry -- what an annoying bug.  The latest commit to the master branch should fix that element of the wizard's behavior at least.  The R wizard feature hasn't been well tested (kinda hard to write good unit tests for a function that is supposed to be interactive) so I appreciate you catching that one.  

I can't reproduce the first error you show, either way I just get the wizard. Might have been a bug I already fixed before going to v0.0-5? Anyway I'd appreciate if you get a chance to check your examples with the latest version and let me know of any other errors or annoying behavior you come across in using the interface.  

",NA,NA,NA
82,33439205,cboettig,2014-01-28T00:20:29Z,2014-01-28T00:20:29Z,"@dfalster Hi Daniel, 

Just wanted to drop a quick note to say that while we're still not up to testing a release-candidate yet, I'm hoping basic functions are stable enough to start playing with.  We have continuous testing with travis now, so as long as the master branch shows ""build passing"" then at least the README and examples in the documentation (& unit tests) should run now.  Thanks for prodding us to get around to more continuous automated testing.  

There's a couple different ways to create EML using the package (compare the approach in the README to the [advanced vignette (draft)](https://github.com/ropensci/EML/blob/master/inst/doc/vignettes/Advanced_parsing_of_EML.md)).  Hoping to have a function interface that is as intuitive (while still flexible) as we can make it, so any critique is more than welcome.  (of course we understand if you're too busy to delve further at this time too).  ",NA,NA,NA
95,33441197,cboettig,2014-01-28T00:55:33Z,2014-01-28T00:55:33Z,"@karthik Thanks for the awesome work getting Travis set up here, nice to have it working.  ",NA,NA,NA
94,33455897,cpfaff,2014-01-28T07:11:05Z,2014-01-28T07:13:47Z,Oh well I see what happened now. Yes we got ouf sync from that point where you renamed the repository. I pull from upstream on my side to get the changes and my config still has the upstream set to reml. Sorry.,NA,NA,NA
95,33496112,karthik,2014-01-28T16:37:22Z,2014-01-28T16:37:22Z,"Anytime, glad it's resolved. Working on examples for package now.",NA,NA,NA
94,33497826,cboettig,2014-01-28T16:52:35Z,2014-01-28T16:52:35Z,"Yup. You should just be able to reset your remotes to the new address and
then pull the latest to fast forward (might want to drop or roll back your
edits first, then reapply them after you are synchronized again). Holler if
you run into trouble

---
Carl Boettiger
http://carlboettiger.info

sent from mobile device; my apologies for any terseness or typos
On Jan 27, 2014 11:11 PM, ""cpfaff"" <notifications@github.com> wrote:

> Oh well I see what happened now. Yes we got ouf sync from that point where
> you renamed the repository. I pull from upstream on my side to get the
> changes and my config still has the upstream set to reml.
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/EML/pull/94#issuecomment-33455897>
> .
>",NA,NA,NA
94,33498134,karthik,2014-01-28T16:55:11Z,2014-01-28T16:55:11Z,"You can 

```
git remote rm origin
git add origin https://...EML.git
```
replacing the url with the current remote. Should update the remote without breaking the code you already have in it.",NA,NA,NA
94,33499189,cpfaff,2014-01-28T17:04:52Z,2014-01-28T17:04:52Z,Thanks for the responses. See the pull requests.,NA,NA,NA
96,33502837,cboettig,2014-01-28T17:38:05Z,2014-01-28T17:38:05Z,"Yes, I am aware this is tricky, hence my suggestion to roll back to the last commit where the sync departed, since all of these commits don't actually change anything.  Then a single merge fast-forward should be possible.  Anyway, that's just ideal, it's not like the commit log is all pretty to begin with, just wanted to be clear how this could be done. 

Thanks for getting this down to three file changes though, at least that's easy for me to review.  ",NA,NA,NA
98,33502973,cboettig,2014-01-28T17:39:21Z,2014-01-28T17:39:21Z,Note that Travis is now failing on this; pls fix. ,NA,NA,NA
82,33554278,dfalster,2014-01-29T03:51:47Z,2014-01-29T03:51:47Z,"Hi Carl,

Great to hear you are using Travis CI, I have also started using this in
some collaborative work and am very impressed.

I came across your package while reading about EML. I must admit, I still
find the question of when to include an EML file quite confusing. If I am
submitting a data package to dryad, should I attempt to provide metadata in
EML, or does that happen automatically during upload process? If you know
of any good introductory texts please let me know. This is something to
bear in ming for other users of your package: most, like me, will have only
limited if any experience with EML, so you need to be quite clear about
when you should and shouldn't use it.

All the best,
Daniel



On 28 January 2014 11:20, Carl Boettiger <notifications@github.com> wrote:

> @dfalster <https://github.com/dfalster> Hi Daniel,
>
> Just wanted to drop a quick note to say that while we're still not up to
> testing a release-candidate yet, I'm hoping basic functions are stable
> enough to start playing with. We have continuous testing with travis now,
> so as long as the master branch shows ""build passing"" then at least the
> README and examples in the documentation (& unit tests) should run now.
> Thanks for prodding us to get around to more continuous automated testing.
>
> There's a couple different ways to create EML using the package (compare
> the approach in the README to the advanced vignette (draft)<https://github.com/ropensci/EML/blob/master/inst/doc/vignettes/Advanced_parsing_of_EML.md>).
> Hoping to have a function interface that is as intuitive (while still
> flexible) as we can make it, so any critique is more than welcome. (of
> course we understand if you're too busy to delve further at this time too).
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/EML/issues/82#issuecomment-33439205>
> .
>",NA,NA,NA
82,33554684,dfalster,2014-01-29T04:01:18Z,2014-01-29T04:01:18Z,"Confirming that examples now work.

Although

- had to install rfigshare --> should this be listed in dependencies?
- I still get warning message  eml_validate(""EML_example.xml"")

> Warning messages:

> 1: In library(package, lib.loc = lib.loc, character.only = TRUE,
logical.return = TRUE,  :  there is no package called 'RHTMLForms'

> 2: In eml_validate(""EML_example.xml"") : Performing XML Schema validation
only.

>           Install RHTMLForms to provide additional EML-specific tests.



On 29 January 2014 14:51, Daniel Falster <daniel.falster@mq.edu.au> wrote:

> Hi Carl,
>
> Great to hear you are using Travis CI, I have also started using this in
> some collaborative work and am very impressed.
>
> I came across your package while reading about EML. I must admit, I still
> find the question of when to include an EML file quite confusing. If I am
> submitting a data package to dryad, should I attempt to provide metadata in
> EML, or does that happen automatically during upload process? If you know
> of any good introductory texts please let me know. This is something to
> bear in ming for other users of your package: most, like me, will have only
> limited if any experience with EML, so you need to be quite clear about
> when you should and shouldn't use it.
>
> All the best,
> Daniel
>
>
>
> On 28 January 2014 11:20, Carl Boettiger <notifications@github.com> wrote:
>
>> @dfalster <https://github.com/dfalster> Hi Daniel,
>>
>> Just wanted to drop a quick note to say that while we're still not up to
>> testing a release-candidate yet, I'm hoping basic functions are stable
>> enough to start playing with. We have continuous testing with travis now,
>> so as long as the master branch shows ""build passing"" then at least the
>> README and examples in the documentation (& unit tests) should run now.
>> Thanks for prodding us to get around to more continuous automated testing.
>>
>> There's a couple different ways to create EML using the package (compare
>> the approach in the README to the advanced vignette (draft)<https://github.com/ropensci/EML/blob/master/inst/doc/vignettes/Advanced_parsing_of_EML.md>).
>> Hoping to have a function interface that is as intuitive (while still
>> flexible) as we can make it, so any critique is more than welcome. (of
>> course we understand if you're too busy to delve further at this time too).
>>
>> --
>> Reply to this email directly or view it on GitHub<https://github.com/ropensci/EML/issues/82#issuecomment-33439205>
>> .
>>
>
>",NA,NA,NA
82,33558301,cboettig,2014-01-29T05:44:38Z,2014-01-29T05:44:38Z,"@dfalster Thanks for the feedback, this is very helpful.  It's nice to hear how you've come across EML and how you are planning on using it.  Yes, I completely agree about the need to explain the logic of EML.  I'm hoping to spell this out in a manuscript and a series of blog posts, since I don't think it can be properly addressed in the technical documentation alone (README, help files, etc, which need to be more concise).  

To answer your question - yes, you would submit the XML file created by the EML package, along with the CSV file, to Dryad.  I'd suggest calling it ""README.XML"" and submitting it at the point where Dryad gives you the option to upload a readme.  You should think of EML files as machine-readable readme's, containing all the information you guys discuss in your Nine Simple Ways paper.  A README should define everything you discuss in that paper, from the names of the data creators down to the format of dates data and the choice of the NULL values.  EML automates a lot of that creation.  

I'm actually trying to use the Nine Simple Ways paper as an example to explain the whole idea better.  I'm still mid-way in writing, but if you get a chance look at this [draft blog post](https://github.com/ropensci/EML/blob/master/inst/doc/pubs/one-simple-way.Rmd) and I'd much appreciate any comments on what makes sense and what still sounds like gibberish. 

Do see the section on data repositories at the end.  While I recognize many users will want to publish data in Dryad (which requires manual uploading at this time), the KNB, (another DataONE member node created by NCEAS before NESCent created Dryad) is in many ways more ideal.  Whereas Dryad asks you to include such information as geographic and species coverage in the metadata, KNB simply speaks EML and can read all that information right out of the EML file.  No extra effort required.  Users can search for this or any other metadata, down to your column headings.  You still get a DOI, robust redundant archiving, etc etc, and most importantly perhaps, you are not restricted to data immediately associated with publication.  


RE: RHTMLForms warning message: yes, that's the expected behavior.  RHTMLForms is not yet available on CRAN, so it is cannot yet be a package dependency/suggest item.  To work around this, we use XML schema validation, which does almost all of the checks.  I'm not sure how best to handle this case where there is awesome software that can enhance the functionality of the EML package, even though it is not required for basic use.  Suggestions welcome.  ",NA,NA,NA
99,33561829,cpfaff,2014-01-29T07:21:29Z,2014-01-29T07:22:01Z,I like the idea very much. I think this not only overcomming the linear approach we have to deal with in R commandline but also is/or can be a very portable way of generating eml. ,NA,NA,NA
99,33612847,karthik,2014-01-29T18:14:49Z,2014-01-29T18:14:49Z,"If @cboettig has no objections or concerns, I can start building this out. It would still allow users to work with the inline wizard if they prefer but offer a much simpler, lower barrier interface for less technical folks.",NA,NA,NA
99,33613262,cboettig,2014-01-29T18:19:03Z,2014-01-29T18:19:03Z,"Sounds awesome. Rock on.

---
Carl Boettiger
http://carlboettiger.info

sent from mobile device; my apologies for any terseness or typos
On Jan 29, 2014 10:14 AM, ""Karthik Ram"" <notifications@github.com> wrote:

> If @cboettig <https://github.com/cboettig> has no objections or concerns,
> I can start building this out. It would still allow users to work with the
> inline wizard if they prefer but offer a much simpler, lower barrier
> interface for less technical folks.
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/EML/issues/99#issuecomment-33612847>
> .
>",NA,NA,NA
93,33720962,ivanhanigan,2014-01-30T19:03:54Z,2014-01-30T19:03:54Z,"I take your point about hacking the descriptive metadata, but our aim is to publish these to a Metacat server as packages including their tabular data. The workflow we imagine is that the R users can ""scrape"" EML from their data and send the result to our data librarian who will give the metadata a scrub and polish in Morpho before pushing them to Metacat using the inbuilt connection between these software.
Hopefully such a process is possible in the future, as our R users (ie me!) are generally not as keen as our librarian  is about entering descriptive metadata.  Thanks!",NA,NA,NA
93,33723522,mbjones,2014-01-30T19:27:53Z,2014-01-30T19:27:53Z,"@ivanhanigan Given your desired workflow, I suggest that you could potentially use the rdataone module in R to publish your EML to a Metacat server.  Carl has written a function in the EML package to publish to the KNB Metacat, but you could also publish using rdataone other Metacat servers. If you set the access control rules as desired, then your data librarian should be able to review and change those metadata in Morpho, and save a new version to Metacat (using the same DataONE API).",NA,NA,NA
93,33725335,cboettig,2014-01-30T19:45:49Z,2014-01-30T19:45:49Z,"@ivanhanigan Definitely consider the [rdataone](https://github.com/DataONEorg/rdataone) package at Matt recommends (the link is the development version; an earlier version of the dataone package is also [on cran](http://cran.r-project.org/web/packages/dataone/).

You can certainly use this `EML` R package to write as much or as little metadata as you'd like. Particularly since you mention the metadata being edited downstream by a curator, I wouldn't worry about the formal schema validation (`eml_validate`) in your workflow, if the goal is just to give the curator a head start filling out specific metadata.

It's great for us to hear how users are thinking about using EML and the R package.  For instance, I understand that Harvard Forest has researchers submit metadata in a MS Word document, from which their curators generate the EML using Morpho and Oxygen. As R can parse MS Word (really just zipped XML) directly, we are looking into automating this extraction.  After all, no one wants to type 3 pages of methods into an R script. (see #62)",NA,NA,NA
104,35137097,mbjones,2014-02-14T23:27:51Z,2014-02-14T23:27:51Z,"Boy, I don't remember any of the details of the `action` and `dependency` fields. In hindsight, it looks crazy. A list of required and optional dependencies seems sufficient.  If we were to do what many package managers do, we'd have to track software using official release names and versions, which would have to be controlled.  To do that across languages/platforms would invite collisions.

I've been thinking a lot about this cross-language software documentation issue via ISEES and now the Community Dynamics workshop, and I think it would be a good time to propose changes that can work across a broad set of communities.  So, let's keep a list of the things we'd like to see changed in EML.",NA,NA,NA
104,35204780,cboettig,2014-02-16T17:51:02Z,2014-02-16T17:51:02Z,"Very good.  will start listing in this issue for the time being.  The ResourceGroup lets us capture most of the standard R package metadata anyway.  Here's the things that are part of the standard R `packageDescription` which I couldn't capture specifically (i.e. other than in `additionalInfo`:

- [ ] BugReports link
- [ ] Dependencies as a plain-text field(?)
- [ ] `Suggests` / `Enhances` (maybe not relevant)
- [ ] `Built` by particular R version (e.g. information that isn't strictly a declared dependency of the package, but might be helpful in troubleshooting.  
- [ ] `Packaged` Date package was built, and user id.  Perhaps not really useful metadata.
- [ ] `Maintainer` (e.g. an `contact`)  (Maybe I'm just missing something here.  I put all ""authors"" into the Creator elements.  R lets maintainers have a variety of roles (see `?person`, and insists maintainer be designated with an email address.  Though I can add contact information to a creator of course, it may or may not make sense to have a `contact` node in which that information isn't optional, just like in `dataset`?
- [ ] A website / URL associated with the package (e.g. usually has documentation, possibly mailing list link, possibly the development site (sourceforge, github, etc))

Other things like the collate list and vignetteBuilder also show up in an R description file but are very unlikely to be relevant.  Some things are not part of an R Description but could be relevant in general, such as a commit SHA, the location of the development repository, etc.  

Yes, it would be fun to tackle the spectrum of documenting a little script associated with a particular analysis to documenting a more formal software package.  ",NA,NA,NA
82,35700612,dfalster,2014-02-21T05:36:10Z,2014-02-21T05:36:10Z,"Hi Carl,

Apologies for the slow reply. I was instructing at two separate swc
bootcamps in the last weeks so a little distracted. On quick skim your
blogpost looks very good. And thanks for the extra information re EML
files. I will try out the approach when I upload some data in the next week
or two.

BTW - I wasn't an author on the 9 simple ways paper, but do like it!

All the best,
Daniel


On 29 January 2014 16:44, Carl Boettiger <notifications@github.com> wrote:

> @dfalster <https://github.com/dfalster> Thanks for the feedback, this is
> very helpful. It's nice to hear how you've come across EML and how you are
> planning on using it. Yes, I completely agree about the need to explain the
> logic of EML. I'm hoping to spell this out in a manuscript and a series of
> blog posts, since I don't think it can be properly addressed in the
> technical documentation alone (README, help files, etc, which need to be
> more concise).
>
> To answer your question - yes, you would submit the XML file created by
> the EML package, along with the CSV file, to Dryad. I'd suggest calling it
> ""README.XML"" and submitting it at the point where Dryad gives you the
> option to upload a readme. You should think of EML files as
> machine-readable readme's, containing all the information you guys discuss
> in your Nine Simple Ways paper. A README should define everything you
> discuss in that paper, from the names of the data creators down to the
> format of dates data and the choice of the NULL values. EML automates a lot
> of that creation.
>
> I'm actually trying to use the Nine Simple Ways paper as an example to
> explain the whole idea better. I'm still mid-way in writing, but if you get
> a chance look at this draft blog post<https://github.com/ropensci/EML/blob/master/inst/doc/pubs/one-simple-way.Rmd>and I'd much appreciate any comments on what makes sense and what still
> sounds like gibberish.
>
> Do see the section on data repositories at the end. While I recognize many
> users will want to publish data in Dryad (which requires manual uploading
> at this time), the KNB, (another DataONE member node created by NCEAS
> before NESCent created Dryad) is in many ways more ideal. Whereas Dryad
> asks you to include such information as geographic and species coverage in
> the metadata, KNB simply speaks EML and can read all that information right
> out of the EML file. No extra effort required. Users can search for this or
> any other metadata, down to your column headings. You still get a DOI,
> robust redundant archiving, etc etc, and most importantly perhaps, you are
> not restricted to data immediately associated with publication.
>
> RE: RHTMLForms warning message: yes, that's the expected behavior.
> RHTMLForms is not yet available on CRAN, so it is cannot yet be a package
> dependency/suggest item. To work around this, we use XML schema validation,
> which does almost all of the checks. I'm not sure how best to handle this
> case where there is awesome software that can enhance the functionality of
> the EML package, even though it is not required for basic use. Suggestions
> welcome.
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/EML/issues/82#issuecomment-33558301>
> .
>",NA,NA,NA
92,35780770,ivanhanigan,2014-02-21T22:30:57Z,2014-02-21T22:30:57Z,"Thanks Carl,
These issues appear to be resolved in new version.
Cheers, Ivan",NA,NA,NA
105,38239992,mbjones,2014-03-21T01:27:16Z,2014-03-21T01:27:16Z,"Probably related to our switch to a new web framework at the KNB, which affected the location of some existing URLs and services.  This one slipped through the cracks.  I'll be sure it is fixed.",NA,NA,NA
105,38242523,leinfelder,2014-03-21T02:09:38Z,2014-03-21T02:09:38Z,"I tried it (just in the browser) and it worked for me...
""Document is XML-schema valid. There were no XML errors found in your document.""

Can you change the url in the R package to use ""https""?",NA,NA,NA
105,38251320,mbjones,2014-03-21T06:07:21Z,2014-03-21T06:07:21Z,"OK, I updated the URL in the eml_validate.R function and retried the test, and it seems to work fine now.  Fixed with sha c3d8db7.",NA,NA,NA
105,38283330,cboettig,2014-03-21T14:55:01Z,2014-03-21T14:55:01Z,"Awesome, thanks!

I'll make this the default validation method again then.

---
Carl Boettiger
http://carlboettiger.info

sent from mobile device; my apologies for any terseness or typos
On Mar 20, 2014 11:07 PM, ""Matt Jones"" <notifications@github.com> wrote:

> OK, I updated the URL in the eml_validate.R function and retried the test,
> and it seems to work fine now. Fixed with sha c3d8db7<https://github.com/ropensci/EML/commit/c3d8db7>
> .
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/EML/issues/105#issuecomment-38251320>
> .
>",NA,NA,NA
82,39172232,karen-ross,2014-04-01T05:32:12Z,2014-04-01T05:32:12Z,"Hi Carl,

I'm working with Daniel Falster (@dfalster) and am trialling using your EML package to get some of our lab data out there. I've only just started exploring and trying some test data but it looks like a great tool!  If you have the time I'll ask a few questions. 
If any of these could be answered easily by searching online please let me know...

1. How can I assign a more sensible name to the .csv file that is created automatically by eml.write. I've tried specifying the title arg:
```
eml_write(tree.data, file = ""EML_tree.data.xml"", title = ""tree.data"")
```
but it doesn't use my title to name the csv as I thought it should, but instead gives it a new unique identifier name.
An associated question is how to avoid duplication of an existing .csv file? Our usual workflow will be that we start from an existing .csv file that we read into R as a data.frame, annotate it to create a data.set, then eml_write an xml file for it. So in that case we will be duplicating our (already named) .csv, and then if we publish it? Is there a way around this other than manually editing the xml file to change the name of the csv file it contains to the name of our existing .csv file? 

2. I have a dataset with a column containing basal area values in units of metres squared per hectare (m^2.ha^-1). When I try to validate the eml file it gives an error:
```
> eml_validate(""EML_tree.data.xml"")
$`XML specific tests`
[1] ""cvc-type.3.1.3: The value 'm^2.ha-1' of element 'standardUnit' is not valid.""
```
So when preparing data for creating metadata, if I am not familiar with EML, how do I avoid this and stick to standard units? I've been looking for an easily accessible list of the standard units in EML but haven't yet managed to find one? What would I do if my unit is not there?
 
Many thanks for your time and any suggestions.

best wishes,
Karen",NA,NA,NA
82,39237358,mbjones,2014-04-01T18:00:09Z,2014-04-01T18:00:09Z,"The list of standard units that ship with EML is in the [eml-unitDictionary.xml](https://code.ecoinformatics.org/code/eml/trunk/eml-unitDictionary.xml) file in the EML distribution.  Skip down to the section of 'unit' definitions (rather than unitType definitions which are at the top).  We've long recognized a need to make a community-shared list of additional units, as the so called standard list is incomplete.  LTER started work on this, but it hasn't passed into widespread use outside of LTER as far as I know.  You can search their [extended unit list on the web](http://unit.lternet.edu/unitregistry/) and their is an associated [REST service](http://unit.lternet.edu/services/unitregistry/unit/) that we could use in the EML package.",NA,NA,NA
82,39240565,cboettig,2014-04-01T18:26:28Z,2014-04-01T18:26:28Z,"@karen-ross Hi Karen,

Thanks for writing with these issues; your feedback is very helpful as we figure out the best way to handle these things.  Will hopefully make these things easier to do soon, but for now you have to use the ""Advanced"" construction of EML to control the file in the way you want; see our draft of [Advanced writing of EML](https://github.com/ropensci/EML/blob/devel/inst/doc/vignettes/Advanced_writing_of_EML.md).  To summarize, you would have to construct the EML a bit more step by step.  At the top of that document, you'll see my summary of the main components of an EML file.  For instance, to build the dataTable entry with a custom file name, do: 

```coffee
dataTable <- eml_dataTable(dat, 
                           description = ""Metadata documentation for S1.csv"", 
                           file = ""S1.csv"")
eml_write(dataTable, file = ""EML_tree.data.xml"", contact = ""your name <your@email.com>"")
```

Hopefully that document outlines the logic of this approach a bit better.  `eml_write` tries to be intelligent and take a variety of input objects, though as you'll see in that vignette it is more precise to construct the eml bit by bit with `eml`.  

Very good question about using an existing csv file.  That's a bit more tricky than it may seem, because without R reading that csv file it cannot know if it is actually what it says it is (e.g. comma delimited in the standard way, has the headings it says, etc).  To avoid the chance of metadata annotations conflicting with the actual file, I would still recommend reading in any original csv file into R as a data.frame, and let the EML package write it back out as a different csv file (you could choose to overwrite or delete the original csv, but that's outside the scope of EML).  This way, you can also set the column classes appropriately in R, and the EML package can make sure that the CSV is properly standardized with the right delimiters, number of lines, etc.  Does this make sense?  Feel free to push back on this if I'm overlooking other concerns.  

I believe @mbjones already replied to your comments about units.  Hopefully we'll have nicer facilities for identifying standard units soon, and for creating new units.  On your unit request, I note that square meter per hectare is a pure number (area per area).  I'm always a bit unclear on the best way to document such numbers; while I'm sure that m^2/hectare is a standard way to report these numbers and can at least be supported by EML as a custom unit, perhaps you might consider documenting it as a pure number (converting by the ratio between m^2 and hectare?)  

I'll open new issues for your suggestions.  Handling units more intelligently is long on the to-do list (#12)  and I'll add the csv file naming behavior. I'd love any feedback about the user interface for EML, if you find some functions counterintuitive, tedious or cumbersome.  Documentation should be improving as well.  ",NA,NA,NA
106,39244980,mbjones,2014-04-01T19:03:10Z,2014-04-01T19:03:10Z,"The filename for an object in EML should go in the [`objectName`](https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/./eml-physical.html#objectName) field, which is distinct from `entityName`.  In a data table, the path to the element would be `/eml/dataset/dataTable/physical/objectName`.  ",NA,NA,NA
106,39248480,cboettig,2014-04-01T19:34:24Z,2014-04-01T19:34:24Z,"@mbjones Yes yes, this is what we do.  I've called `objectName` as `filename` and  `entityName` as `title` in my constructor functions such as `eml_dataTable` and `eml_physical`.  

The problem is one of the user interface.  The top-level function `eml_write` has the argument `title`, which I use for the `/eml/dataset/title` value, and reuse for the title for `/eml/dataset/dataTable/physical/objectName` because I thought it would be confusing or tedious to insist the user give explicit titles to each of these.  They can always do so if they want by using the more modular construction, but I wanted a top-level construction that didn't require more input than necessary, and it seems common in EML files that people reuse the values in this way).  It seems that this user also anticipates that `objectName` and `entityName` should be the same by default, instead of my strange choice of using a UUID for an `objectName` unless it is specified.  (I did that to avoid accidental overwriting of an existing file, but probably that was a silly idea).   

What do you think?  Should we avoid re-using elements in this way? How should we generate the names of these various slots if they are not provided?
",NA,NA,NA
106,39271315,mbjones,2014-04-01T23:01:06Z,2014-04-01T23:02:03Z,"@cboettig I'm perplexed by your overview...  are you sure you said that right in the paragraph above?  Why would you use the overall data package title (`/eml/dataset/title`) as a filename for one or more specific files in the package (`/eml/dataset/dataTable/physical/objectName`).  That seems misguided, as the title would rarely be a good filename.   In Morpho, which seems analogous to R, for `objectName` we use the filename of the file that a user asks us to import as our starting value.  If the user is reading a file off of disk, couldn't you default to using that filename?  For data that are generated in R and not connected to a file on disk, maybe a default of 'data_file_n.csv' or similar could be used?

Like you, we occasionally use object identifiers for filenames to avoid name conflicts for our internal storage, but we have to watch out for file naming restrictions on some OSes that reserve use of certain characters (which is also true for user supplied filenames across OSes).  So we often use a name-mangling algorithm to also strip out disallowed characters. It gets messy. Plus, I think its useful for `objectName` in any case to have an appropriate filename suffix as so many OSes use that as a clue to mime-type for opening files.",NA,NA,NA
106,39272630,cboettig,2014-04-01T23:21:24Z,2014-04-01T23:21:24Z,"Thanks, this sounds good.  Yes, we add the appropriate extension already,
and we wouldn't use the title without first replacing characters like
spaces that wouldn't be good in file names, but gets messy as you say

We often have workflows that don't read from a CSV file to start, or at
least don't track the filename, so that's not easy as a generic default
behavior.  (E.g. see my comment on previous issue to Karen)

But I agree that a hash is a poor filename even with the extension. Your
proposed pattern sounds like a much better default.

---
Carl Boettiger
http://carlboettiger.info

sent from mobile device; my apologies for any terseness or typos
On Apr 1, 2014 4:01 PM, ""Matt Jones"" <notifications@github.com> wrote:

> @cboettig <https://github.com/cboettig> I'm perplexed by your overview...
> are you sure you said that right in the paragraph above? Why would you use
> the overall data package title (/eml/dataset/title') as a filename for
> one or more specific files in the package (
> /eml/dataset/dataTable/physical/objectName). That seems misguided, as the
> title would rarely be a good filename. In Morpho, which seems analogous to
> R, forobejctName` we use the filename of the file that a user asks us to
> import as our starting value. If the user is reading a file off of disk,
> couldn't you default to using that filename? For data that are generated in
> R and not connected to a file on disk, maybe a default of 'data_file_n.csv'
> or similar could be used?
>
> Like you, we occasionally use object identifiers for filenames to avoid
> name conflicts for our internal storage, but we have to watch out for file
> naming restrictions on some OSes that reserve use of certain characters
> (which is also true for user supplied filenames across OSes). So we often
> use a name-mangling algorithm to also strip out disallowed characters. It
> gets messy. Plus, I think its useful for objectName in any case to have
> an appropriate filename suffix as so many OSes use that as a clue to
> mime-type for opening files.
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/EML/issues/106#issuecomment-39271315>
> .
>",NA,NA,NA
82,39690029,karen-ross,2014-04-07T01:38:52Z,2014-04-07T01:38:52Z,"Thank you both @cboettig  and @mbjones for your rapid replies to my queries. I'm working part time so my responses will be more sporadic. I'll add more comments in the new issues you've opened for my questions. 

Just a note for you to check some broken links: the links to ""Advanced writing of EML"" and ""Advanced parsing of EML"" back in the original EML page (https://github.com/ropensci/EML) don't seem to work. However, the link to the Advanced writing of EML you gave in your comment above does work.
thanks again",NA,NA,NA
12,39697171,karen-ross,2014-04-07T05:28:28Z,2014-04-07T05:28:28Z,"Hi @cboettig, 

So having read the above and your and @mbjones responses to my units question, you're suggesting two ways to currently handle non-standard units:

1. I could export an STMML file description for the unit if I can find it for example in the LTER unit registry. I am not sure then how to then take this STMML definition into the metadata when using the rEML package?

2. I could write my own custom unit definition as above, providing at a minimum a unit name and description. Again though, being an introductory user, I am unclear on how to get this definition into the metadata in rEML?

Do these non-standard unit definitions always have to be put into the additionalMetadata element?

Could you also give me some pointers on how I could use the ""REST service"" to get unit descriptions into the EML package?

thanks again,
Karen",NA,NA,NA
106,39697342,karen-ross,2014-04-07T05:33:19Z,2014-04-07T05:33:19Z,"@cboettig  Thanks Carl,
Yes that makes sense - that it is necessary for EML to re-create the CSV file to be sure it is all in the correct format. I have now had a chance to look at the advanced writing to EML vignette and can see how you explain I can choose a file name for the .csv instead of the uuid.
many thanks,
Karen",NA,NA,NA
12,39700271,karen-ross,2014-04-07T06:44:54Z,2014-04-07T06:44:54Z,"Hello again,
Please could you give an example of how exactly you would specify and define a character string column in the unit.defs list in the EML package?
many thanks,
Karen",NA,NA,NA
12,39799576,dfalster,2014-04-08T00:24:40Z,2014-04-08T00:24:40Z,"Hi @cboettig,
Many thanks for all the helpful suggestions. The issue of sorting out units seems like a potential sticking point in getting people (including us) to use the package (and EML in general). With current capabilities it will be a lot of work to get the units for our files sorted out. We're thinking about a workflow for quickly identifying units already described and for describing those that aren't. 
Daniel
",NA,NA,NA
12,39804462,cboettig,2014-04-08T01:59:51Z,2014-04-08T01:59:51Z,"Yeah, this is definitely an issue for us to address on our end rather than
something to leave users to work around.  We need a workflow that is simple
but still encourages all the best practices linked in the pdf above.
Thanks to you both for bravely trying out the package before we have closed
these issues and released a stable version to Cran, your feedback really
helps.

Hope to have both these issues you raise addressed soon.

---
Carl Boettiger
http://carlboettiger.info

sent from mobile device; my apologies for any terseness or typos
On Apr 7, 2014 5:24 PM, ""Daniel Falster"" <notifications@github.com> wrote:

> Hi @cboettig <https://github.com/cboettig>,
> Many thanks for all the helpful suggestions. The issue of sorting out
> units seems like a potential sticking point in getting people (including
> us) to use the package (and EML in general). With current capabilities it
> will be a lot of work to get the units for our files sorted out. We're
> thinking about a workflow for quickly identifying units already described
> and for describing those that aren't.
> Daniel
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/EML/issues/12#issuecomment-39799576>
> .
>",NA,NA,NA
12,40335632,karen-ross,2014-04-14T06:07:55Z,2014-04-14T06:07:55Z,"Hi @cboettig, 

Thanks for looking into these issues - appreciate that this is a work in progress not yet released on Cran, and that these things take time to iron out.

In the meantime, it would be great if you could clarify a couple of things:

1. Could you give an example of how exactly you would specify and define a character string column in the unit.defs list in the EML package? I'm really stuck with this one and it's holding me back from getting a lot of the data described. For example, how would I specify the units for the character variables of site, collector and species in the following:

```
 col.defs <- c(""site"" = ""site where sammple was taken"",   
              ""collector"" = ""who collected sample"",
              ""spp"" = ""species name"",
              ""height"" =  ""plant height"",
              ""diameter"" = ""diameter at 10% tree height, or breast height for trees >20m tall"",
              ""sampled"" = ""was individual sampled for biomass and traits?"")

unit.defs = list(site, 
                 collector,
                 species, 
                 ""m"",
                 ""cm"",
                 c(y = ""yes"", NA =""no""))
```
2. Do non-standard unit definitions always have to be put into the additionalMetadata element (as in the example given by mbjones above)?

many thanks!
Karen",NA,NA,NA
12,40432965,cboettig,2014-04-15T00:15:09Z,2014-04-15T00:15:09Z,"Hi @karen-ross 

Good questions.  You can put anything you like as a description for character units -- in EML files that I've looked at, it's reasonably common to simply repeat the column definition. 

```
 col.defs <- c(""site"" = ""site where sample was taken"",   
              ""collector"" = ""who collected sample"",
              ""spp"" = ""species name"",
              ""height"" =  ""plant height"",
              ""diameter"" = ""diameter at 10% tree height, or breast height for trees >20m tall"",
              ""sampled"" = ""was individual sampled for biomass and traits?"")

unit.defs = list(""site"" = ""site where sample was taken"",   
              ""collector"" = ""the first name of the person who collected sample"",
              ""spp"" = ""species scientific name (as genus_species)"",
                 ""m"",
                 ""cm"",
                 c(y = ""yes"", NA =""no""))
```


 However, I think you should ask yourself if it would be more reasonable to consider many of the things you are treating as `characters` as `factors` instead, and define the units as such.  The [README](https://github.com/ropensci/EML) shows an example of encoding species names as factors in the unit data.  Likewise you might do the same for collector.  This allows you to specify more information about the factor -- for instance, your collector column may only use first names for convenience of data entry, so in the metadata you might provide the full name of the person. 


I believe this is an area of some contention among R users at least, with some people preferring to encode factors as character strings because of the strange things R can do to when coercing between factors if you aren't careful.  But I think having them as factors makes sense from the R point of view as well, as it is somewhat more robust to be subsetting by factors than by factor == level then by string == pattern.  Metadata-wise, characters would really only be things that are not levels but literal data strings that you are unlikely to subset by.  (I agree that's vague, one person's characters are another person's factors). 

Also, EML isn't really trying to tell the user whether they should treat data as factors or characters, or anything else really, it's just there to help them describe the data they have.  So don't take my advice too seriously, and just provide a description of the characters if that is what you want.  


Yes, I believe you should always provide the unit definition in additionalMetadata -- don't worry though, once I've written that utility the R package will generate that for you so it won't require any extra work!
",NA,NA,NA
107,40649640,cboettig,2014-04-16T20:43:00Z,2014-04-16T20:43:00Z,"Hi @dfalster ,

Sorry my documentation still sucks.  

Wherever you currently have a line that assigns a `class` to a list, you need to instead call `new`, because these are S4 classes instead of S3 classes.  For instance:

```coffee
creators <- lapply(details$creator, function(x) as(x$name, ""creator""))
class(creators) <- ""ListOfcreator""
```

Should instead be:

```coffee
creators <- new(""ListOfcreator"", lapply(details$creator, function(x) as(x$name, ""creator"")))
```

You'll need to do the same with `method.steps`, `dataTables`, `other_researchers`, and `contact`, and then everything works (at least for me running your script).

Yeah, I agree that feels cumbersome.  I've written concatenate methods, so that when you have just a few `creator`s or a few `methodStep`s you can get a `ListOfcreator` by using `c(creator1, creator2)`.  But your use case with lapply makes sense.  I should probably write coercion methods for the list type, such that you can just hand the list that comes from lapply to any of the slots in `new` and it will attempt to convert it or provide an intelligent error.  Very nice to see your use case, looks awesome

p.s. in a further attempt to confuse you, we decided the package should simply be called ""EML"" instead of ""reml"".  
",NA,NA,NA
107,40650885,cboettig,2014-04-16T20:54:23Z,2014-04-16T20:54:23Z,"One more thing: contact isn't a list, as there must be only one contact.  For that one just adapt your code to: 

```coffee
contact <- make_contact(details$creator)
```",NA,NA,NA
107,40678462,dfalster,2014-04-17T03:54:07Z,2014-04-17T16:09:34Z,"Hi Carl,

Many thanks for the suggestions. The EML builds and validates once your
suggested changes were made. However, I did need to use a different method
to make a list of associatedParties  than other lists (see
https://github.com/dfalster/Falster_2005_JEcol_data/blob/master/build_eml.R#L71
).

So the only issue to finalise is to set the units correctly. Currently all
numeric units are set to ""meter"". Do you know when you might implement new
functionality for handling diverse units?

Also, I went ahead and testing publishing to figshare using rfigshare (see
https://github.com/dfalster/Falster_2005_JEcol_data/blob/master/publish.R).
In addition to the xml file and data csv's, I also uploaded my metadata
files and a readme file in md and html format.

For the moment I've added you as coauthor, in the hope that you can access
the file set (currently private) and provide feedback (
http://figshare.com/preview/_preview/1001506).

A couple of questions re uploading to figshare with rfigshare:
1. The dataset gets created as a 'fileset', but I'd prefer to release it as
'dataset'. The difference is that a 'dataset' has CC0 license. Is there any
way to change this?

2. Is there any way I can test the EML file works? It passed all the
validation tests but I'd really like to test it out before publishing.

best wishes, Daniel

(Cross link to [issue](https://github.com/dfalster/Falster_2005_JEcol_data/issues/3))



On 17 April 2014 06:54, Carl Boettiger <notifications@github.com> wrote:

> One more thing: contact isn't a list, as there must be only one contact.
> For that one just adapt your code to:
>
> contact <- make_contact(details$creator)
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/EML/issues/107#issuecomment-40650885>
> .
>",NA,NA,NA
108,40782463,cboettig,2014-04-18T02:39:00Z,2014-04-18T02:39:00Z,"Hi Ted,

This is the idea behind the XMLSchema package. (Search the issues for some
previous discussions).  In short, it's helpful but that package can't
handle some of EML complexity out of the box.  Even with it, most of the
work making a good user interface and mapping to common R data structures
like data.frame, person, or citation would remain.  Still, it would save a
lot of time on the boring part of writing the S4 classes, and most
importantly would be easy to update if the xsd changed drastically (which
EML isn't likely to do, since so many client apps depend on it)

---
Carl Boettiger
http://carlboettiger.info

sent from mobile device; my apologies for any terseness or typos
On Apr 17, 2014 7:30 PM, ""Edmund Hart"" <notifications@github.com> wrote:

> Hey Carl, I was just thinking about how we're generating EML at NEON using
> pyxb or jaxb to creating objects with bindings from XML. Thinking about
> this idea in R, it would mean scanning an XSD file, and on the fly writing
> an S4 class with slots for every element in the XSD. I'm curious if you
> think some of the approaches you've used EML could be extended to this more
> generalizable package. I'm not sure that it would even be worthwhile to
> recreate what you can already easily do in python, but maybe if much of the
> code in the EML pkg is reusable.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/EML/issues/108>
> .
>",NA,NA,NA
12,40846037,cboettig,2014-04-18T21:27:43Z,2014-04-18T21:36:19Z,"@mbjones I'm finally getting around to this units implementation and wondered if you might clarify a few quick things.  I've reviewed the Best Practices guide but it left a few questions open about the other elements of the schema beyond naming the unit itself that I can't quite figure out from the [schema documentation](http://www.ch.ic.ac.uk/rzepa/codata2/) 

- Is there a table of base class / `unitTypes` somewhere?  I don't see them declared for the [standard unit documentation](https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/eml-unitTypeDefinitions.html#StandardUnitDictionary)  
- Would a user sometimes need to define a `unitType`? 
- Are there cases that would justify using other attributes, such as defining the unit abbreviation? (Seems like that is a case of confusing attribute and unit metadata, as the Best Practices guide discusses, but I see it is available in the STMML vocabulary so am wondering if I should implement it).  

Lastly, @karen-ross's example raises an additional question where the unitType is not obvious to me.  The units are m^2/hectare, I would think of defining this as dimensionless as follows, but maybe that loses the notion of area per area:

```xml
 <additionalMetadata>
 <metadata>
    <unitList>
      <unit id=""metersSquaredPerHectare"" multiplerToSI="".0001"" 
               name=""metersSquaredPerHectare""
               parentSI=""dimensionless"" unitType=""dimensionless"">
        <description>meters squared per hectare (or perhaps a more specific description, 
        like that this is a trunk area per land area or whatever it is??)
        </description>
      </unit>
    </unitList>
  </metadata>
  </additionalMetadata>
```

Perhaps it would it be better to define a new unitType of areaPerArea and then use that? Would that go elsewhere than in the `unitList` ?  


Thanks for clarifying these issues! 

p.s. I think the example you gave above comes from the older schema that didn't use the `<metadata>` element, but we need for in eml-2.1.1 like in my example, right?

",NA,NA,NA
107,40851295,cboettig,2014-04-18T22:50:48Z,2014-04-18T22:50:48Z,"Hi Daniel,

Thanks again for testing all of this out. I'm looking into how best to implement the custom units soon and should have a workable solution soon, (though I hope to refine it further later with your feedback; see #110 for some initial ideas).  Just some quick notes to you queries meanwhile: 

- I can't see the figshare file, not quite sure why.  

- Great point about `dataset` type, thanks for bringing it to my attention.  That type didn't exist when I first wrote the API interface ;-) and `fileset` was the only option for uploading multiple files to a single DOI.  Is now issue #109

- In my opinion, there is actually a substantial advantage to be had publishing your EML to DataONE through NCEAS's  KNB node then publishing it through figshare.  You still get DOIs, etc, but the data is also indexed on the DataONE network, and most importantly, KNB ""speaks"" EML, such that a user can query for any of the metadata provided in the EML file (e.g. ""give me all data with units in metersSquaredPerHectare"").  With figshare we try and extract what metadata we can for the tags and categories, but it is a poor substitute.  

I haven't fully documented this feature, though we have a nice basic implementation already in `eml_knb`, available through `eml_publish`.  I'll try and write up an example since the authentication can be a bit more tricky than figshare (meanwhile you can see my notes on that: http://carlboettiger.info/2013/10/10/notes.html). (Also @mbjones is working on a rewrite of the `dataone` package we use for this, which would avoid the potentially troublesome java dependencies and expose some new features.) Would you be willing to give that a go?  ",NA,NA,NA
12,40852487,mbjones,2014-04-18T23:14:01Z,2014-04-18T23:14:01Z,"Heya @cboettig,

A few remarks on your questions:
- We've listed the unitTypes that ship with EML in the [eml-unitDictionary.xml](https://code.ecoinformatics.org/code/eml/tags/RELEASE_EML_2_1_1/eml-unitDictionary.xml) file that is included in the EML distribution.  That list, in turn, is derived from the list of base quantities and derived quantities provided by NIST in their page on [NIST units](http://physics.nist.gov/cuu/Units/units.html).  NIST provides a much more complete description of the concepts behind units and quantities.  The only new baseQuantity that has been discussed is currency, which isn't technically a physical quantity at all, and is only barely definable.  Currencies are certainly not in the same category as for example `meter` in terms of their use as a unit because their values change arbitrarily over time.
- People will from time to time need to define new `unitType`s.  For the most part, this should only be needed when a dimensional analysis of a unit shows that there is no other unitType that shares that dimensionality.  For example, if someone wants to create a unit for density in numbers per unit area, they could use the existing `unitType` `arealDensity`, and they would provide the conversion factor `multiplierToSI` for the base unit for that `unitType`, in this case `numberPerMeterSquared`.  You can tell that a unit is the base unit for its unitType because it will have a `multiplierToSI` of 1.
- When defining units in STMML, it is certainly useful to include attributes such as the abbreviation; you'll see that most units in eml-unitDictionary.xml include these attributes in the STMML definitions.

Regarding @karen-ross's  example, that is a classic case of a dimensionless quantity (area per area) in which the two units in the numerator and denominator cancel.  In her case, a square meter is the base unit for area, and a hectare is 10^4 square meter, so the ratio of those is 10^-4, which is a dimensionless number.  The problem with these dimensionless numbers is that you can't treat any two dimensionless numbers as equivalent, as sometime it is important to know what kind of entity was measured in the numerator and denominator.  For example, the mass ratio of two substances (e.g., a CN ratio) will be dimensionless, but it is critical to know that the numerator is Carbon and the denominator is Nitrogen, even if they are both measured in the unit `grams` that cancel.  With our work on [OBOE](http://dx.doi.org/10.1016/j.ecoinf.2007.05.004), we have extended our definitions of units to include these semantics of the Entity so that these ratio measures can be better labeled, compared, and understood than is possible with traditional metadata standards like EML.

If you are creating some methods for creating unitTypes or units, it would be useful to compare them to the standard set and make sure the new unit doesn't match an existing one (in terms of `parentSI` and `multiplierToSI`), and that new unitTypes don't match an existing unitType in terms of dimensional analysis. Warnings in those areas would help reduce duplication (although it technically doesn't matter because we should be able to programmatically find any duplicates).",NA,NA,NA
108,40853630,mbjones,2014-04-18T23:40:14Z,2014-04-18T23:40:14Z,"XMLSchema in R isn't the only package that struggles with complex schemas.  We use jaxb and pyxb in DataONE, and they struggle with some of our schema constructs as well.  Recursion in a schema gets them all as far as I know.",NA,NA,NA
12,40860018,cboettig,2014-04-19T03:57:30Z,2014-04-19T04:04:40Z,"Hi @mbjones!

Thanks much!  More questions on `unitType` to come I think in a different issue, but one more quick question for now:

I'm not clear which `id` I should prefer to use in the `describes` element of `additionalMetadata` element containing the `unitList` (something that I guess didn't exist in the earlier schema in the example above).  It seems most precise to use the `id` of the `unit` being defined, but if the `metadata` element contains the entire `unitList` this would create a problem.  It seems that the logical options are either:

- use the id of the `attributeList` in the `additionalMetadata/describes`, __or__ 
- create one `unitList` per `unit`, such that each `unit` got it's own metadata element and could refer directly to the unit definition.  


Regardless of the choice we should be able to figure out the unit definition by matching the unit names, so the `describes` reference doesn't seem crucial.  What do you recommend here?  
",NA,NA,NA
107,40861800,cboettig,2014-04-19T05:52:26Z,2014-04-19T05:52:26Z,"Okay, I've added a interface for custom units to the current `devel` branch. You don't have to mess around with `additionalMetadata` as it should be handled automatically.  Just define the unit itself, like this [^1]:

```coffee
  create_custom_unit(id = ""metersSquaredPerHectare"",
                     parentSI = ""dimensionless"",
                     unitType = ""dimensionless"",
                     multiplierToSI = ""0.0001"",
                     description = ""Square meters per hectare"")
```

and then use the `id` you give as the unit type in your `unit.defs`.  `create_custom_unit` updates a `custom_units` list in the `EMLConfig` environment, which the `eml` or `write_eml` functions detect and use to write in the additional metadata.  [^2]

[^1]: Note that I just use ""dimensionless"" as an illustration here, a the better choice might perhaps be to define an area/area `unitType` as @mbjones discusses in #12.  (We'll add support for OBOE semantics eventually, but that's much more complex)

[^2]: This isn't ideal, since the use of environments means the function has ""side effects"" and you could inadvertently include unit definitions you defined for some other reason in your EML file (if you didn't do `eml_reset_config()` or start a fresh R session).  That wouldn't break anything technical, but would seem a bit strange to define units you didn't use.  The environment mechanism can be avoided by explicitly passing list of custom_units to the `eml` or `eml_write` functions.  ",NA,NA,NA
111,41303376,cboettig,2014-04-24T16:45:28Z,2014-04-24T16:45:28Z,"Thanks!  That documentation needs further work now anyway, to show some of the custom unit handling.  The EML package now maintains a copy of the standard unit list: 
https://github.com/ropensci/EML/blob/master/inst/units/standard_unit_list.csv

which I currently use to check if the unit provided is a standard unit or not: https://github.com/ropensci/EML/blob/8c5b7c66a89bee53abab9cb86759e10b80088202/R/unit_methods.R#L177  

I think ideally this should do fuzzy matching (#110), so that a user can declare units without writing out the camelCase, e.g. perhaps it would recognize `kilogram per meter squared` or even `kg/m^2` as `kilogramPerSquareMeter`, etc.  

I should at least update the vignette to show the use custom units, as described here https://github.com/ropensci/EML/issues/107#issuecomment-40861800",NA,NA,NA
111,41303457,cboettig,2014-04-24T16:46:06Z,2014-04-24T16:46:06Z,p.s. probably better to send pull requests to the `devel` branch,NA,NA,NA
12,41517611,dfalster,2014-04-28T02:04:08Z,2014-04-28T02:04:08Z,"Hi @mbjones and @cboettig,

Thanks for suggestions re new unit types. I have been looking at the
standard list of EML units with @karen-ross find suitable base SI units for
most of our variables. However, I cannot find any suitable base units for
the following:

- ""g m-1"",
- ""mg mm-1"",
- ""mm-1""

So I guess I'll need to define some new unit types for those. But it's
unclear from examples above what I should list for several of the
additional meta data fields when there is no logical parent, i.e. just
leave as empty? If so, would the following suit (for g m^-1)

```
<additionalMetadata>
 <metadata>
    <unitList>
      <unit id=""gramsPerMeter"" multiplerToSI="""" name=""gramsPerMeter""
       parentSI="""" unitType="""">
        <description>Grams per meter</description>
      </unit>
    </unitList>
  </metadata>
  </additionalMetadata>

many thanks!


On 19 April 2014 13:57, Carl Boettiger <notifications@github.com> wrote:

> Hi @mbjones <https://github.com/mbjones>!
>
> Thanks much! More questions on unitType to come I think in a different
> issue, but one more quick question for now:
>
> I'm not clear which id I should prefer to use in the describes element of
> metadata element containing the unitList (something that I guess didn't
> exist in the earlier schema in the example above). It seems most precise to
> use the id of the unit being defined, but if the metadata element
> contains the entire unitList this would create a problem. It seems that
> the logical options are either:
>
>    - use the id of the attributeList in the metadata/describes, *or*
>    - create one unitList per unit, such that each unit got it's own
>    metadata element and could refer directly to the unit definition.
>
> Regardless of the choice we should be able to figure out the unit
> definition by matching the unit names, so the describes reference doesn't
> seem crucial. What do you recommend here?
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/EML/issues/12#issuecomment-40860018>
> .
>",NA,NA,NA
107,41519673,dfalster,2014-04-28T03:07:03Z,2014-04-28T03:07:03Z,"Hi Carl,

Thanks again for all the suggestions and apologies for my slow reply.

I see that you are quite enthusiastic about knb because it works in EML by
default- nice! We'll definitely consider it as a possible destination. At
the same time, I quite like the flexibility of figshare, and I'm sure
indexing of figshare will improve over time (for example, I wonder if data
one will start to index figshare sometime soon, in the same way they are
indexing dryad).

I haven't yet tried the options for uploading to knb.

As you can see, I listed a couple of other issues re units and figshare. I
hope the constant flow of issues from us is indeed helpful for ironing out
issues with eml package, as from my end it feels like I'm hassling you a
lot.

With warm thanks,
Daniel





On 19 April 2014 15:52, Carl Boettiger <notifications@github.com> wrote:

> Okay, I've added a interface for custom units to the current develbranch. You don't have to mess around with
> additionalMetadata as it should be handled automatically. Just define the
> unit itself, like this [^1]:
>
>   create_custom_unit(id = ""metersSquaredPerHectare"",
>                      parentSI = ""dimensionless"",
>                      unitType = ""dimensionless"",
>                      multiplierToSI = ""0.0001"",
>                      description = ""Square meters per hectare"")
>
> and then use the id you give as the unit type in your unit.defs.
> create_custom_unit updates a custom_units list in the EMLConfigenvironment, which the
> eml or write_eml functions detect and use to write in the additional
> metadata. [^2]
>
> [^1]: Note that I just use ""dimensionless"" as an illustration here, a the
> better choice might perhaps be to define an area/area unitType as @mbjones<https://github.com/mbjones>discusses in
> #12 <https://github.com/ropensci/EML/issues/12>. (We'll add support for
> OBOE semantics eventually, but that's much more complex)
>
> [^2]: This isn't ideal, since the use of environments means the function
> has ""side effects"" and you could inadvertently include unit definitions you
> defined for some other reason in your EML file (if you didn't do
> eml_reset_config() or start a fresh R session). That wouldn't break
> anything technical, but would seem a bit strange to define units you didn't
> use. The environment mechanism can be avoided by explicitly passing list of
> custom_units to the eml or eml_write functions.
>
> --
> Reply to this email directly or view it on GitHub<https://github.com/ropensci/EML/issues/107#issuecomment-40861800>
> .
>",NA,NA,NA
107,41583003,cboettig,2014-04-28T16:51:07Z,2014-04-28T16:51:07Z,"No hassle, as you say the issues are indeed helpful for us, so keep them coming.  

Yeah, the convenience of figshare is quite nice, though I believe KNB / DataONE can also accept just about anything one could put on figshare, it doesn't have to be tabular data etc.  @mbjones could probably comment more accurately on this.  

figshare's handling of metadata is sometimes frustratingly minimal -- for instance, their DataCite records are about as minimal as possible, not even distinguishing between a paper and a dataset or stating the license terms.  I've known Mark since before figshare and he's been quite responsive in implementing things in the past though, so I'm hopeful this will improve.  I've spoken to Mark about becoming a DataONE member node during the last call for member node training (I believe the repositories have to apply and implement certain standards, rather than DataONE just choosing to index them), and it's something his team is already thinking about. 

Meanwhile, also hopeful the DataONE login etc might become as convenient as figshare.  If you have specific observations about things you like in figshare that aren't in KNB, I'm sure @mbjones and team would love to hear them.  ",NA,NA,NA
111,46526877,ivanhanigan,2014-06-19T06:14:13Z,2014-06-19T06:14:13Z,"Hi Carl, Interesting you use the kilogramPerSquareMeter as an example.  Out of interest what would you use for say number of individuals per square meter?  I've looked at some options in the standard unit list but nothing jumps out at me.",NA,NA,NA
111,46572831,cboettig,2014-06-19T15:09:03Z,2014-06-19T15:09:03Z,"Looks like numberPerMeterSquared is on the standard list (no idea why some
units call it MeterSquared and some and SquareMeter...)

---
Carl Boettiger
http://carlboettiger.info

sent from mobile device; my apologies for any terseness or typos
On Jun 18, 2014 11:14 PM, ""Ivan Hanigan"" <notifications@github.com> wrote:

> Hi Carl, Interesting you use the kilogramPerSquareMeter as an example. Out
> of interest what would you use for say number of individuals per square
> meter? I've looked at some options in the standard unit list but nothing
> jumps out at me.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/pull/111#issuecomment-46526877>.
>",NA,NA,NA
111,46602490,ivanhanigan,2014-06-19T19:00:12Z,2014-06-19T19:00:12Z,"Thanks Carl, sorry I was dumb.
I've been working more in Morpho recently and it did not jump out of the options in that drop down menu.

It turns out this is an option in morpho found under the secondary drop-down menu, once the first is set to areal density.
I found this out by going to the .morpho folder, opened the raw XML and pasted into the tag like this, then opened in morpho and then 'edit column documentation'

    <attribute id=""1394589176349"" scope=""document""> <attributeName>&quot;Before&quot;</attributeName>
    <attributeDefinition>Before treatment</attributeDefinition>
    <measurementScale> <ratio> <unit> <standardUnit>numberPerMeterSquared</standardUnit>
    </unit>
    <numericDomain> <numberType>real</numberType>
    </numericDomain>
    </ratio>
    </measurementScale>
    </attribute>

Thanks",NA,NA,NA
111,46603445,mbjones,2014-06-19T19:08:38Z,2014-06-19T19:08:38Z,"Sorry, @ivanhanigan, that the contextual menus aren't more intuitive in Morpho.  Others have had similar issues to you, but we also have the problem that a single unstructured list of units is far too long and difficult to navigate as well.  So, we cluster them by unit type.  If you have suggestions as to how to improve that, we'd be grateful.",NA,NA,NA
111,46608023,ivanhanigan,2014-06-19T19:49:25Z,2014-06-19T19:49:25Z,"Thanks @mbjones.  I am grateful you and Carl didn't just tell me to go and RTFM!  I think users like me probably should not expect these to 'jump out of the list'.

But just brainstorming ways to make it better perhaps Carl's point about fuzzy matching (https://github.com/ropensci/EML/pull/111#issuecomment-41303376) might go some way to enhance findability? 

A quick grab from my notes about tools in this space are:

- https://github.com/markvanderloo/stringdist
- http://notesofdabbler.bitbucket.org/norvigSpellCheck/norvigNLP.html
 
It'd be really great to use something like Google’s natural language processing

- http://scweiss.blogspot.com.au/2014/05/didyoumean-function-using-google-to.html",NA,NA,NA
62,46705105,cboettig,2014-06-20T17:33:35Z,2014-06-20T17:33:35Z,"@emhart Just going through my issues list and wanted to see if you were still interested in exploring this.  There's a basic example in [Advanced Writing of EML](https://github.com/ropensci/EML/blob/devel/vignettes/Advanced_writing_of_EML.md) Vignette. Installation is still a bit hacky, but I've written a script to get it working on travis (since I configured travis to check vignettes, which it doesn't do by default).  See [.travis.yml](https://github.com/ropensci/EML/blob/devel/.travis.yml) 

I know this isn't a trivial feature to support (at least for anything richer than my basic use case in the vignette, which just drops the full text of a Word doc into the <methods> node), but given the prominence of Word it's probably worth it...",NA,NA,NA
102,46705153,cboettig,2014-06-20T17:34:02Z,2014-06-20T17:34:02Z,"Now that we're deprecating the `data.set` object, I think we'll consider this closed as well. ",NA,NA,NA
62,46706278,emhart,2014-06-20T17:44:32Z,2014-06-20T17:44:32Z,"I had let this fall by the wayside because my concern is that there's no support for the dependency.  It hasn't been worked on for 3 years, and I can't even reliably install it.  So the link to the source from your travis file points to an empty directory: http://www.omegahat.org/R/src/contrib/RWordXML/, and when I run the line from travis I get:
```r
>install.packages(""RWordXML"", repos=""http://www.omegahat.org/R"", type=""source"")
Installing package into ‘/Users/thart/.Rlibs’
(as ‘lib’ is unspecified)
Warning in install.packages :
  package ‘RWordXML’ is not available (for R version 3.0.3)
```
And I still can't install it via github.  Given the unreliability of this library I don't think this is worth the effort. 
",NA,NA,NA
62,46706451,duncantl,2014-06-20T17:46:29Z,2014-06-20T17:46:29Z,"If we want RWordXML to work, it isn't very hard to get it to do so. ",NA,NA,NA
62,46706827,cboettig,2014-06-20T17:50:03Z,2014-06-20T17:50:03Z,"@emhart Sorry, there's a few different lines in the .travis.yml that refer to it (I never commented out the deprecated lines).  You should be looking a bit higher up in the .travis.yml, where the RScript just calls this: 

https://github.com/ropensci/EML/blob/devel/inst/examples/install_RWordXML.R

(Obviously the install works since Scott has configured travis to test the package every day....)

@duncantl Yeah, a slightly more streamlined install (e.g. that works from CRAN or omegahat directly) would be helpful. ",NA,NA,NA
62,46707378,duncantl,2014-06-20T17:55:07Z,2014-06-20T17:55:07Z,"The packages are there in the source repository and 
 install.packages(""RWordXML"", repos = ""http://www.omegahat.org/R"")
works for me.",NA,NA,NA
62,46707555,emhart,2014-06-20T17:56:43Z,2014-06-20T17:56:43Z,"@duncantl I'm sure that it can work, but thinking about end users, the current hoops needed for installation don't make it feasible to use the package in something that is deployed to CRAN.   

```r
> install.packages(""RWordXML"", repos = ""http://www.omegahat.org/R"")
Installing package into ‘/Users/thart/.Rlibs’
(as ‘lib’ is unspecified)
Warning in install.packages :
  cannot open: HTTP status was '404 Not Found'
Warning in install.packages :
  cannot open: HTTP status was '404 Not Found'
Warning in install.packages :
  unable to access index for repository http://www.omegahat.org/R/bin/macosx/contrib/3.0
Warning in install.packages :
  package ‘RWordXML’ is not available (for R version 3.0.3)
```",NA,NA,NA
62,46708394,duncantl,2014-06-20T18:04:18Z,2014-06-20T18:04:18Z,"As I said, they are in the source repository, not in the binary sub-directories.
So you need type = ""source"" in the call to install.packages().
I don't care if EML wants to use RWordXML or not, but the issues of getting it to work for end users ""deployed to CRAN"" isn't an issue.   R checks CRAN and Omegahat, along with BioC. And the packages could be put on CRAN.",NA,NA,NA
62,46709632,cboettig,2014-06-20T18:15:58Z,2014-06-20T18:15:58Z,"@duncantl Yup, I realize R checks CRAN and Omegahat, so I wasn't worried about that part. Its just some of the dependencies for `RWordXML`  that I needed to install manually first in that script I linked (perhaps you already had them installed on your machine). The current travis setup first does:

```r
download.file(""http://cran.r-project.org/src/contrib/Archive/rimage/rimage_0.5-8.2.tar.gz"", ""rimage.tar.gz"")
untar(""rimage.tar.gz"")
writeLines("""", ""rimage/NAMESPACE"")
install.packages(""rimage/"", repos=NULL)

## Ugh, install_github fails on this one too!
download.file(""https://github.com/omegahat/Rcompression/archive/master.zip"", ""Rcompression.zip"", ""wget"")
unzip(""Rcompression.zip"")
setwd(""Rcompression-master"")
system(""chmod a+x configure"")
library(""devtools"")
install(""."")
```

Haven't checked if RCompression and rimage dependencies install with `install.packages` in the past few months.",NA,NA,NA
62,46710129,duncantl,2014-06-20T18:20:13Z,2014-06-20T18:20:13Z,"rimage  isn't necessary anymore. It was used to determine the size of JPEG image. 
We'll use a different package for that.
Again, I don't care if RWordXML is used. However, if one wants the functionality of reading Word documents, these issues are quite easily solved. ",NA,NA,NA
115,46710950,cboettig,2014-06-20T18:27:35Z,2014-06-20T18:27:35Z,"@emhart Good point, we should make it so that install_github() works out of the box. 

Building the vignettes on install is just what R does unless you tell it not to.  You can try: 

```r
install_github(""ropensci/EML"", build=FALSE)
```

If you don't want to build vignettes, but the package cannot just change the default behavior of R's install functions. 

Note that installations from CRAN won't have this problem, since most users will get the binaries, and by the time we're on CRAN the external dependencies that are giving you trouble will all be on native R repositories (Omegahat or CRAN).  
",NA,NA,NA
62,46711694,cboettig,2014-06-20T18:34:24Z,2014-06-20T18:34:24Z,"@duncantl I'd like to use RWordXML, I think it's a neat use case. 

Thanks for dropping `rimage`, and it looks like I can install RCompression from omegahat just fine, such that

```r
install.packages(""RWordXML"", repos=""http://www.omegahat.org/R"", type=""source"")
```

is now working for me without any high jinks. Yay! will update .travis.yml.  Think you can get binaries for RWordXML up on omegahat? @emhart Does the above install work for you now?",NA,NA,NA
62,46712025,emhart,2014-06-20T18:37:00Z,2014-06-20T18:37:00Z,"It only works from vanilla R for some reason, not within Rstudio.  Which is
weird, but I guess if that's a common occurrence we can cross that bridge
when we come to it.


On Fri, Jun 20, 2014 at 12:34 PM, Carl Boettiger <notifications@github.com>
wrote:

> @duncantl <https://github.com/duncantl> I'd like to use RWordXML, I think
> it's a neat use case.
>
> Thanks for dropping rimage, and it looks like I can install RCompression
> from omegahat just fine, such that
>
> install.packages(""RWordXML"", repos=""http://www.omegahat.org/R"", type=""source"")
>
> is now working for me without any high jinks. Yay! will update
> .travis.yml. Think you can get binaries for RWordXML up on omegahat?
> @emhart <https://github.com/emhart> Does the above install work for you
> now?
>
> --
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/62#issuecomment-46711694>.
>



-- 
Edmund M. Hart, PhD
Staff Scientist - Ecoinformatics
National Ecological Observatory Network
@distribecology
http://emhart.info <http://emhart.github.com/>",NA,NA,NA
62,46712973,karthik,2014-06-20T18:45:52Z,2014-06-20T18:45:52Z,"Hi @cboettig @duncantl

I installed `EML` on a bare ubuntu box earlier this week. It was the biggest bear to install but not impossible. You can see the install script I used here: https://github.com/ropensci/workshop-stanford-2014-06/blob/master/scripts/install.R

All the vignettes compiled fine and you can spin up the AMI for yourself (`ami-a69b65ce`). It will automatically launch a RStudio server and you can use any username `user[1-40]` with the password `ropensci`. 

Only thing that didn't work was [`Sxslt`] (https://github.com/ropensci/workshop-stanford-2014-06/blob/master/scripts/install.R#L49) but that was expected.

I agree that we should work to lower the burden to set up since it would really difficult for most users to do a simple `github_install` as one can do even with more complex packages like `dplyr`.

But I don't think that packages being on OmegaHat is a problem since CRAN allows those dependencies. It might just require an update to install on R `3.1` without throwing warnings.",NA,NA,NA
62,46714096,emhart,2014-06-20T18:56:15Z,2014-06-20T18:56:15Z,"@cboettig Now that I have it up and running I was just working through your [example above](https://github.com/ropensci/EML/issues/62#issuecomment-29940701), and I'm not sure what you're trying to do with the `methods` call.  I get the following error:
```r
>  doc <- methods[[getDocument(f)]]
Error in methods[[getDocument(f)]] : 
  object of type 'closure' is not subsettable
```",NA,NA,NA
116,46714676,cboettig,2014-06-20T19:01:34Z,2014-06-20T19:01:34Z,"I think you're specifically interested in constructing non-standard units?
 For completely interactive EML construction online it seems to me it might
be better to build directly from Morpho...  Perhaps we need a convenient
way to start creating EML from, say, R, but then read the EML into Morpho
for parts where an interactive interface is most necessary?  @mbjones how
hard would it be to have a browser-based morpho tool, in place of the
standalone desktop javascript environment?

For instance, the team at Harvard Forest LTER mentioned that they currently
tend to construct most of the boiler plate with Oxygen (a proprietary XML
editor), because Morpho is a bit tedious for that stuff, and then use
Morpho for the attributeList / unit definitions.  Perhaps we could
accomplish a similar interaction with R.

just thinking out loud here, so I could be spouting nonsense.


On Fri, Jun 20, 2014 at 11:51 AM, Karthik Ram <notifications@github.com>
wrote:

> As discussed with @cboettig <https://github.com/cboettig> I'll push a
> shiny module that will make it easier to document metadata in a browser
> than use the command line wizard which can be painful to use if you don't
> know what option to use or want to document fields out of order.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/116>.
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
62,46714843,cboettig,2014-06-20T19:03:04Z,2014-06-20T19:03:04Z,@emhart the example in the issue tracker you link is from the early days of thinking about this ... please see the Advanced Writing vignette I linked for an example that is *hopefully* working (at least appears to run without errors on the daily travis checks).  ,NA,NA,NA
116,46715076,karthik,2014-06-20T19:05:06Z,2014-06-20T19:05:06Z,"> I think you're specifically interested in constructing non-standard units?

No. At least not at the moment. But if one does not know what unit to choose with going through the R command line wizard (as I experienced myself), then there is no choice but to quit and restart. 

I'm thinking of a fairly simple Shiny interface (to begin with) that populates all fields into a form, with the units and description as two other columns. The units will have a dropdown. Once everything is correctly set up, clicking ok, should return the user back to the R console. As an added bonus, users can copy this out and reuse programmatically to avoid going back to the menu in the future. At least that's my thinking.",NA,NA,NA
116,46716178,cboettig,2014-06-20T19:15:44Z,2014-06-20T19:15:44Z,"Yeah, sounds great to me.

Yeah, I meant to say standard units.  I agree that the text-based
interactive wizard for entering units is pretty much unusable at the moment
(it was mostly a proof-of-principle placeholder); and while it could be
improved it would probably be wise to just switch it off for the time.

Simple sounds good; I do think there's a role for a lightweight interface
to help with common tasks like units. Just wanted to make sure you weren't
aiming for a full-blown implementation of Morpho in shiny.

The reuse element sounds great as well.  It always takes some time getting
started annotating, but the thing that frustrated me with Morpho was that
it didn't get much quicker even after I'd done a very similar dataset.
 Certainly programmatic construction and reuse of annotation elements
should be the strength of the R package, making annotation easy to do,
edit, and take advantage of regularly.


On Fri, Jun 20, 2014 at 12:05 PM, Karthik Ram <notifications@github.com>
wrote:

> I think you're specifically interested in constructing non-standard units?
>
> No. At least not at the moment. But if one does not know what unit to
> choose with going through the R command line wizard (as I experienced
> myself), then there is no choice but to quit and restart.
>
> I'm thinking of a fairly simple Shiny interface (to begin with) that
> populates all fields into a form, with the units and description as two
> other columns. The units will have a dropdown. Once everything is correctly
> set up, clicking ok, should return the user back to the R console. As an
> added bonus, users can copy this out and reuse programmatically to avoid
> going back to the menu in the future. At least that's my thinking.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/116#issuecomment-46715076>.
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
116,46716726,mbjones,2014-06-20T19:21:20Z,2014-06-20T19:21:20Z,"@cboettig A web-based metadata editor that had the level of detail as Morpho would be great.  We have a high-level web-based editor already -- that's what you get using the 'Share' link on the KNB.  But it doesn't include entity and attribute metadata, which is when we send people to Morpho. We have contemplated doing this for a long time, but just have never gotten over the hump because we had a tool that worked.  We view Morpho as a way to collect metadata for a few data sets, or by people who haven't done it much so need the guidance.  Once you have to generate a lot of metadata, we recommend using a scripted system with metadata templates.  Connecting the EML R package to a web based system would be a win-win.

There's a fair amount of detail to be collected, and it can be tedious.  In Morpho, we parse the data files, looking to set as much metadata automatically as possible -- for example, we try to detect the delimiters, character encoding, variable names, variable types, enumerated values, and other fields to the best of our ability so the user only has to review and fill in code definitions.  This speeds up the process of filling out metadata a lot.  Moving this type of a system to the web would be a huge advance for most users (although we do have a fair contingent of users at field stations that use Morpho disconnected from the net, and then upload later when they regain Internet connectivity).
",NA,NA,NA
62,46720380,emhart,2014-06-20T20:00:58Z,2014-06-20T20:00:58Z,"Yup, that one is working. I guess a couple of thoughts then now that we're cranking on this topic before I have to jump to a meeting.

1).  Do we want to dump the whole methods section into EML?  It seems like most people would want a subset of the methods (at least when I look at existing EML files in KNB vs the actual publications).  If not how do you decide part of the methods goes in the EML?

2). We should think about other things we might offer.  For instance, we could try and extract taxanomic coverage with regex perhaps, and the abstract should also be pretty simple.

3). What about templates for publications.  So if your publication is formatted for Ecology, we could build a simple set of parsers to grab MD, but that wouldn't work for Oikos necessarily.  Just a thought.
",NA,NA,NA
62,46722522,cboettig,2014-06-20T20:23:19Z,2014-06-20T20:23:19Z,"@emhart Great questions, I'm open to ideas.  For my little example in that vignette I was just thinking of the user using Word to compose a methods section (instead of typing it into an R console or plain text editor).  

I hadn't thought of just grabbing the methods section directly from manuscript, though that's an interesting idea. (The example from harvest forest, hf205.xml, that I use in the vignette has a methods section in the EML that is both different and much longer than the methods in their PNAS paper.  Haven't looked too closely at many other EML examples of methods though).  ",NA,NA,NA
62,46727191,mbjones,2014-06-20T21:12:16Z,2014-06-20T21:12:16Z,"Although I think its great to be able to grab methods from a paper, my initial reaction is that most of those will be less detailed than we want to encourage (although, some is always better than none!).  The methods describing data collection should be complete if possible, and include all experimental design, sampling, and methodological details.  Whereas journal papers still typically have extremely abbreviated methods sections (although maybe we'd do better extracting from a thesis or dissertation chapter).  So, although its good to be able to import from papers, I suspect we'd want people to grab methods from other written documents they might have.  It would be interesting to know where people keep these if not in a handwritten lab notebook now.  So, maybe what we need is the ability to extract a particular (numbered?) section from a document.  

Maybe it would be worthwhile to ask a few tens of researchers where they store their methods before embarking on implementation -- might save time if we can discover some generalities.",NA,NA,NA
62,46728363,cboettig,2014-06-20T21:25:07Z,2014-06-20T21:25:07Z,"@mbjones Thanks for weighing in, that sounds good to me.  I agree that supporting arbitrary docx formats is likely to get complicated quickly. 

Another thing I was thinking about when first exploring the docx parsing is the ability to enter other metadata elements beyond methods. I noticed that the Harvard Forest group, for instance, was using a docx template that they asked researchers to fill out, and then someone had to manually generate the EML file using Oxygen and Morpho, with the docx as a reference.  My thought was that if people have found that approach more effective with researchers than expecting them to learn to use an EML tool themselves, well, we could at least automatically parse the docx. 

For instance, the EML package could ship with a docx template file, and users paste ""methods"" etc under the appropriate headings.  I imagine many researchers are at least as comfortable copy-pasting from wherever they keep their methods into a word doc as they are specifying a path to said document on the command line (if not more so).  

This way, we'd have more control over what the docx headings / node structure looked like, and users could benefit from the template. I believe @emhart mentioned that even at NEON they use a lot of docx files for templated data entry too, just as HF does.  If that's a workflow people already like and if it gives us sane XML, then I think it's worth a stab.  


",NA,NA,NA
62,46731361,cboettig,2014-06-20T22:00:27Z,2014-06-20T22:00:27Z,"@karthik 

Sorry about the headache of EML dependencies for your last workshop.  

Note that all of the non CRAN dependencies are not needed for most of the EML functions.  The only reason you're having trouble has to do with `install_github` deciding to do things differently than native R / install.packages.  Those packages are on the `SUGGESTS` list, which is ignored by default when a user install from CRAN, but not ignored by install_github (partly because devtools::install() insists on building the vignettes, which needs the suggests).  

If you simply do

```r
devtools::install_github(""ropensci/EML"", build=F, dependencies=c(""IMPORTS"", ""DEPENDS""))
```

you'll get the default install.packages behavior, which should install all the functionality you need to use the package without having to manually worry about dependencies. Only fancier things in the Advanced vignettes use RWordXML or Sxslt. (`eml_validate` uses RHTMLForms, but it has a fallback to XML's schema validation only if not available). Blame `install_github` for having different default behavior than what install.packages() will do once we're on CRAN.... ;-)

I'll also add that a note about this to the README. ",NA,NA,NA
78,46731994,cboettig,2014-06-20T22:09:23Z,2014-06-20T22:09:23Z,"I've implemented a basic date parser.  It's not completely satisfactory in the multiple-column case #17, but does detect a handful of common formats. See code: https://github.com/ropensci/EML/blob/6f052acfb64ece7f0577f7842a9c8ed28910a5e0/R/eml_get.R#L113-123

",NA,NA,NA
62,46732056,emhart,2014-06-20T22:10:07Z,2014-06-20T22:10:07Z,"I was thinking of creating a way that people could generate EML with as little headaches as possible.  I guess it might be useful in specific cases like HF.  Here we won't be parsing docx into EML, but we will need to parse docx into DB's and then to EML.  I guess what I imagined was someone writes a paper, and then calls a fxn `generateEML(myDocx)`, and then it text mines and populates the EML objects.  There's probably a lot of stuff we could grab...like author information, contact info, affiliation, even temporal, spatial scale, and taxonomic coverage.  Then people could view what was mined, and change it, but it gets them 50% of the way there in terms of creating the EML.  I'll be the first to admit this is probably way too ambitious.  Getting the text is the easy part, it's mining it that's really hard.

Also, while I'm just wildly speculating, a web app that uses machine learning to extract metadata.  People can upload PDF's or we could crawl for them, and then use ML to extract specific elements of MD.  An ecology MD generation engine essentially. Sorry, I saw too many talks on ML and situation awareness at a conference this past week.   ",NA,NA,NA
17,46732060,cboettig,2014-06-20T22:10:11Z,2014-06-20T22:10:11Z,"Seems like some configurable options are in order to deal with this.  

1. Allow columns containing dateTime data to be treated as `character` or `factor` (all or specified columns)
1. Collapse separate columns containing a single date into a single column.  Would either need to specify which columns to collapse or have a way to identify consecutive related columns. (E.g. we need to avoid collapsing unrelated columns -- e.g. wrongly concatenate the month of a 'first flower' column with the day of a 'first fruit' column.  etc,  
1. Coerce columns into dateTimes regardless of precision/multiple columns


",NA,NA,NA
117,46732363,cboettig,2014-06-20T22:14:22Z,2014-06-20T22:14:22Z,"Just realized this is already implemented in `eml_publish` via `eml_figshare`.  Just pass either function a value to `figshare_id =137` or whatnot and it will update the specified id.  (Recall that eml_publish passes all extra arguments to eml_figshare via ...)   

Let me know if you think that interface can be improved (e.g. rename the argument from `figshare_id` to something else?) or if you think a wrapper would still be helpful.  Sorry I didn't read my own documentation earlier...",NA,NA,NA
62,46733661,cboettig,2014-06-20T22:33:57Z,2014-06-20T22:33:57Z,"Yeah, the ML idea is definitely interesting, but I think somewhat of a
philosophical divide from EML too.  One of the defining features of the XML
schema approach is that it's designed to avoid the fuzziness of ML.  We
wouldn't need structured tags if everything could be guessed by context.
 It's partly an issue of trust: EML uses all those XML tags explicitly to
avoid all the ways in which a ML algorithm could misinterpret what's what,
because ultimately a human was involved in that classification.

I'm trying to say one approach is better than the other, but I think we
should be cautious about mixing them freely. The EML isn't supposed to be a
summary of what a computer thinks is the metadata, but what the original
author says is the metadata.

Even when that metadata is coming from a far more reliable, structured
source than a ML algorithm, informatics folks can be very careful to
document the provenance of just how it came up. We saw a good example from
@hilmar in the NeXML library, where he recommends recording the full
provenance of a taxize query.  In this way, the metadata doesn't read that
""1234 is the id of species XXX"", but rather that ""this version of ITIS
given this query on this date decided with this confidence etc that 1234
was the best matching id for the species represented by text query
XXXX""....
https://github.com/ropensci/RNeXML/issues/24#issuecomment-26551613


I do think a docx template could be more general than a specific use case.
Think of it just as an alternative interface.  Rather than saying ""go into
R and type in all this stuff as we show in the examples"", you say to
people: ""Don't know R? Just fill out our word template.  R could
automatically generate the EML, producing a human readable summary people
could look over.  Yeah, maybe not, I don't know.  Here's their template.
http://harvardforest.fas.harvard.edu/sites/harvardforest.fas.harvard.edu/files/data/doc/HF%20Metadata%20Form.doc







On Fri, Jun 20, 2014 at 3:10 PM, Edmund Hart <notifications@github.com>
wrote:

> I was thinking of creating a way that people could generate EML with as
> little headaches as possible. I guess it might be useful in specific cases
> like HF. Here we won't be parsing docx into EML, but we will need to parse
> docx into DB's and then to EML. I guess what I imagined was someone writes
> a paper, and then calls a fxn generateEML(myDocx), and then it text mines
> and populates the EML objects. There's probably a lot of stuff we could
> grab...like author information, contact info, affiliation, even temporal,
> spatial scale, and taxonomic coverage. Then people could view what was
> mined, and change it, but it gets them 50% of the way there in terms of
> creating the EML. I'll be the first to admit this is probably way too
> ambitious. Getting the text is the easy part, it's mining it that's really
> hard.
>
> Also, while I'm just wildly speculating, a web app that uses machine
> learning to extract metadata. People can upload PDF's or we could crawl for
> them, and then use ML to extract specific elements of MD. An ecology MD
> generation engine essentially. Sorry, I saw too many talks on ML and
> situation awareness at a conference this past week.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/62#issuecomment-46732056>.
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
113,46736318,cboettig,2014-06-20T23:22:03Z,2014-06-20T23:22:03Z,"Okay, while the `data.set` mechanism still works as before, I've updated functions like `eml` so that they can take a good ol `data.frame` directly, along with the arguments for `col.defs` and `unit.defs`:

```r
demo(""ex1"", ""EML"") # load dat, col.defs, unit.defs
eml(dat, col.defs = col.defs, unit.defs = unit.defs, creator = ""joe plummer <joe@plummer.com>"")
```

I've also adjusted the functions so that they can take `dat` as a path to an external CSV file as suggested by @karen-ross and others (e.g. #106, https://github.com/ropensci/EML/issues/82#issuecomment-39172232).  To use this a user must also specify the `col.classes`, since unlike data.frames csvs are obviously untyped. (EML will still get the column names by peaking at the top of the CSV file, which could be handy if the csv file is too big to read in easily).  

```r
demo(""ex1"", ""EML"") # load dat, col.defs, unit.defs
write.csv(dat, ""test.csv"")
eml(""test.csv"", col.defs = col.defs, unit.defs = unit.defs, creator = ""joe plummer <joe@plummer.com>"")
```
",NA,NA,NA
118,47252995,gavinsimpson,2014-06-26T17:13:10Z,2014-06-26T17:13:10Z,I didn't edit the `PKG_HOME/README.md` (the base level README) in this PR as I wasn't sure how this file is generated - I guess you copy the rendered md from `PKG_HOME/vignettes/` to `PKG_HOME/` in which case this PR is all you need to fix these two links. If not you'll also need to edit `PKG_HOME/README.md` accordingly; I have submit a PR for that if it is needed.,NA,NA,NA
109,49244324,cboettig,2014-07-17T00:09:44Z,2014-07-17T00:09:44Z,"Closing this as dataset does not support multiple files (e.g. csv + EML xml file.  An EML submission can also document more than one csv or other external resource. While those could be separate objects, having a single figshare submission for the fileset probably makes more sense (i.e. a single DOI for the whole packet of EML+csv.  Not sure if figshare assigns sub-dois for components, but KNB/DataONE is probably a better alternative for such use cases than having two unrelated DOIs from figshare for the various components).  It is slightly unfortunate that dataset gets CC0 while fileset gets CC-by, so perhaps this may be worth revisiting later.  ",NA,NA,NA
106,49817845,cboettig,2014-07-23T00:07:46Z,2014-07-23T00:07:46Z,"Closing this, since as noted in the previously linked issue, it's now possible to specify the an already-named external csv file and not have EML change the csv name: 

```r
demo(""ex1"", ""EML"") # load dat, col.defs, unit.defs
write.csv(dat, ""test.csv"")
eml(""test.csv"", col.defs = col.defs, unit.defs = unit.defs, creator = ""joe plummer <joe@plummer.com>"")
```

",NA,NA,NA
64,49906601,cboettig,2014-07-23T17:25:41Z,2014-07-23T17:25:41Z,"Closing for now, though perhaps demos need to be updated to avoid teaching the `data.set` and just use the single `eml` command.  Hopefully the EML package README is always a useful starting point for a demo.  ",NA,NA,NA
34,49906853,cboettig,2014-07-23T17:27:33Z,2014-07-23T17:27:33Z,"Recursive schema seem to be a generic challenge to this approach (see #108), so closing for now.  ",NA,NA,NA
71,49908108,cboettig,2014-07-23T17:37:28Z,2014-07-23T17:37:28Z,"Most of this has been addressed, though could still be improved perhaps.  Here's the current convention: 

- `eml_<nodeName>()` is a constructor function for that node, e.g. `eml_dataTable()`.  
- Other helper functions are named as `eml_<verb>`, e.g. `eml_read()` or `eml_write()`
- all (exported to namespace) functions are prefixed with `eml`.  Prefixing avoids having functions with names like `keyword`, `person`, or `read` that could refer to something else.
- some effort has been made not to export functions unlikely to be necessary.
- `eml_get` is defined to provide a generic function interface to assist in subsetting. ",NA,NA,NA
100,49908887,cboettig,2014-07-23T17:43:35Z,2014-07-23T17:43:35Z,"@mbjones Suggested strategies for handle this? (e.g. is there an XSLT stylesheet we could use to first 'promote' the old format?  (If it's just this a few changes, might be best for us to just handle the mapping explicitly -- e.g. map an additionalInformation node into our `metadata` S4 object)",NA,NA,NA
69,49909020,cboettig,2014-07-23T17:44:36Z,2014-07-23T17:44:36Z,"Looks like this is in place and passing checks, so closing now.  ",NA,NA,NA
12,49913832,cboettig,2014-07-23T18:20:17Z,2014-07-23T18:20:17Z,"I wanted to check with @mbjones if I have this right.  A new unit type for ""g m-1""  would look like this: 

```xml
<unitType id=""massPerLength"" name=""massPerLength"">
  <dimension name=""mass""/>
  <dimension name=""length"" power=""-1""/>
</unitType>
```

Then one could use that as the unit type:

```xml
 <unit id=""gramsPerMeter"" multiplerToSI="".001"" name=""gramsPerMeter"" parentSI=""KilogramsPerMeter"" unitType=""massPerLength"", abbreviation=""g/m"">
        <description>Grams per meter</description>
</unit>
```

Note above we have also defined the conversion to the parent SI, since SI base unit for mass is kilogram, not gram.  (See [NIST](http://physics.nist.gov/cuu/Units/units.html)). I'm not sure if it is also necessary for us to define the unit `KilogramPerMeter`, in this case?  

```xml
 <unit id=""kilogramsPerMeter"" name=""kilogramsPerMeter"" unitType=""massPerLength"" abbreviation=""kg/m"">
        <description>Kilograms per meter</description>
</unit>
```


@dfalster @karen-ross Looks like I overlooked this one, sorry.  I think this is a case where you need to define a new unitType, which I think I still need to add an interface to support.  Will add once I pin this down.  Thanks again for your questions.

",NA,NA,NA
12,49948538,mbjones,2014-07-23T23:10:35Z,2014-07-23T23:10:35Z,"@cboettig That looks right to me -- although running it through an EML validator would help catch any errors.  I think it would be good to define the `kilogramsPerMeter` but I'm not sure its strictly required from a schema perspective. But its useful if you are referencing it.
",NA,NA,NA
112,50513763,cboettig,2014-07-29T17:59:05Z,2014-07-29T17:59:05Z,"Thanks for the bug report.  Now fixed.  That example file doesn't exist, but here's another that demonstrates this.  

```r
          eml <- eml_read(""http://files.figshare.com/1239121/figshare_landings.xml"")
          dat <- eml_get(eml, ""data.frame"")

```
It actually has the correct URL in `eml@dataset@dataTable[[1]]@physical@distribution@online@url` but `eml_get` was confusing the full XML root path, `http://files.figshare.com/1254196/` with the local filepath and was trying to save the file to that (remote) location. Now just downloads the file into the working directory and then loads it from there.  ",NA,NA,NA
101,50514341,cboettig,2014-07-29T18:02:47Z,2014-07-29T18:02:47Z,"@mbjones This KNB EML file reports the `distribution/online/url` as ecogrid://knb/commdyn.4.1

How do I map that ecogrid location to a URL I can download from directly?  (e.g. via `download.file()`)",NA,NA,NA
12,50521668,cboettig,2014-07-29T18:56:10Z,2014-07-29T18:56:10Z,"@mbjones @leinfelder  One more quick question, should both the `<unitType>` and `<unit>` elements go in a `unitList` element like Matt has in the example above, under `additionalMetadata/metadata`? 

Thanks.",NA,NA,NA
12,50689009,mbjones,2014-07-30T22:25:47Z,2014-07-30T22:25:47Z,"@cboettig Yeah, it should.  If you look at stmml.xsd, you'll see that UnitList is defined as:
```xsd
<xsd:sequence>
        <xsd:element ref=""unitType"" minOccurs=""0"" maxOccurs=""unbounded""/>
        <xsd:element ref=""unit"" minOccurs=""0"" maxOccurs=""unbounded""/>
</xsd:sequence>
```

So basically, list your unitType's first, and then all of your units.",NA,NA,NA
12,50690171,cboettig,2014-07-30T22:38:32Z,2014-07-30T22:38:32Z,"Perfect, done.  We now have S4 classes for the `stmml:unit` and `stmml:unitType` which should make it relatively straight forward to add new units and new `unitTypes`, e.g. see my unit test for an example:

https://github.com/ropensci/EML/blob/dc278e4d22ee4cb7594e21634a228dbf5cc33899/tests/testthat/test_custom_unit.R#L92-L121

I hope to add some more features to the helper functions `eml_define_unit` to help identify if a unit is already defined with fuzzy matching / abbreviation matching; and if not, identify if an appropriate unitType is already defined. At the moment, these functions don't provide much for training wheels.  

Since these tasks are now in #110 and #120, will (finally) close this issue thread.  ",NA,NA,NA
12,50692544,dfalster,2014-07-30T23:05:14Z,2014-07-30T23:05:14Z,"Wonderful! I look forward to trying out new functionality. Thanks @cboetigg
and @mbjones


On 31 July 2014 08:38, Carl Boettiger <notifications@github.com> wrote:

> Closed #12 <https://github.com/ropensci/EML/issues/12>.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/12#event-147470501>.
>",NA,NA,NA
121,52956111,cboettig,2014-08-21T17:46:46Z,2014-08-21T17:46:46Z,"Sure, though I don't have much hope for them.

Related: on a Mac, can you install the uuid package from CRAN?  It's
imported by EML now, and I recall there being problems about that at some
point...


On Thu, Aug 21, 2014 at 10:17 AM, Scott Chamberlain <
notifications@github.com> wrote:

> If so, i can do, let me know
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/121>.
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
121,52956935,karthik,2014-08-21T17:52:56Z,2014-08-21T17:52:56Z,"@cboettig: It works on a mac if you install from source.


On Thu, Aug 21, 2014 at 10:46 AM, Carl Boettiger <notifications@github.com>
wrote:

> Sure, though I don't have much hope for them.
>
> Related: on a Mac, can you install the uuid package from CRAN? It's
> imported by EML now, and I recall there being problems about that at some
> point...
>
>
> On Thu, Aug 21, 2014 at 10:17 AM, Scott Chamberlain <
> notifications@github.com> wrote:
>
> > If so, i can do, let me know
> >
> > —
> > Reply to this email directly or view it on GitHub
> > <https://github.com/ropensci/EML/issues/121>.
> >
>
>
>
> --
> Carl Boettiger
> UC Santa Cruz
> http://carlboettiger.info/
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/121#issuecomment-52956111>.
>",NA,NA,NA
121,52959018,cboettig,2014-08-21T18:07:49Z,2014-08-21T18:07:49Z,"@karthik thanks. but if you don't install from source?  just trying to be
aware of what kind of problems are likely to trip up less experienced
users...


On Thu, Aug 21, 2014 at 10:52 AM, Karthik Ram <notifications@github.com>
wrote:

> @cboettig: It works on a mac if you install from source.
>
>
> On Thu, Aug 21, 2014 at 10:46 AM, Carl Boettiger <notifications@github.com>
>
> wrote:
>
> > Sure, though I don't have much hope for them.
> >
> > Related: on a Mac, can you install the uuid package from CRAN? It's
> > imported by EML now, and I recall there being problems about that at
> some
> > point...
> >
> >
> > On Thu, Aug 21, 2014 at 10:17 AM, Scott Chamberlain <
> > notifications@github.com> wrote:
> >
> > > If so, i can do, let me know
> > >
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <https://github.com/ropensci/EML/issues/121>.
> > >
> >
> >
> >
> > --
> > Carl Boettiger
> > UC Santa Cruz
> > http://carlboettiger.info/
> >
> > —
> > Reply to this email directly or view it on GitHub
> > <https://github.com/ropensci/EML/issues/121#issuecomment-52956111>.
> >
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/121#issuecomment-52956935>.
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
122,57190294,cboettig,2014-09-29T16:46:00Z,2014-09-29T16:46:00Z,"Thanks for this report, date-time things are indeed a real challenge here and I'm never sure what the sensible thing is to do, so I'd really appreciate any suggestions you have. (See #17 and #78)   (Advice from @mbjones @karthik and others would also be appreciated here.  Getting sensible date time behavior is perhaps the last big issue to solve before I think we have a valid minimal package for CRAN...))

The heart of the problem is whenever dates are broken across columns like this, which seems rather common. I've no idea what the right default should be for the column class of the R data frame. 

To get something as a plain year like ""2012"", or a Julian day as ""171"" one would need to declare the column class as integer or character. Maybe that would be a sensible choice.  

Instead, I thought -- hey, R has a lot of ""date-time"" classes, why not use them?  So I've told the data.frame in this case to format each of the columns as real POSIXlt time objects. R understands the concepts of Julian days, etc, but these are all defined as different printing formats of a single date-time object.  So when R reads in the data, it sees ""171"" and the data saying ""julian day"", so it converts this to a date like so: 

```r
    as.Date(""171"", ""%j"")
```

Note that the metadata doesn't specify the year, and doesn't contain any machine-readable information to reassure the computer that the adjacent year column is indeed referring to the correct year.  As a result, R is adding the current year by default, which is kinda terrible, and we get back:

```
""2014-06-20""
```

Also, I'm missing an appropriate print method for the column.  All Date objects just print in the default print format. If the EML package used the appropriate print method when writing out to CSV or showing the structure, then this would look like it should:

```
> format(as.Date(""171"", format=""%j""), ""%j"")
[1] ""171""
```

I could add that to the write.csv method, but it would still display the funny result you see above in the terminal, since R is using the default print methods for the column class.  (And we decided against writing a custom class in place of data.frame with it's own print methods, etc, due to the difficulty of supporting all R operations on such an object in a way that respects the metadata. The above being just one example of how subtle that would be). 

So where does that leave us?  Perhaps I should avoid using the Date format by default and just stick with character class.  I'm not sure.",NA,NA,NA
122,57922593,ivanhanigan,2014-10-05T00:20:38Z,2014-10-05T00:20:38Z,"Thanks Carl,  I think using the date-time classes is really clever.  I
guess I was expecting to see it print a plain year like ""2012"", or Julian
day ""171"" as integer or character...  I think the great power of what you
are doing with this package is to enable R to know more about the data than
ever before, so it would be a shame to lose information about date-time
types and trick R into thinking these are merely numbers or characters.

On Tue, Sep 30, 2014 at 2:46 AM, Carl Boettiger <notifications@github.com>
wrote:

> Thanks for this report, date-time things are indeed a real challenge here
> and I'm never sure what the sensible thing is to do, so I'd really
> appreciate any suggestions you have. (See #17
> <https://github.com/ropensci/EML/issues/17> and #78
> <https://github.com/ropensci/EML/issues/78>) (Advice from @mbjones
> <https://github.com/mbjones> @karthik <https://github.com/karthik> and
> others would also be appreciated here. Getting sensible date time behavior
> is perhaps the last big issue to solve before I think we have a valid
> minimal package for CRAN...))
>
> The heart of the problem is whenever dates are broken across columns like
> this, which seems rather common. I've no idea what the right default should
> be for the column class of the R data frame.
>
> To get something as a plain year like ""2012"", or a Julian day as ""171"" one
> would need to declare the column class as integer or character. Maybe that
> would be a sensible choice.
>
> Instead, I thought -- hey, R has a lot of ""date-time"" classes, why not use
> them? So I've told the data.frame in this case to format each of the
> columns as real POSIXlt time objects. R understands the concepts of Julian
> days, etc, but these are all defined as different printing formats of a
> single date-time object. So when R reads in the data, it sees ""171"" and the
> data saying ""julian day"", so it converts this to a date like so:
>
>     as.Date(""171"", ""%j"")
>
> Note that the metadata doesn't specify the year, and doesn't contain any
> machine-readable information to reassure the computer that the adjacent
> year column is indeed referring to the correct year. As a result, R is
> adding the current year by default, which is kinda terrible, and we get
> back:
>
> ""2014-06-20""
>
> Also, I'm missing an appropriate print method for the column. All Date
> objects just print in the default print format. If the EML package used the
> appropriate print method when writing out to CSV or showing the structure,
> then this would look like it should:
>
> > format(as.Date(""171"", format=""%j""), ""%j"")
> [1] ""171""
>
> I could add that to the write.csv method, but it would still display the
> funny result you see above in the terminal, since R is using the default
> print methods for the column class. (And we decided against writing a
> custom class in place of data.frame with it's own print methods, etc, due
> to the difficulty of supporting all R operations on such an object in a way
> that respects the metadata. The above being just one example of how subtle
> that would be).
>
> So where does that leave us? Perhaps I should avoid using the Date format
> by default and just stick with character class. I'm not sure.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/122#issuecomment-57190294>.
>",NA,NA,NA
123,58042481,mbjones,2014-10-06T16:16:00Z,2014-10-06T16:16:00Z,"Looks like the schematic isn't at that location. I get a 404 there. I
recommend shipping a copy of the schematic with the EML package so that
validation can be done against the local copy of the schema.  That's what
we do with Morpho, metacat, etc.

Matt

On Monday, October 6, 2014, Scott Chamberlain <notifications@github.com>
wrote:

> i can share everything if you want, but maybe you recognize the error:
>
> eml_validate(""conditionality_data_prelim_eml.xml"")
>
> failed to load HTTP resource
> Failed to locate the main schema resource at 'http://cboettig.github.com/eml/eml.xsd'.
> Error in xmlSchemaValidate(""http://cboettig.github.com/eml/eml.xsd"", doctext) :
>   trying to get slot ""ref"" from an object of a basic class (""NULL"") with no slots
> In addition: Warning messages:1: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
>   there is no package called ‘RHTMLForms’2: In xmlParse(file, asText = asText, isSchema = TRUE, xinclude = xinclude,  :
>   NULL value for external reference
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/123>.
>",NA,NA,NA
123,58042870,cboettig,2014-10-06T16:18:21Z,2014-10-06T16:18:21Z,"Thanks -- validate needs RHTMLForms installed from Omegahat, that does the
more formal validation.

It's using the fallback method and trying to get the schema from an old
github repo of mine, which like matt says doesn't exist (moved to
eml-schema).  I'll follow Matt's suggestion and just ship the schema with
the package.

I also need to add a test for the fallback method -- currently it doesn't
get tested because travis installs RHTMLForms...

On Mon, Oct 6, 2014 at 9:11 AM, Scott Chamberlain <notifications@github.com>
wrote:

> i can share everything if you want, but maybe you recognize the error:
>
> eml_validate(""conditionality_data_prelim_eml.xml"")
>
> failed to load HTTP resource
> Failed to locate the main schema resource at 'http://cboettig.github.com/eml/eml.xsd'.
> Error in xmlSchemaValidate(""http://cboettig.github.com/eml/eml.xsd"", doctext) :
>   trying to get slot ""ref"" from an object of a basic class (""NULL"") with no slots
> In addition: Warning messages:1: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
>   there is no package called ‘RHTMLForms’2: In xmlParse(file, asText = asText, isSchema = TRUE, xinclude = xinclude,  :
>   NULL value for external reference
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/123>.
>



-- 
Carl Boettiger
UC Santa Cruz
http://carlboettiger.info/",NA,NA,NA
123,58043220,sckott,2014-10-06T16:20:44Z,2014-10-06T16:20:44Z,"Thanks @mbjones and @cboettig - Just got `RHTMLForms`, and now it works great, thanks!",NA,NA,NA
122,58236123,cboettig,2014-10-07T18:35:33Z,2014-10-07T18:35:33Z,"@ivanhanigan Thanks for the reply.  I've now added an internal function that will make sure that `eml_write()` and it's subsidiary functions print the CSV files in the format specified by the metadata.  

however, the issue you show above will always exist as long as the data.frame uses dateTime formatted columns.  You could get it to display correctly now using the new internal function, which also requires the unit.def metadata

```r
unit.defs = eml_get(f, ""unit.defs"")
pretty_dat <- EML:::print_format(dat, unit.defs=unit.defs)
head(pretty_dat)
```

displays: 

```
  run.num year day hour.min i.flag variable value.i
1       1 2012 170     1204      R  control   16.65
2       1 2012 170     1205      R  control   16.64
3       1 2012 170     1206      R  control   16.57
4       1 2012 170     1207      R  control   16.47
5       1 2012 170     1208      R  control   16.42
6       1 2012 170     1209      R  control   16.48
```

Still, this probably isn't good default behavior.  To actually use this data with the dateTime class in R, one would still need a data frame with a single column containing the complete date-time object, not have the date-time data split over 3 columns.  Using the `data.frame` in which we have just coerced the strings into dates results in fictitious data, since R arbitrarily fills in the missing bits of the datetime.  To avoid fictitious behavior, we should probably _only_ use a datetime format if the column uses a string that we can unambiguously coerce into a complete date or datetime.  Or perhaps avoid any automatic coercion.  

Instead, we could provide a helper function for fusing multiple date-time columns like the example into a single datetime column we append to the dataset, based on the metadata.  

Of course there are use-cases for having the datetime split over three columns -- for instance, a user wants to treat `year` as a factor in a statistical analysis. (Technically speaking such analyses would sometimes be better served using the rich [libraries for time-series analysis](http://cran.r-project.org/web/views/TimeSeries.html) and aggregation, though I suspect that's often ignored.)
",NA,NA,NA
20,72338054,cboettig,2015-01-31T21:37:00Z,2015-01-31T21:37:00Z,"@mbjones looks like the `eml_knb` function isn't compatible with the `dataone 2.0.0` (e.g. `D1Client` not found). Clearly I need to get up to speed on the new package interface etc.

I did try a quick work-around with direct REST calls to the API.  It looks like the DEV node I was using for testing is down (502 error): https://mn-demo-5.test.dataone.org/knb/d1/mn, but checking  https://cn-dev.test.dataone.org/cn/v1/node I realized I could just switch from `5` to `6`.  I could get non-authenticated calls to work this way, but tried uploading an example file using a CIlogon certificate but I get 'permission denied' (401 error).  Maybe not worth debugging this since I should just be able to use the dataone package, but not sure why it's not working all the same.  (Example of this failing test here: https://github.com/cboettig/dataone-lite/blob/master/tests/testthat/d1_upload.R )",NA,NA,NA
20,72551735,mbjones,2015-02-02T22:26:16Z,2015-02-02T22:26:16Z,"@cboettig yeah, that's only because we are working from the low-level API back up towards the top in terms of refactoring.  So, D1Client will be the last API to be refactored.  I'll look into why its not working.  ",NA,NA,NA
125,78826828,mbjones,2015-03-13T06:19:34Z,2015-03-13T06:19:34Z,Initial implementation checked in via sha d10e075465c75337076296cdce48afd4cd1b45d8. Needs review.,NA,NA,NA
127,98250954,cboettig,2015-05-01T22:00:11Z,2015-05-01T22:00:11Z,"No, it's dataset.  See ?dataset.   We're deprecating the data.set idea,
which was a kind of extension of data.frame.

On Fri, May 1, 2015, 2:28 PM Ryan Batt <notifications@github.com> wrote:

> https://github.com/ropensci/EML/blob/master/man/eml_write.Rd#L25
>
> should be ?data.set, right?
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/127>.
>
",NA,NA,NA
127,98253315,rBatt,2015-05-01T22:17:07Z,2015-05-01T22:17:07Z,"I get an error when doing `?dataset`, but not for `?data.set`",NA,NA,NA
127,98300031,cboettig,2015-05-02T04:14:31Z,2015-05-02T04:14:31Z,"Ah good point; yeah, the docs are really incomplete right now.  dataset
passes most of these options down to the lower-level stucture 'dataTable'
anyhow, so you can see ?eml_dataTable. The 'writing EML' vignette might be
a bit better intro
https://github.com/ropensci/EML/blob/master/vignettes/Advanced_writing_of_EML.Rmd
for now, but lots of rough edges still

On Fri, May 1, 2015 at 3:17 PM Ryan Batt <notifications@github.com> wrote:

> I get an error when doing ?dataset, but not for ?data.set
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/127#issuecomment-98253315>.
>
",NA,NA,NA
128,98878215,rBatt,2015-05-04T22:47:43Z,2015-05-04T22:47:43Z,I only found the right page because of links in #127 ,NA,NA,NA
129,98884975,rBatt,2015-05-04T23:37:09Z,2015-05-04T23:38:24Z,"@cboettig 

Here is a simpler example that produces the same problem for me:

```R
test <- data.frame(""idt""=1:10, ""valt""=rnorm(10))

new.unit <- eml_define_unit(""numberPerHectare"", unitType=""otherUnitType"", parentSI=""numberPerKilometerSquared"", multiplierToSI=""0.01"", description=""number per hectare"")

col.defs <- c(
		""idt""=""some ID"",
		""valt""=""some value""
)
unit.defs <- list(
		""idt""=c(
			unit = ""number"",
			precision = 1
		)
                new.unit
)

blah <- eml(dat=test, title=""testMD"", col.defs=col.defs, unit.defs=unit.defs, creator=""Ryan Batt <battrd@gmail.com>"", custom_units=list(new.unit))
```",NA,NA,NA
130,99263222,cboettig,2015-05-05T23:47:06Z,2015-05-05T23:47:06Z,"It isn't really expected that users will write additionalMetadata nodes
manually.  Rather, metadata in this section may be generated by various
other automated utilities when an appropriate field does not already exist.

Can you let me know a bit more about what you mean by tags?  If you are
looking for keywords, there the eml_keyword function that is shown in the
'advanced writing' vignette.

@mbjones can correct me but I'm not sure that having additionalLinks in the
additionalMetadata section as in that Harvard forest example is really the
best/correct structure for providing a list of additional links.  @mbjones
thoughts?

On Tue, May 5, 2015, 2:38 PM Ryan Batt <notifications@github.com> wrote:

> I'm unsure of how to get some of the formatting correct. Thus far I've
> relied on the S4 classes and filling slots, or using the helpful functions.
>
> for the additionalMetadata, the metadata slot doesn't really provide much
> help, I don't think.
>
> In particular, how am I supposed to enter tags?
>
> An object of class ""ListOfadditionalMetadata""
> [[1]]An object of class ""additionalMetadata""Slot ""describes"":character(0)
> Slot ""id"":character(0)
> Slot ""metadata"":<metadata>
>   <additionalClassifications>
>     <status>ongoing</status>
>     <researchTopic>community</researchTopic>
>     <studyType>short-term measurement</studyType>
>     <studyType>modeling</studyType>
>   </additionalClassifications></metadata>
>
>
> [[2]]An object of class ""additionalMetadata""Slot ""describes"":character(0)
> Slot ""id"":character(0)
> Slot ""metadata"":<metadata>
>   <additionalLinks>
>     <url name=""Ecophysiology of Carnivorous Plants Worldwide 1980-2011"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf168</url>
>     <url name=""Effects of Prey Availability on Sarracenia Physiology at Harvard Forest 2005"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf109</url>
>     <url name=""Food Web of Sarracenia Purpurea in United States and Canada since 1999"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf193</url>
>     <url name=""Thresholds and Regime Shifts at Four LTER Sites (CCE, JRN, PAL, SBC) 1951-2009"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf170</url>
>     <url name=""Nitrogen Cycling Dynamics in Sarracenia Purpurea at Harvard Forest 2004-2005"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf096</url>
>     <url name=""Organic and Inorganic Nitrogen Uptake by Sarracenia Purpurea at Harvard Forest and Fort Albany ON 2007"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf146</url>
>     <url name=""Prey Capture by Carnivorous Plants Worldwide 1923-2007"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf111</url>
>     <url name=""Sarracenia Purpurea Prey Capture at Harvard Forest 2008"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf114</url>
>     <url name=""Allochthonous Nutrients in the Sarracenia Microecosystem at Harvard Forest 2005-2007"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf098</url>
>   </additionalLinks></metadata>
>
> How am I supposed to write that manually?
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/130>.
>
",NA,NA,NA
130,99266632,mbjones,2015-05-06T00:04:14Z,2015-05-06T00:04:14Z,"@cboettig Well, it depends if there is a standard field where that information belongs.  For example, if these are taxonomic references that help define the taxonomy used in the data, then they should probably be listed in the [`classificationSystemCitation` field](https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/./eml-coverage.html#classificationSystemCitation).  But, if there is no appropriate location for them in the main EML document, then `additionalMetadata` is the appropriate place.",NA,NA,NA
130,99270294,rBatt,2015-05-06T00:18:34Z,2015-05-06T00:18:34Z,"@cboettig by ""tags"" I meant `<additionalLinks> </additionalLinks>`, etc. I.e., HTML tags (that's what they look like to me).

As I'm learning more about the standard fields in EML, I'm able to find homes for the information I have. That's great. However, there are some things that just aren't easily included.

For example, describing the crossed/nested structure of columns, whether something like ""weight per unit effort"" accounts for changes in gear, and what methodological approaches were consistent in sampling (beyond what's stated in the units). Like I said, I think a lot of this might go into the methods field (still figuring some of this out), but I'm anticipating important bits of information that might require new fields or just ad hoc chunks of text.

For now, I have website pages I want to link to.",NA,NA,NA
131,99559866,mbjones,2015-05-06T18:21:43Z,2015-05-06T18:21:43Z,Good idea. I agree.,NA,NA,NA
131,99609388,cboettig,2015-05-06T21:03:35Z,2015-05-06T21:03:35Z,"+1.  

I wonder if we should rethink the data structure for specifying these things in order to make it harder to make that kind of error in the first place?  For instance, it might be better to specify the column definition and the unit definitions in the same data structure to make sure they are aligned properly.  

Originally we had something closer to this: instead of `unit.defs` and `col.defs` you just gave one list called `meta` which had to be structured with one list element per column, in which each element was itself a list which had the column name, column definition, and unit definition.  However, this kind of deeply nested list structure seems kind of fragile and complex (e.g. the kind of thing you might define a custom S3 class for, but that starts to feel like we're just re-inventing the attributeList S4 object we already have; which matches the EML definition more precisely.  

I wonder if it would be better to ask the user to define this data in a data.frame rather than in nested lists.  A data.frame would also impose the constraint of not miss-matching metadata, (ironically each row of this metadata `data.frame` would define a column of the actual data).  Though perhaps the idea of having a vector of factor definitions in a single cell of a data.frame is too off-putting to consider...

Having a really natural way to do this part, (including custom unit definitions) has long been a stumbling block for me; I've been hesitant to move towards a CRAN release until I feel like we won't need to change the function API.  The current `unit.defs` & `col.defs` still feels a bit too clumsy.

",NA,NA,NA
131,99616118,rBatt,2015-05-06T21:34:07Z,2015-05-06T21:34:07Z,"I agree that it's tough to think of how things should be organized.

My biggest challenge has been figuring out what should be filled in, and
*how* it can be filled in (e.g., it was a surprise to me that I could only
list 1 contact). Keep in mind that I'm learning about EML via this package,
so for me it's really an intro to metadata as well. Regardless, the only
thing that feels clumsy about the definitions, to me, is that what I supply
to unit.defs is so varied, and I'm not really sure what I can or should
supply.

You could also require that the unit and col defs be named, and make sure
the names match.

On Wed, May 6, 2015 at 5:03 PM, Carl Boettiger <notifications@github.com>
wrote:

> +1.
>
> I wonder if we should rethink the data structure for specifying these
> things in order to make it harder to make that kind of error in the first
> place? For instance, it might be better to specify the column definition
> and the unit definitions in the same data structure to make sure they are
> aligned properly.
>
> Originally we had something closer to this: instead of unit.defs and
> col.defs you just gave one list called meta which had to be structured
> with one list element per column, in which each element was itself a list
> which had the column name, column definition, and unit definition. However,
> this kind of deeply nested list structure seems kind of fragile and complex
> (e.g. the kind of thing you might define a custom S3 class for, but that
> starts to feel like we're just re-inventing the attributeList S4 object we
> already have; which matches the EML definition more precisely.
>
> I wonder if it would be better to ask the user to define this data in a
> data.frame rather than in nested lists. A data.frame would also impose the
> constraint of not miss-matching metadata, (ironically each row of this
> metadata data.frame would define a column of the actual data). Though
> perhaps the idea of having a vector of factor definitions in a single cell
> of a data.frame is too off-putting to consider...
>
> Having a really natural way to do this part, (including custom unit
> definitions) has long been a stumbling block for me; I've been hesitant to
> move towards a CRAN release until I feel like we won't need to change the
> function API. The current unit.defs & col.defs still feels a bit too
> clumsy.
>
> —
> Reply to this email directly or view it on GitHub
> <https://github.com/ropensci/EML/issues/131#issuecomment-99609388>.
>
",NA,NA,NA
126,120805214,ivanhanigan,2015-07-13T04:13:10Z,2015-07-13T04:13:10Z,"Apology, I'll resubmit after running proper tests.  ",NA,NA,NA
126,121012595,cboettig,2015-07-13T18:20:53Z,2015-07-13T18:20:53Z,"Thanks! 

Sorry development has stalled on this; but we should be able to push things forward significantly soon with the benefit of a few new and updated packages (e.g. `dataone`, `readr`, and maybe more utilities for metadata associated with databases and spatial graphics thanks to new packages for those functions as well).  ",NA,NA,NA
132,94794474,cboettig,2015-07-13T19:35:51Z,2015-07-13T19:35:51Z,"The `udunits2` package provides conversions between most common units using bindings to the Unidata library: http://cran.r-project.org/web/packages/udunits2/index.html.  Since synthesis across data tables usually requires conversion between units, a clear interface to the package can be handy.  

At a more basic level, the unidata bindings appears to have utilities for parsing strings into recognizable unit names. ",open,0,Units and unit conversions integration with udunits2 package
131,73694552,rBatt,2015-05-06T18:09:50Z,2015-05-06T21:34:07Z,"in `eml_dataTable` it would be helpful if there was an initial check to unsure that the length of the arguments `col.defs` and `unit.defs` matched `ncol(dat)`.

I was getting pretty confusing behavior when I had accidentally only defined 36 of the 37 columns – I was being prompted to define the `unitType` of one of the values in an element of my `unit.def`. Took me a while to track down the error because I thought it was due to a mismatch between column class and the definitions, but really it was trying to apply the definitions to the wrong column.",open,3,no check for definition length to match column #
130,73439241,rBatt,2015-05-05T21:37:41Z,2015-05-06T00:18:34Z,"I'm unsure of how to get some of the formatting correct. Thus far I've relied on the S4 classes and filling slots, or using the helpful functions.

for the additionalMetadata, the `metadata` slot doesn't really provide much help, I don't think.

In particular, how am I supposed to enter tags?

```R
An object of class ""ListOfadditionalMetadata""
[[1]]
An object of class ""additionalMetadata""
Slot ""describes"":
character(0)

Slot ""id"":
character(0)

Slot ""metadata"":
<metadata>
  <additionalClassifications>
    <status>ongoing</status>
    <researchTopic>community</researchTopic>
    <studyType>short-term measurement</studyType>
    <studyType>modeling</studyType>
  </additionalClassifications>
</metadata> 


[[2]]
An object of class ""additionalMetadata""
Slot ""describes"":
character(0)

Slot ""id"":
character(0)

Slot ""metadata"":
<metadata>
  <additionalLinks>
    <url name=""Ecophysiology of Carnivorous Plants Worldwide 1980-2011"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf168</url>
    <url name=""Effects of Prey Availability on Sarracenia Physiology at Harvard Forest 2005"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf109</url>
    <url name=""Food Web of Sarracenia Purpurea in United States and Canada since 1999"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf193</url>
    <url name=""Thresholds and Regime Shifts at Four LTER Sites (CCE, JRN, PAL, SBC) 1951-2009"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf170</url>
    <url name=""Nitrogen Cycling Dynamics in Sarracenia Purpurea at Harvard Forest 2004-2005"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf096</url>
    <url name=""Organic and Inorganic Nitrogen Uptake by Sarracenia Purpurea at Harvard Forest and Fort Albany ON 2007"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf146</url>
    <url name=""Prey Capture by Carnivorous Plants Worldwide 1923-2007"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf111</url>
    <url name=""Sarracenia Purpurea Prey Capture at Harvard Forest 2008"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf114</url>
    <url name=""Allochthonous Nutrients in the Sarracenia Microecosystem at Harvard Forest 2005-2007"">http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf098</url>
  </additionalLinks>
</metadata>

```

How am I supposed to write that manually?",open,3,example for creating additionalMetadata manually?
129,73152987,rBatt,2015-05-04T23:05:46Z,2015-05-04T23:38:24Z,"First, note that on the front page of the repo, the read me's link ""[define a custom unit](https://github.com/ropensci/EML#)"" simply returns one back to the same page – this is it's own issue perhaps.

When I do `?eml`, I see the `custom_units` argument, and that has a link to the help file for `eml_define_unit`. As far as I can tell I've successfully created the custom unit, but I can't seem to use it properly:

```R

# create custom unit for ""count per unit effort (cntpue)""
cntpue.unit <- eml_define_unit(""numberPerHectare"", unitType=""otherUnitType"", parentSI=""numberPerKilometerSquared"", multiplierToSI=""0.01"", description=""number per hectare"")

```


I'll create a reproducible example below. These data are bottom trawl data from the Aleutian Islands. I'm trying to follow the instructions from the package readme, and from: https://github.com/ropensci/EML/blob/master/vignettes/Advanced_writing_of_EML.Rmd


```R
library(data.table)
library(EML)

# head of the data set I'm using
ai.mdData <- structure(list(year = c(1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 
1983L, 1983L, 1983L, 1983L), datetime = c(""07/15/1983 18:00"", 
""07/15/1983 18:00"", ""07/15/1983 18:00"", ""07/15/1983 18:00"", ""07/15/1983 18:00"", 
""07/15/1983 18:00"", ""07/15/1983 18:00"", ""07/15/1983 18:00"", ""07/15/1983 18:00"", 
""07/15/1983 18:00""), spp = c(""Bathymaster signatus"", ""Berryteuthis magister"", 
""Icelus spiniger"", ""Malacocottus zonurus"", ""Neptunea sp."", ""Pleurogrammus monopterygius"", 
""Rajidae"", ""Sebastes alutus"", ""Sebastes babcocki"", ""Sebastes polyspinis""
), haulid = c(""554-198301-  1"", ""554-198301-  1"", ""554-198301-  1"", 
""554-198301-  1"", ""554-198301-  1"", ""554-198301-  1"", ""554-198301-  1"", 
""554-198301-  1"", ""554-198301-  1"", ""554-198301-  1""), stratum = c(""-189.5 52.5"", 
""-189.5 52.5"", ""-189.5 52.5"", ""-189.5 52.5"", ""-189.5 52.5"", ""-189.5 52.5"", 
""-189.5 52.5"", ""-189.5 52.5"", ""-189.5 52.5"", ""-189.5 52.5""), 
    stratumarea = c(4065L, 4065L, 4065L, 4065L, 4065L, 4065L, 
    4065L, 4065L, 4065L, 4065L), lat = c(52.91667, 52.91667, 
    52.91667, 52.91667, 52.91667, 52.91667, 52.91667, 52.91667, 
    52.91667, 52.91667), lon = c(-189.18667, -189.18667, -189.18667, 
    -189.18667, -189.18667, -189.18667, -189.18667, -189.18667, 
    -189.18667, -189.18667), depth = c(181L, 181L, 181L, 181L, 
    181L, 181L, 181L, 181L, 181L, 181L), stemp = c(7.4, 7.4, 
    7.4, 7.4, 7.4, 7.4, 7.4, 7.4, 7.4, 7.4), btemp = c(4, 4, 
    4, 4, 4, 4, 4, 4, 4, 4), wtcpue = c(0.5629, 6.0319, 0.7988, 
    2.865, 0.022, 791.9208, 0.7642, 257.8379, 0.0817, 22.9767
    ), cntcpue = c(1.7333, NA, 5.824, 27.2479, NA, 2644.1603, 
    0.0693, 674.0574, 0.0693, 53.4559), region = c(""AFSC_Aleutians"", 
    ""AFSC_Aleutians"", ""AFSC_Aleutians"", ""AFSC_Aleutians"", ""AFSC_Aleutians"", 
    ""AFSC_Aleutians"", ""AFSC_Aleutians"", ""AFSC_Aleutians"", ""AFSC_Aleutians"", 
    ""AFSC_Aleutians""), s.reg = c(""ai"", ""ai"", ""ai"", ""ai"", ""ai"", 
    ""ai"", ""ai"", ""ai"", ""ai"", ""ai"")), .Names = c(""year"", ""datetime"", 
""spp"", ""haulid"", ""stratum"", ""stratumarea"", ""lat"", ""lon"", ""depth"", 
""stemp"", ""btemp"", ""wtcpue"", ""cntcpue"", ""region"", ""s.reg""), sorted = c(""year"", 
""datetime"", ""spp"", ""haulid"", ""stratum"", ""stratumarea"", ""lat"", 
""lon"", ""depth""), class = c(""data.table"", ""data.frame""), row.names = c(NA, 
-10L)) # note I removed the dput() output part giving the internal pointer location

# Create column definitions
ai.md.col.defs <- c(
	""year"" = ""year of haul"", 
	""datetime"" = ""the day and time of the haul"", 
	""spp"" = ""species scientific name; Genus species"", 
	""haulid"" = ""a unique identifier for the haul; vessel ID - cruise ID - haul number"", 
	""stratum"" = ""the center of the 1 degree grid cell in which the haul was taken; longitude latitude"", 
	""stratumarea"" = ""the area of the statistical stratum (no longer in data set)"", 
	""lat"" = ""latitude of the haul"", 
	""lon"" = ""longitude of the haul, in western hemisphere degrees (for lon > 0, do lon-360)"", 
	""depth"" = ""the maximum depth of the water at the location of the haul"", 
	""stemp"" = ""water temperature at the surface at the location of the haul"", 
	""btemp"" = ""water temperature at the bottom at the location of the haul"",
	""wtcpue"" = ""weight (mass) of the catch"", 
	""cntcpue""=""number of individuals caught per hectare in the haul"", 
	""region"" = ""the name of this data set, identified by north american sampling region and by the organization doing the sampling"",
	""s.reg"" = ""short region name""
)

# Create custom cntpue unit (duplicates what I have near the start of this issue)
# need numberPerHectare for cntcpue
cntpue.unit <- eml_define_unit(""numberPerHectare"", unitType=""otherUnitType"", parentSI=""numberPerKilometerSquared"", multiplierToSI=""0.01"", description=""number per hectare"")

# Create the unit definitions (note this is likely where I'm screwing up, and I've tried a few things here)
ai.md.unit.defs <- list(
	""year""=c(
		unit = ""number"",
		precision = 1
		), 
	""datetime"" = c(
		format = ""MM/DD/YYYY HH:MM""
		), 
	""spp"" = ""Genus species"", 
	""haulid"" = ""haul id"", 
	""stratum"" = ""1 degree lon lat"", 
	""stratumarea""=c(
		unit = ""kilometer"",
		precision = 1
		), 
	""lat""=c(
		unit = ""degree"",
		precision = 0.5
		), 
	""lon""=c(
		unit = ""degree"",
		precision = 0.5
		), 
	""depth""=c(
		unit = ""meter"",
		precision = 1
		), 
	""stemp""=c(
		unit = ""celsius"",
		precision = 0.1
		), 
	""btemp""=c(
		unit = ""celsius"",
		precision = 0.1
		), 
	""wtcpue"" =c(
		unit = ""kilogramsPerHectare"",
		precision = 0.0001
		),
	""cntcpue""=cntpue.unit, # PROBLEMATIC LINE? Also, I know ""cntcpue"" has an extra c
	""region""=""region"", 
	""s.reg""=""short region""
	
)


# Create a data object associated with meta data (?)
# Gives error: Error in unit.def[[1]] : this S4 class is not subsettable
eml_dataTable(
	ai.mdData,
	col.defs=ai.md.col.defs,
	unit.defs=ai.md.unit.defs,
	description=""test meta data for AI"",
	filename=""testMD.csv""
)


# at the problematic line, I've also tried
# ""cntcpue""=c(
# 		unit = ""numberPerHectare"",
# 		precision = 0.0001
# 		)
# but if I use that, then the eml_dataTable() doesn't seem to recognize the unit definition, as it's asking me to enter more information.

```

It's also possible that I'm not using `eml_dataTable` properly, but I've gotten similar error messages using `eml`.

I'm just trying to write the meta data file, I'm not concerned with writing the actual data set. I don't know if the intended uses would change any of these steps – I'm still trying to figure out how I should be using the package, so sorry for any ignorance.

@cboettig ",open,1,How to use a custom unit? Confused.
128,73150276,rBatt,2015-05-04T22:47:04Z,2015-05-04T22:47:44Z,"The read me has a link to the following:

https://github.com/ropensci/EML/blob/master//vignettes/Advanced_writing_of_EML.md

But I think it's intended to go here:

https://github.com/ropensci/EML/blob/master/vignettes/Advanced_writing_of_EML.Rmd

2 characters off",open,1,broken link to `Advanced writing of EML`
127,72527607,rBatt,2015-05-01T21:28:26Z,2015-05-02T04:14:31Z,"https://github.com/ropensci/EML/blob/master/man/eml_write.Rd#L25

should be ?data.set, right?",open,3,?write_eml typo
126,66989217,ivanhanigan,2015-04-07T20:59:18Z,2015-07-13T18:20:53Z,"After a brief discussion at https://github.com/ropensci/EML/issues/93 regarding combining the EML authoring tools and making best use of each of their features (authoring for Morpho, automation for R-EML) I have worked on the automation for simple unit definitions.  the use case I have in mind is for a large number of columns which are a variety of numeric, character/factor and dates.  

I want primarily to get an R script to provide good basic top-level metadata (and this can also be used for a EDA and data cleaning munging script, that documents the resulting cleaned data into the EML standard metadata too).

Primarily this scripting should avoid some of the problems we have encountered trying to write out definitions for 100s of columns or factor levels using Morpho. The example script in /inst/examples/hf205_unit_defs_boilerplate.R is heading in a direction I think scripts can be really useful for.  To a)  generate informative unit-level metadata, but also b) ensure that the metadata is closely related to the data (not as many possibilities for typographic or copy/paste errors during the creation of the column-wise metadata.

PS some of the steps in your example did not work and I have commented those bits out with the warnings I received.

Thanks for your work on this package so far guys!
",closed,2,add unit defs boilerplate code and example
125,60994768,mbjones,2015-03-13T06:18:54Z,2015-03-13T06:19:36Z,Need otherEntity to define non-tabular data.  Must also include related classes such as `physical/dataFormat/externallyDefinedFormat`.,open,1,add otherEntity support
124,58714226,mbjones,2015-02-24T09:41:12Z,2015-02-24T09:41:12Z,"Several fields that in EML are typed as `TextType`, such as `abstract` and `intellectualRights`, are produced via the R EML package as if they were simply character strings.  I may be missing it, but I don't see a mechanism for adding the `para` and `section` elements of the `TextType`.",open,0,missing TextType implementation
123,45002152,sckott,2014-10-06T16:11:58Z,2014-10-06T16:23:52Z,"i can share everything if you want, but maybe you recognize the error:

```r
eml_validate(""conditionality_data_prelim_eml.xml"")
```

```r
failed to load HTTP resource
Failed to locate the main schema resource at 'http://cboettig.github.com/eml/eml.xsd'.
Error in xmlSchemaValidate(""http://cboettig.github.com/eml/eml.xsd"", doctext) : 
  trying to get slot ""ref"" from an object of a basic class (""NULL"") with no slots
In addition: Warning messages:
1: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘RHTMLForms’
2: In xmlParse(file, asText = asText, isSchema = TRUE, xinclude = xinclude,  :
  NULL value for external reference
```",closed,3,Getting error on `eml_validate()`
122,44002708,ivanhanigan,2014-09-26T05:17:34Z,2015-10-27T01:18:08Z,"Thanks again for the great work guys. 

I am looking at https://github.com/ropensci/EML/blob/master/vignettes/Advanced_writing_of_EML.Rmd
And find a discrepancy between the data and metadata for year, day and hour.min


#### R code
    > library(EML)
    > f <- eml_read(""knb-lter-hfr.205.4"")
    > dat <- eml_get(f, ""data.frame"")
    > str(dat)
    'data.frame':	279224 obs. of  7 variables:
    $ run.num : chr  ""1"" ""1"" ""1"" ""1"" ...
    $ year    : POSIXlt, format: ""2012-09-26"" ""2012-09-26"" ""2012-09-26"" ""2012-09-26"" ...
    $ day     : POSIXlt, format: ""2014-06-19"" ""2014-06-19"" ""2014-06-19"" ""2014-06-19"" ...
    $ hour.min: POSIXlt, format: ""2014-09-26 12:04:00"" ""2014-09-26 12:05:00"" ""2014-09-26 12:06:00"" ""2014-09-26 12:07:00"" ...
    $ i.flag  : Factor w/ 3 levels ""B"",""I"",""R"": 3 3 3 3 3 3 3 3 3 3 ...
    $ variable: Factor w/ 8 levels ""air.temp"",""control"",..: 2 2 2 2 2 2 2 2 2 2 ...
    $ value.i : Factor w/ 19838 levels ""-0.001"",""-0.002"",..: 4770 4769 4756 4739 4732 4742 4739 4750 4752 4747 ...


But the metadata say 
- ""year"" = ""year, 2012"",
- ""day"" = ""Julian day. Range: 170 - 209."",
- ""hour.min"" = ""hour and minute of observation. Range 1 - 2400 (integer)"",

",closed,3, discrepancy between the data and metadata for example dataset for Advanced_writing_of_EML.Rmd
121,40827057,sckott,2014-08-21T17:17:23Z,2014-08-21T18:07:49Z,"If so, i can do, let me know",open,3,Interested in adding windows builds with Appveyor?
120,38549779,cboettig,2014-07-23T17:03:39Z,2014-07-30T22:38:32Z,"- [ ] provide numbered list to select a standard unit
- [ ] suggest standard unit based on partial name matching
- [ ] attempt matches against standard unit abbreviations

- [ ] Present options for a unitType from existing list
- [ ] prompt new unitType creation",open,0,Add tutorial on handling standard and custom units
119,36595326,gavinsimpson,2014-06-26T17:10:36Z,2014-07-23T00:31:11Z,"I'm not sure how you are generating the base `README.md` file (there is `README.Rmd` and `README.md` in the `PKG_HOME/vignettes/` folder which look identical to the base one. PR in #118 corrects two incorrect links in `PKG_HOME/vignettes/README.Rmd` but a third remains.

The link text `See the EML generated by this example.` has the URL:

    https://github.com/ropensci/EML/tree/master/inst/doc/EML_example.xml

and `PKG_HOME/inst/doc/` no longer exists and I couldn't find a file with the name `EML_example.xml` in the rest of the repo (although I didn't do an exhaustive search). This link needs fixing in `PKG_HOME/vignettes/README.Rmd` and in `PKG_HOME/README.md` (the base level README) if you don't copy across the MD file rendered from `PKG_HOME/vignettes/README.Rmd`.",closed,0,Other incorrect URL in README.md
118,36594814,gavinsimpson,2014-06-26T17:04:11Z,2014-07-23T17:15:29Z,Looks like the package structure was reorganised to house vignettes in `PKG_HOME/vignettes/` but the URLs in `README.Rmd` were not updated. This commit fixes two of them.,closed,1,Fix two incorrect URLs in links to md vignettes
117,36199086,karthik,2014-06-20T20:15:29Z,2014-07-23T00:34:31Z,"As a thin wrapper to `fs_update` so once datasets are uploaded to figshare, they can be updated with each run instead of pushing new copies.",closed,1,Add eml_update
116,36192856,karthik,2014-06-20T18:51:10Z,2014-07-23T17:21:20Z,"As discussed with @cboettig I'll push a shiny module that will make it easier to document metadata in a browser than use the command line wizard which can be painful to use if you don't know what option to use or want to document fields out of order.

",open,4,Add a shiny module to write metadata
115,36190507,emhart,2014-06-20T18:20:56Z,2014-07-23T00:35:34Z,"I'm not sure we necessarily want to build the vignette everytime the package is installed. 
1. It fails and I can't install the latest version
2. It seems to create a figshare article everytime the vignette build attempts, and I'm not sure where this is living. 

Full output from the install is below.
```r
> install_github(""EML"",""ropensci"")
Installing github repo EML/master from ropensci
Downloading EML.zip from https://github.com/ropensci/EML/archive/master.zip
Installing package from /var/folders/k8/fd3rfp2538n9jqxs_hjwhbcr0000gp/T//RtmpoQ5z9h/EML.zip
arguments 'minimized' and 'invisible' are for Windows only
Installing EML
'/Library/Frameworks/R.framework/Resources/bin/R' --vanilla CMD build  \
  '/private/var/folders/k8/fd3rfp2538n9jqxs_hjwhbcr0000gp/T/RtmpoQ5z9h/devtools6be3b0a8d44/EML-master' --no-manual --no-resave-data 

* checking for file '/private/var/folders/k8/fd3rfp2538n9jqxs_hjwhbcr0000gp/T/RtmpoQ5z9h/devtools6be3b0a8d44/EML-master/DESCRIPTION' ... OK
* preparing 'EML':
* checking DESCRIPTION meta-information ... OK
* installing the package to build vignettes
* creating vignettes ... ERROR
Loading required package: XML
trying URL 'http://harvardforest.fas.harvard.edu/data/p20/hf205/hf205-01-TPexp1.csv'
Content type 'text/csv' length 8787641 bytes (8.4 Mb)
opened URL
==================================================
downloaded 8.4 Mb

Loading required package: ROOXML
Warning in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called 'RHTMLForms'
Warning in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called 'RHTMLForms'
Warning in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called 'RHTMLForms'
Your article has been created! Your id number is 1064362
make README.md
make[1]: `README.md' is up to date.
make ../README.md
cp README.md ../../../README.md
sed -i 's/```r/```coffee/' ../../../README.md
sed: 1: ""../../../README.md"": invalid command code .
make[1]: *** [../README.md] Error 1
make: *** [all] Error 2
Error in tools::buildVignettes(dir = ""."", tangle = TRUE) : 
  running 'make' failed
Execution halted
Error: Command failed (1)
```",closed,1,Vignette building causes install to fail
114,33228551,cboettig,2014-05-10T04:48:07Z,2014-07-29T00:41:08Z,"Hi @mbjones,

Currently I can't include automated tests (e.g. as would be run by travis-ci, CRAN, or included in the vignette on the topic) for the KNB publishing integration because I don't know what to do about authenticating credentials.  

Since figshare is OAuth based, we're able to just provide consumer token keys that correspond to only a sandbox account (so it doesn't really matter that the keys are public).  

Though I have a unit test and a vignette for the KNB publishing (which of course use the KNB testing node, since they are uploading dummy data), I still have these switched off from actually running the tests since they would fail to authenticate.  Is there any way to accomplish this with some kind of testing credentials for the Dataone CILogon thing?

Maybe this is crazy, but since the CILogon supports Google accounts and Google accounts support OAuth, and OAuth2 at least supports frequently expiring tokens, it seems like it might not be inconsistent to just have some kind of OAuth pathway to login to dataone/KNB?  Perhaps that might make this easier? 

(Actually I'm not quite sure how the automated tests would handle the OAuth2 expiring tokens, figshare is OAuth1 and the token doesn't expire so it hasn't been a problem, but @hadley might have some suggestions/magic in that regard).  ",open,0,Login credentials for testing dataone / KNB 
113,33133632,cboettig,2014-05-08T23:31:43Z,2014-07-23T00:08:09Z,"Having to define unit and column metadata as a `data.set` and then pass that to `eml` is awkward.  The top-level functions like `eml` and `write_eml` should be designed to work with `data.frames` (or external csv files) directly without jumping through this extra confusing hoop.  

A more natural workflow would be an `attributeList` constructor that takes the current `unit.def` and `col.def` vectors.  This would more explicitly reflect the natural EML structure and thus be more flexible.  Something like:

```coffee
eml(a.data.frame, meta, contact)
```

where `meta` was any of: `attributeList` S4 object, a call to the `eml_attributeList` constructor, or possibly just a list of metadata by column? Anyone have preferences/thoughts on this (particularly this last structure?)  

This would be more similar to the way we handle other elements, like `coverage`.  

Fixing this may break a lot of tests and possibly code from @dfalster etc, so should be done with some care....


The ""data.set"" idea (more generally the idea of defining an extension of the `data.frame` that contains more metadata) might have made sense if it contained all the EML metadata, and/or if it allowed you to do things like have data.frames that could be in different units but know how to concatenate to each other, automatically adjusting the units.  Hadley convinced me this was effectively impossible during the hackathon, and that it was much more desirable to build functions around the base `data.frame` class.  ",closed,1,Replace `data.set` mechanism
112,32328429,dfalster,2014-04-28T03:00:55Z,2014-07-29T17:59:51Z,"Hi @cboettig,

I was keen to try using an EML file to fetch and load data into R, so turned to tried using one of your example EMLs on figshare. During that I ran into an error i thought you would want to know about. I found [this fileset](http://figshare.com/articles/reml_example/829573) and ran the following commands, with an error on the last one (get data set):

```
> 
> obj <- eml_read(""http://files.figshare.com/1254196/figshare_reml_example.xml"")
> eml_get(obj, ""unit.defs"")
$attribute
                   SAC                     AM 
""The Sacramento River""   ""The American River"" 

$attribute
[1] ""Scientific name""

$attribute
               parr               smolt 
 ""third life stage"" ""fourth life stage"" 

$attribute
[1] ""number""

$attribute
[1] ""YYYY-MM-DD""

> eml_get(obj, ""col.defs"")
                                            attribute 
                     ""River site used for collection"" 
                                            attribute 
                            ""Species scientific name"" 
                                            attribute 
                                         ""Life Stage"" 
                                            attribute 
                        ""count of live fish in traps"" 
                                            attribute 
""day traps were sampled (usually in morning thereof)"" 
> eml_get(obj, ""contact"")
[1] ""Carl Boettiger <cboettig@gmail.com>""
> dat <- eml_get(obj, ""data.set"")
Error in download.file(from@distribution@online@url, destfile = path,  : 
  cannot open destfile 'http://files.figshare.com/1254196/reml_example.csv', reason 'No such file or directory'
```

Note, it tries to fetch the file `http://files.figshare.com/1254196/reml_example.csv`but the right url for the csv file is `http://files.figshare.com/1254195/reml_example.csv`. Note the different number. The first (incorrect) number comes form the URL for the xml file (http://files.figshare.com/1254196/figshare_reml_example.xml).
 ",closed,1,Problem downloading data from figshare example
111,32123049,ivanhanigan,2014-04-24T06:17:29Z,2014-06-19T19:49:25Z,I'm not sure I found the correct new link but at least this resolves to a page in the EML specs family.  THanks for the great work!,closed,7,Link to StandardUnitDictionary has moved?
110,31829889,cboettig,2014-04-18T22:44:10Z,2014-09-19T22:54:17Z,"### Standard Units 

The EML R package currently assumes a user knows the list of standard units and specifies them in camelCase.  This should be relaxed: ideally we should be able to identify any common pattern and map it to the appropriate standardUnit, though care must be taken not to introduce mistakes or unexpected opaque behavior in so doing.  

### Custom Units

- [x] Add support for declaring custom units and automatically add the appropriate STMML definitions to the addtionalMetadata.  
- [x] Provide utilities to help specify custom units.  In particular, help figuring out the appropriate unitType to give.  ",open,0,better interface for declaring units
109,31829712,cboettig,2014-04-18T22:40:15Z,2014-07-17T00:10:24Z,,closed,1,eml_figshare should use `dataset` instead of `fileset`
108,31777710,emhart,2014-04-18T02:30:56Z,2014-07-23T17:27:33Z,"Hey Carl,  I was just thinking about how we're generating EML at NEON using pyxb or jaxb to creating objects with bindings from XML.  Thinking about this idea in R, it would mean scanning an XSD file, and on the fly writing an S4 class with slots for every element in the XSD. I'm curious if you think some of the approaches you've used EML could be extended to this more generalizable package.  I'm not sure that it would even be worthwhile to recreate what you can already easily do in python, but maybe if much of the code in the EML pkg is reusable.",closed,2,generic XML bindings in R
107,31614274,dfalster,2014-04-16T05:51:30Z,2014-07-23T17:20:53Z,"Hi @cboettig,

With help from @karen-ross, I have been preparing some datafiles to make public and trying to apply your REML package. I expect to do this somewhat often, so have tried to set up a worklow that

1. stored all key metatdata in files
2. uses these files to generate a standard readme file
3. uses the same files to generate an rich EML file (as per instructions [here](https://github.com/ropensci/EML/blob/devel/inst/doc/vignettes/Advanced_writing_of_EML.md).

My goal was to set it up so that I could get collaborators to give me information, put that into standard format, then squeeze out a readme and XML file. 

My first attempt at doing this is available here: [https://github.com/dfalster/Falster_2005_JEcol_data](https://github.com/dfalster/Falster_2005_JEcol_data)

It's mostly working but I have hit an error in generating the EML file, as described [here](https://github.com/dfalster/Falster_2005_JEcol_data/issues/2):

```
Error in as(i, ""XMLInternalNode"") : 
  no method or default for coercing “creator” to “XMLInternalNode”
```
 
I would appreciate any suggestions you have (feel free to clone, send PRs). Also, do let me know if you have any feedback on my approach for wrapping around your EML package.

with best regards,
Daniel",open,7,Workflow for advanced EML files
106,30625080,cboettig,2014-04-01T18:33:24Z,2014-07-23T00:07:46Z,"It is easy to mistake (#82) the `title` field of the `dataTable` element for the file name of the csv.    Currently the csv file is named with a uuid which potentially a cumbersome way to go and a strange default.  Perhaps this should use the title as the filename by default? In this case, how do we avoid unintentionally overwriting an existing CSV file? (or is that not a concern?)

Currently the csv file name and description can only be customized by using the lower level functions like `eml_dataTable`.  ",closed,6,Adjusting the title for csv file?
105,29855031,cboettig,2014-03-20T20:30:07Z,2014-03-21T14:55:01Z,"This is now failing. 

```coffee
library(EML)
eml_validate(system.file(""examples"", ""reml_example.xml"", package=""EML""))
```

inspection suggests a 301 error:

```
<!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN"">
<html>
<head><title>301 Moved Permanently</title></head>
<body>
<h1>Moved Permanently</h1>
<p>The document has moved <a href=""https://knb.ecoinformatics.org/emlparser/parse"">here</a>.</p>
<hr>
<address>Apache/2.2.14 (Ubuntu) Server at knb.ecoinformatics.org Port 80</address>
</body>
</html>
```


Attempting to manually upload a file on the website isn't working for me either: https://knb.ecoinformatics.org/emlparser/

so I think this might be an issue with KNB's parser.  Meanwhile, I might make the default behavior to just do schema validation only.  ",closed,4,eml_validate RHTMLForms based method broken again
104,27633832,cboettig,2014-02-14T22:58:37Z,2014-07-23T17:20:53Z,"@mbjones Not sure I follow how dependencies are supposed to be documented (e.g. not sure I understand the `action` element), but I'm worried about recursion here.  I'm writing a utility to generate the EML software node for an R package using the information R provides for installed packages through `packageDescription`.  It seems natural to list dependencies the way they are listed in the DESCRIPTION file.  I mean, I see why we would want to capture particular versions of the dependencies, which may or many not be given in this file, but it seems silly to start recursively writing out full Software elements for each dependency, and then for their dependencies and so on.  What is the best strategy to handle this?

For the moment I'm using the work-around of omiting the `dependency` element and including the list in the `additionalInfo` element instead...",open,2,Documenting software dependencies in EML
103,27572266,cboettig,2014-02-14T03:50:56Z,2014-07-29T17:33:02Z,"Particularly relational database from remote source, accessed through specified software protocol (use case from @ethanwhite https://twitter.com/ethanwhite/status/434014008715120640 )",open,0,Add example documenting data from a relational database
102,27032491,karthik,2014-02-06T08:08:44Z,2014-06-20T17:34:02Z,"data.set in the memisc package

http://cran.r-project.org/web/packages/memisc/memisc.pdf
See page 20-21.

Could incorporate/import this functionality into EML.",closed,1,data.set exists in memisc
101,27018896,cboettig,2014-02-06T00:51:15Z,2014-07-29T18:02:47Z,"e.g. 

```coffee
a = read.eml(""knb.434.4"")
eml_get(a, ""data.set"")
```",open,1,Improve remote access of associated CSV
100,27018718,cboettig,2014-02-06T00:47:39Z,2014-07-23T17:43:35Z,"For instance, eml-2.0.1 `additionalMetadata` has `additionalInformation` child node, not in a `metadata` node, which just breaks, for instance in: `knb-lter-sev.17804.1.xml`

",open,1,Handle reading or errors of older EML files gracefully
99,26502436,karthik,2014-01-29T07:18:42Z,2014-07-23T17:21:47Z,"In relation to the [wizard](https://github.com/ropensci/EML/blob/master/R/eml_wizards.R) wondering if it would be worth adding a Shiny module as an enhancement. It would allow users to document all the fields in a browser, often suggesting standard units. It also gets around the linear approach one has to work with on the command line.  
New version of Shiny can take input and send everything back to the kernel (not just for readonly use).",open,3,Worth adding Shiny support?
98,26421960,cpfaff,2014-01-28T07:58:44Z,2014-06-27T08:44:49Z,There is still the differences from the synchro shown here. But this comes from a fresh branch for the project module only introduces and changes one file `project.R`,closed,1,Project
97,26421259,cpfaff,2014-01-28T07:39:18Z,2014-07-23T17:23:30Z,,open,0,Create the project module
96,26421030,cpfaff,2014-01-28T07:32:28Z,2014-01-28T17:38:25Z,Now much better and in sync again. Find it hard to squash the commits as there is a lot of commits of you and me intermixed now after syncing. ,closed,1,Litrature
95,26391297,cboettig,2014-01-27T20:34:42Z,2014-01-28T16:37:22Z,"Will require: 

- [x] Removing the non-CRAN hosted packages from SUGGESTS list
- [ ] Setting a trigger to skip all test functions that require non-CRAN packages
- [ ] Likewise something for handling the vignettes that include those packages
- [x] Trigger to skip tests that require KNB authentication.
- [x] Include testing figshare credentials ",closed,6,Get Travis integration up and running
94,26349336,cpfaff,2014-01-27T12:26:42Z,2014-07-07T14:05:57Z,"* Fixes typo (in tests of literature)
* Starts project module",closed,6,Litrature
93,25993380,ivanhanigan,2014-01-21T12:52:35Z,2014-01-30T19:45:49Z,"    # eml_write for morpho_read
    # ivanhanigan
    # 2014-01-21

    # Issue: Morpho 1.8 unable to read data from eml_write

    # aim

    # I am basically very lazy when it comes to entering metadata and
    # when I use the Morpho package for metadata data entry I get
    # frustrated with having to step through ever SINGLE variable and
    # use the drop down menus etc to describe them as essentially
    # ""number"" or ""text""

    # The aim of this experiment is to use the EML package to create
    # some advanced metadata quickly and then finish this off with
    # Morpho, using ""boilerplate code"" wherever possible.

    # For technical reasons, I am running an older version of the
    # Morpho software because I am working with an older version of
    # the Metacat portal software and so are also constrained to
    # running the older Morpho version too (but will be upgrading)

    # func
    #library(""devtools"")
    #install_github(""EML"", ""ropensci"")
    library(""EML"")

    # load
    # test data from McCarthy, M. a., & Masters, P. (2005). Profiting from
    # prior information in Bayesian analyses of ecological data. Journal
    # of Applied Ecology, 42(6),
    # 1012–1019. doi:10.1111/j.1365-2664.2005.01101.x

    #Brief description is:

    #  Experimental data: effect of cover reduction on mulgara
    #  Dasycercus cristicauda: A manipulation of habitat was
    #  conducted by Masters, Dickman & Crowther (2003) in which
    #  vegetation cover of a site in arid inland Australia was
    #  reduced and the response of the mammal fauna monitored.

    #you can find the data in the download file from the ""Code for
    #analysing the mulgara experiment"" from here
    #http://www.nceas.ucsb.edu/~mccarthy/research.html
    datatext <- 'Treat, Before, After1, After2
    0,  2.833213344,    1.609437912,    2.48490665
    0,  1.791759469,    2.197224577,    2.079441542
    0,  3.044522438,    2.708050201,    3.135494216
    0,  2.772588722,    1.791759469,    2.197224577
    0,  1.098612289,    1.609437912,    2.63905733
    1,  2.944438979,    0.693147181,    1.791759469
    1,  2.564949357,    0.693147181,    1.791759469
    1,  2.564949357,    1.609437912,    1.609437912
    1,  0.693147181,    1.098612289,    1.098612289
    1,  1.609437912,    0,      1.098612289'
    dat <- read.csv(textConnection(datatext))

    # check
    dat

    # do
    ## from a work dir with a subdir for data
    dir.create(""data"")
    setwd(""data"")

    eml_config(creator=""Ivan Hanigan <ivanhanigan@gmail.com>"")
    eml_write(dat, file=""mulgara.xml"", title= ""mulgara"")

    # now read this into Morpho 1.8
    # Warnings:

    # 1. unable to display metadata: null > hit OK
    # 2. unable to display these data. however an empty table blah blah > hit OK
    # 3. this data package uses an older version of EML.
    #  you will not be able to edit unless upgrade.  do you want to upgrade? > hit Yes
    # 4. unable to display these data. however an empty table will be shown > hit OK

    # now I have a clear morpho screen.
    # for eg I have a table but the values in the columns are missing
    # file > save > locally = warning unable to display these data blah blah > hit OK and close
    # open existing data package > warning: unable to display these data > OK > shows clear screen
    # file > export > to directory > etc > OK
    # file > import > browse > packagename/metadata/packagename > OK =
    # warning: unable to display these data > OK.  Shows clear screen.
    sessionInfo()
    #     R version 3.0.2 (2013-09-25)
    #     Platform: x86_64-pc-linux-gnu (64-bit)
    #     
    #     locale:
    #       [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C         LC_TIME=C            LC_COLLATE=C         LC_MONETARY=C        LC_MESSAGES=C       
    #     [7] LC_PAPER=C           LC_NAME=C            LC_ADDRESS=C         LC_TELEPHONE=C       LC_MEASUREMENT=C     LC_IDENTIFICATION=C 
    #     
    #     attached base packages:
    #       [1] stats     graphics  grDevices utils     datasets  methods   base     
    #     
    #     other attached packages:
    #       [1] EML_0.0-4
    #     
    #     loaded via a namespace (and not attached):
    #       [1] RCurl_1.95-4.1 XML_3.98-1.1   digest_0.6.3   httr_0.2       stringr_0.6.2  tools_3.0.2    uuid_0.1-1     yaml_2.1.10",closed,7,Morpho1.8 unable to read eml_write output
92,25991127,ivanhanigan,2014-01-21T12:26:16Z,2014-02-21T22:30:58Z,"#### R code
        # eml_write issues
        # ivanhanigan
        # 2014-01-21
        
        # aim
        # there are a couple issues here
        # 1 eml_write(my_named_dataframe, ...) fails but dat <- my_named_dataframe; eml_write(dat, ...) works
        # 2 once the wizard launches it asked me twice for each variable
        
        # func
        #library(""devtools"")
        #install_github(""EML"", ""ropensci"")       
        library(""EML"")
        
        # load test data from     
        # McCarthy, M. a., & Masters, P. (2005). Profiting from prior information in Bayesian analyses of ecological data. Journal of Applied Ecology, 42(6), 1012–1019. doi:10.1111/j.1365-2664.2005.01101.x
        
        #Brief description is:
          
        #  Experimental data: effect of cover reduction on mulgara Dasycercus cristicauda: A manipulation of habitat was conducted by Masters, Dickman & Crowther (2003) in which vegetation cover of a site in arid inland Australia was reduced and the response of the mammal fauna monitored.
        
        #you can find the data in the download file from the ""Code for analysing the mulgara experiment"" from here http://www.nceas.ucsb.edu/~mccarthy/research.html
        datatext <- 'Treat, Before, After1, After2
        0,  2.833213344,    1.609437912,    2.48490665
        0,  1.791759469,    2.197224577,    2.079441542
        0,  3.044522438,    2.708050201,    3.135494216
        0,  2.772588722,    1.791759469,    2.197224577
        0,  1.098612289,    1.609437912,    2.63905733
        1,  2.944438979,    0.693147181,    1.791759469
        1,  2.564949357,    0.693147181,    1.791759469
        1,  2.564949357,    1.609437912,    1.609437912
        1,  0.693147181,    1.098612289,    1.098612289
        1,  1.609437912,    0,      1.098612289'
        analyte <- read.csv(textConnection(datatext))
        
        # check
        analyte
        
        # do
        ## from a work dir with a subdir for data
        dir.create(""data"")
        setwd(""data"")
        write.csv(analyte, ""mulgara.csv"", row.names = F)
        
        eml_config(creator=""Ivan Hanigan <ivanhanigan@gmail.com>"")
        eml_write(analyte, file=""mulgara.xml"", title= ""mulgara"")
        # Error in is(dat, ""data.set"") : object 'dat' not found
        # > traceback()
        # 7: is(dat, ""data.set"") at dataTable_methods.R#14
        # 6: eml_dataTable(dat = dat, title = title)
        # 5: initialize(value, ...)
        # 4: initialize(value, ...)
        # 3: new(""dataset"", title = title, creator = who$creator, contact = who$contact, 
        #        coverage = coverage, methods = methods, dataTable = c(eml_dataTable(dat = dat, 
        #                                                                            title = title)), ...) at eml_methods.R#61
        # 2: eml(dat = dat, title = title, creator = creator, contact = contact, 
        #        ...) at eml_write.R#27
        # 1: eml_write(analyte, file = ""mulgara.xml"", title = ""mulgara"")
        
        #### try something else ####
        dat <- analyte
        eml_write(dat, file=""mulgara.xml"", title= ""mulgara"")  
        # at this point I get the wizard...
        # BUT it asks me to enter info for the columns twice (ie Treat.. etc, then Treat again?)
        sessionInfo()
        # R version 3.0.2 (2013-09-25)
        # Platform: x86_64-pc-linux-gnu (64-bit)
        # 
        # locale:
        #   [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C         LC_TIME=C            LC_COLLATE=C         LC_MONETARY=C        LC_MESSAGES=C       
        # [7] LC_PAPER=C           LC_NAME=C            LC_ADDRESS=C         LC_TELEPHONE=C       LC_MEASUREMENT=C     LC_IDENTIFICATION=C 
        # 
        # attached base packages:
        #   [1] stats     graphics  grDevices utils     datasets  methods   base     
        # 
        # other attached packages:
        #   [1] EML_0.0-4
        # 
        # loaded via a namespace (and not attached):
        #   [1] RCurl_1.95-4.1 XML_3.98-1.1   digest_0.6.3   httr_0.2       stringr_0.6.2  tools_3.0.2    uuid_0.1-1     yaml_2.1.10   ",closed,3,eml_write issues
91,25955340,cboettig,2014-01-20T23:58:57Z,2014-07-29T17:34:06Z,"Unfortunately it isn't obvious where to write such a citation in the latest EML, as [discussed](http://lists.nceas.ucsb.edu/ecoinformatics/pipermail/eml-dev/2013-December/002004.html) on eml-dev.  

@mbjones has a proposal for a revision that would add a `citation` class to the `dataset` (and potentially similar objects like spatialRaster etc). https://projects.ecoinformatics.org/ecoinfo/issues/6283

In R this citation will be accessed with `citation_info` method (or `eml_get`), and written with the standard constructors.  Could probably be added to the `eml` helper function constructor to accept bibentry class objects directly.  ",open,0,"Tools for adding and accessing a ""canonical citation"""
90,25770138,cpfaff,2014-01-16T22:15:46Z,2014-07-17T15:16:55Z,o also builds the rmd to md readme,closed,0,adds me as author to rmd readme
89,25631471,cpfaff,2014-01-15T08:01:53Z,2014-01-18T00:30:24Z,Please merge!,closed,1,Litrature
88,25628491,ivanhanigan,2014-01-15T06:15:16Z,2014-07-14T18:00:59Z,,closed,0,fix typo in readme dataon -> dataon_e_
87,25619173,cboettig,2014-01-15T01:12:10Z,2014-01-27T22:17:37Z,"@karthik nice work with the travis integration, looks really promising.  Um, haven't delved into it, not sure what's up with the current build.  It appears that Github is detecting the presence of the travis setup, but then it appears that Travis doesn't find the files it needs to actually run the tests? 

https://travis-ci.org/ropensci/reml/builds/16935548

Any hints?

Also, um, what checks does Travis run anyway?  Given that I have an `inst/tests` directory but not a `/tests` directory, R CMD check doesn't run all tests automatically (since tests assume a network connection, and some assume credentials being available, guess things I need to address still...)  

Anyway, any pointers on what to do to get a successful build passing from the travis integration would be awesome.  ",closed,11,Travis integration
86,25337821,cboettig,2014-01-09T17:54:29Z,2014-01-20T23:40:03Z,"Most things that have a  `citation` slot should really have it as type `ListOfcitation`, since 0 or more `citation` nodes are permitted (e.g. see coverage module).  Likewise we need a `c` method for the `ListOfcitation`.  


Then because bibentry can be a list of bibentry objects

```r
setAs(""bibentry"", ""ListOfcitation"", function(from)
  new(""ListOfcitation"", lapply(from, as, ""citation"")
```",closed,1,ListOfCitation
85,25304056,cpfaff,2014-01-09T09:05:02Z,2014-01-16T19:33:28Z,"o Removes the class union in resource
  - As discussed this is not necessary

o Makes the bib Tex field editor a creator of the
  eml book and edited book

o Adds more tests for the literature module

o belongs to issue #27",closed,6,Improvements to literature and testing
84,25226345,cpfaff,2014-01-08T09:00:18Z,2014-01-08T18:57:56Z,"o Test for article and book coercions
o Belongs to issue #27

I wish you all a good new year. Please merge ;-)",closed,0,Adds testing for literature module
83,25149398,cboettig,2014-01-07T05:51:29Z,2014-07-29T00:39:18Z,Closely related to #68,open,0,eml constructor / dataTable constructor should permit list of data.set / data.frames
82,24947395,dfalster,2014-01-01T23:14:51Z,2014-06-20T23:22:03Z,"Hi guys,

Just trialling the reml package. Unfortunately the example on front page of package does not work straight out of the box, which is a bit discouraging. I did the following 

```coffee
library(""reml"")
dat = data.set(river = c(""SAC"",  ""SAC"",   ""AM""),
               spp   = c(""king"",  ""king"", ""ccho""),
               stg   = c(""smolt"", ""parr"", ""smolt""),
               ct    = c(293L,    410L,    210L),
               col.defs = c(""River site used for collection"",
                            ""Species common name"",
                            ""Life Stage"", 
                            ""count of live fish in traps""),
               unit.defs = list(c(SAC = ""The Sacramento River"", 
                                  AM = ""The American River""),
                                c(king = ""King Salmon"", 
                                  ccho = ""Coho Salmon""),
                                c(parr = ""third life stage"", 
                                  smolt = ""fourth life stage""),
                                ""number""))
eml_config(creator=""Daniel Falster <my@email.com>"")
eml_write(dat, file = ""reml_example.xml"")
```
and get this error

```
Warning messages:
1: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘uuid’
2: In `[<-.data.frame`(`*tmp*`, , value = list(river = c(2L, 2L, 1L :
  Setting class(x) to NULL;   result will no longer be an S4 object
```

Subsequent commands also fail, e.g.

```coffee
 eml_validate(""reml_example.xml"")
Loading required package: RHTMLForms
Error in eml_validate(""reml_example.xml"") : 
  could not find function ""error""
In addition: Warning message:
In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘RHTMLForms’
```
",closed,11,Example has errors
81,24594039,cpfaff,2013-12-19T22:51:05Z,2014-08-02T05:30:26Z,o Belongs to issue #27,closed,0,Adds more literature type coercions
80,24341804,cpfaff,2013-12-16T10:49:06Z,2014-07-12T23:37:21Z,"o Address to character 
o More coercions of bibliography types",closed,6,Litrature
79,24300042,cpfaff,2013-12-14T21:32:48Z,2014-07-09T18:40:42Z,More improvements on litrerature module. ,closed,1,Litrature
78,24286733,cboettig,2013-12-14T06:02:57Z,2014-09-29T16:46:00Z,"Date columns are somewhat problematic than they should be.  Several challenges exist here:

- [ ] the R user may represent dates as almost any type: integer, numeric, character, factor, or Date.  So detecting the format based on the column class isn't reliable.  (there may be good reasons of course to treat date as a factor for a given statistical test or plot, etc)
- [ ] R's Date type insists on a fully specified date-time object. So if a user has separate columns for year and day, we can't treat each as an date column independently.  
- [x] EML's date format does't use the ISO C99 / POSIX standard for `strftime`, for instance Julian days might be specified using the less explicit format string `DDD` rather than `%j`.  

current automated handling of dates based on column types can result in incorrect EML (see Advanced writing example).  Typing these as characters and explicitly declaring it as a date in the unit metadata.  ",open,1,Dealing with dates
77,24235393,cpfaff,2013-12-13T09:35:05Z,2014-07-11T05:00:44Z,"o Moves classes
	- recipient, performer, institution to publisher in party.R
o Adds coercions and lists
	- recipient, performer, institution in party.R
o refactors citation classes
	- now the classes no longer call simple classes other in slots
	  simple type is used directly
o citation class coercions
	- refactors for new design
	- finishes book <-> s4",closed,5,Improves the literature module
76,24201272,cboettig,2013-12-12T19:15:54Z,2014-07-29T00:40:57Z,"Data frames may often include actual lat-long coordinates, species names, dates, etc.  It would be useful for a user to just point reml to which column has the relevant information (or perhaps more ideally but also tricky: extract automatically based on trigger words in the column metadata) and construct the appropriate coverage nodes automatically.  ",open,1,Infer coverage from data
75,24165963,cpfaff,2013-12-12T09:27:44Z,2013-12-13T09:40:35Z,"Coercions: 

o article <-> bibentry 
o book -> bibentry
",closed,2,Litrature module improvements
74,24134447,cpfaff,2013-12-11T20:15:24Z,2014-06-29T22:10:31Z,"o Uses setAs now as it is a coercion method (only one way atm: s4 to bibentry)
o See working example below.

```coffee
b <- new(""article"")
c <- new(""ListOfcreator"", firstName = ""test"")
j <- new(""journal"", journal = ""Testjournal"")
v <- new(""volume"", volume = ""1"")

b@creator = c
b@title = ""the title""  # So that the entry isn't empty
b@journal = j
b@pubDate = ""Test""

b@volume = v

as(b, ""XMLInternalElementNode"")
theentry = as(b, ""bibentry"")

print(theentry)

print(theentry, style = ""citation"")
print(theentry, style = ""html"")
print(theentry, style = ""bibtex"")
print(theentry, style = ""R"")
```

Belongs to issue #27",closed,2,Adds working version of bib entry mapping for article
73,24127394,cpfaff,2013-12-11T18:33:29Z,2014-06-18T10:56:21Z,"Fixes the atomic class slot names

o now all slot names of the atomic classes for the citation types
  are named after the class.

Puts ""citation"" slot in place for ""eml""

This is related to issue #27

Adds more coercion methods for literature classes

o This belongs to issue #27

Changes documentation and inclusion

o Adds the modules to be included into the literature module
	- adds modules already there for inclusion
o changes the citation prototype in comments

defines XML/S4 coercions for citation class, corrects them for article class #27",closed,2,Represents the work of 5 commits squashed
72,24125968,cboettig,2013-12-11T18:13:58Z,2014-07-29T17:34:06Z,"This is actually a non-trivial problem due to changes in the EML design, see discussion thread on the eml-dev listserve:

http://lists.nceas.ucsb.edu/ecoinformatics/pipermail/eml-dev/2013-December/thread.html

@cpfaff you probably want to join the listserve too.  ",open,1,Tool for adding citation metadata for the publication associated with the data file
71,23992886,cboettig,2013-12-09T21:04:44Z,2014-07-23T17:38:10Z,"Several functions/methods are being exported to the user that may be cluttering the namespace.  Function naming conventions for the user API also need redesigning. (Both of these are slightly tedious to implement since the test suite must be updated as well, lots of `sed -i`...).  

In particular: 
- function naming convention should clearly distinguish accessor/get methods vs write/add methods.  e.g. not clear if function `keywords` is `get_keywords` or `add_keywords`.  
- Consider single master `eml_get` and `eml_add` functions rather than export a function for every element for which we want an add/append method and/or an extraction method.  

",closed,1,clean up namespace / improve user interface
70,23924622,cpfaff,2013-12-08T14:56:25Z,2014-06-14T10:49:56Z,The litrature module so far and correction of a missing bracket in the constructor call in `eml`. Adds validation for `listOfAttributes` as well. Please merge.,closed,5,Litrature plus
69,23899698,cboettig,2013-12-07T04:37:57Z,2014-07-23T17:44:36Z,"one file -- one function doesn't really make sense for the class definitions and the coercion methods to/from XML, and a file for each class would be nuts.  Probably most sensible to mimic the structure of the xsd files (and eml documentation), with class definitions in a .R file named corresponding to the .xsd that defines the class in the schema.  All other functions (at least all other exported functions) would then be in a file of corresponding name.  

Organization is roughly like this, but not completely.  ",closed,2,Define a logical organization for file names
68,23892046,cboettig,2013-12-06T23:12:14Z,2014-01-07T05:51:29Z,because we can have multiple dataTable children of one dataset.  This particular case will be somewhat more annoying to fix then usual do to heavy use elsewhere in the code...,closed,0,dataset@dataTable slot should have class ListOfdataTable
67,23891958,cboettig,2013-12-06T23:10:26Z,2013-12-12T01:05:57Z,"Because it is always more elegant to do: `dataset@datatable <- c(dataTable, dataTable2)` then it is to do: `dataset@dataTable <- new(""ListOfdataTable"", list(dataTable, dataTable2))`, and so forth.  ",closed,1,Define concatenation methods for any class that includes a `ListOf` class
66,23891803,cboettig,2013-12-06T23:07:49Z,2014-01-21T01:39:46Z,"As I mention in #64 the current interface has some drawbacks.   

I'm pretty content with the `data.set` part of the interface (though I sometimes wonder about extending it to have other metadata?)  Once a user has created a `data.set`, a lot of the additional metadata can be created using the default S4 constructor for the class rather than a dedicated function.  

For instance, here I build up the ""contact"" and ""publisher"" nodes, illustrating re-use of elements: 



```coffee
aaron <- as.person(""Aaron Ellison <fakeaddress@email.com>"")

HF_address <- new(""address"", 
        deliveryPoint = ""324 North Main Street"",
        city = ""Petersham"",
        administrativeArea = ""MA"",
        postalCode = ""01366""
        country = ""USA"") 

contact <- new(""contact"", 
    individualName = aaron,
    organizationName = ""Harvard Forest"", 
    electronicMailAddress = aaron$email, 
    phone = ""000-000-0000"",
    address = HF_address)

publisher <- new(""publisher"", 
    organizationName = ""Harvard Forest"",
    address = HF_address)
  
creator <- as.person(c(""Aaron Ellison"", ""Nicholas Gotelli""))
researchers <- as.person(c(""Benjamin Baiser [ctb]"", ""Jennifer Sirota [ctb]""))
```

Elsewhere this looks less elegant.  For instance, define `keywordSet` using the constructor is ugly:

```coffee
new(""ListOfkeywordSet"", 
        list(new(""keywordSet"", 
                      keywordThesaurus = ""LTER controlled vocabulary"", 
                      keyword = new(""ListOfkeyword"", 
                                      list(""bacteria"", ""carnivorous plants"", ""genetics"", ""thresholds""))),
             new(""keywordSet"", ...

))
````

While giving a named list of character vectors is much more intuitive and consise:

```
list(""LTER controlled vocabulary"" = c(""bacteria"", ""carnivorous plants"", ""genetics"", ""thresholds""),
                 ""LTER core area"" = c(""populations"", ""inorganic nutrients"", ""disturbance""),
                 ""HFR default"" = c(""Harvard Forest"", ""HFR"", ""LTER"", ""USA""))
```

Defining a concatenation method for classes such as `keyword` would avoid having to write `ListOfkeyword` etc, but still not be elegant.  


Elsewhere we use helper functions that don't capture the full expressiveness of EML but simplify the declaration quite a bit.  A full coverage node can be built with:  

```coffee
coverage <- eml_coverage(scientific_names = ""Sarracenia purpurea"", 
             dates = c('2012-06-01' - '2013-12-31'),
             geographic_description = ""Harvard Forest Greenhouse, Tom Swamp Tract (Harvard Forest)"", 
             NSEWbox = c( 42.55,  42.42, -72.1, -72.29, 160, 330))
```


Elsewhere we discuss the use case of reading in paragraph or longer text strings from MS Word, which is probably the natural place someone will write things like methods (sure, they could attempt copy-paste, but that's non-programmatic workflow).  Proof of principle looks like this: 


```coffee
library(RWordXML)
f <- wordDoc(""inst/examples/methods.docx"")
doc <- methods[[getDocument(f)]]
txt <- xpathSApply(doc, ""//w:t"", xmlValue)
methods_section <- paste(txt, collapse = ""\n\n"") 
```

A better implementation would add `<title>` `<section>` and `<para>` blocks, @emhart may be working on this (and other cool tricks in parsing docx).  #62 


As a fourth alternative construction strategy, a user might simply reuse nodes from existing EML files: 

```coffee
additionalMetadata <- hf205@additionalMetadata 
```


Having built these elements, the remaining assembly of `dataTable`, `dataset`, and `eml` is probably _too_ modular: 

```coffee
dataTable <- eml_dataTable(dat, 
                           title = ""Supplemental Data, table 1"", 
                           description = ""Metadata documentation for S1.csv"", 
                           file = ""S1.csv"")
dataset <- new(""dataset"", 
                title = title,
                creator = creator,
                contact = contact,
                coverage = coverage,
                methods = methods,
                dataTable = dataTable)
eml <- new(""eml"", 
            dataset = dataset,
            additionalMetadata = additionalMetadata)
```

I'm undecided on if/how to wrap these steps into a single function with sensible defaults, but remaining more flexible than the current `eml` or eml_write functions.  

Constructing an internal `eml` in a separate command from `write.eml` (creating the external XML file) is probably sensible, as a host of functions from the READ side of our API are designed to work with the `eml` object, as could some of the additional methods (perhaps things like add_additionalMetadata, add_dataTable, etc, to add these elements to existing EML).  


Writing out would then be the last step, a simple command:

```coffee
write.eml(eml, ""hf205_from_reml.xml"")
```

Ideally eml could be a data.frame with metadata given separately, or a data.set, but would usually be a complete eml class object.  ",closed,1,Improved modular user interface
65,23890360,cboettig,2013-12-06T22:41:47Z,2014-01-21T00:55:56Z,"@mbjones So I'm thinking about putting these XSLT files from #60 as part of the package and wrapping an R function around them, something like get_rdf perhaps.  It looks like they are under the Apache license, whereas `reml` currently declares CC0 license.  Perhaps we should just move everything to Apache?  Also, I didn't see info on authorship/acknowledgement.  If we actually include the XSL files in the R package, anything we should add to the author/contributor/compiler/creator/translator (see `?person`) info in the package DESCRIPTION?

Note any other license/acknowledgement issues to address under this package too.  e.g.: 

- [ ] currently using Aaron Ellison's hf205.xml eml in some examples.
- [ ] various other XSL files we might wrap in a function for converting to/from other formats.  See  https://github.com/ropensci/reml/tree/master/inst/xsl",closed,3,License and acknowledgement questions
64,23882045,karthik,2013-12-06T20:09:57Z,2014-07-23T17:25:45Z,"In light of the fact that the package has changed significantly since the last time we wrote a demo, time to work on a few new examples that include the `data.set` implementation.",closed,4,Update the demo and examples for workshops
63,23846227,cpfaff,2013-12-06T09:32:42Z,2014-01-21T02:00:10Z,"I think the name could lead to a bit confusion as reml is already 
taken for restricted maximum likelihood. :-)",closed,8,reml as package name
62,23818789,cboettig,2013-12-05T21:23:16Z,2014-07-29T00:41:23Z,"No one wants to write 3 pages of text at the R command line.  Most researchers will write the methods paragraphs in Word.  Since this is XML we can parse with packages like [RWordXML](), we should be able to extract the text and basic structure (paragraph, title, section) and write that into the appropriate EML methods structure. 

For a concrete example, [here is a docx version of the methods](https://github.com/ropensci/reml/raw/14d30700b8e9f82ebdeb76d0f5a266d01a770853/inst/examples/methods.docx), taken from [this section of a real EML file](https://github.com/ropensci/reml/blob/14d30700b8e9f82ebdeb76d0f5a266d01a770853/inst/examples/hf205.xml#L144-L162).  How would we parse the docx to recover/generate that EML element?  

I got only as far as

```coffee
methods <- wordDoc(""methods.docx"")
```

not seeing how I access the text. @duncantl any hints?",open,25,Parse a .docx file to get `methods` and other text
61,23666625,cboettig,2013-12-03T19:23:49Z,2013-12-06T23:12:54Z,"At a couple points the EML schema will define a node that can contain itself.  For instance, `<protocol>` [contains](http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-protocol.png) `<proceduralStep>` [which contains](http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-methods.png) `<protocol>`.

I'm not sure how the schema accomplishes this, but R doesn't like defining S4 objects that contain themselves.  @mbjones Am I missing something or is there a clever way around this?  ",closed,2,How do we handle recursive / self-referencing elements?
60,23664577,cboettig,2013-12-03T18:53:22Z,2014-07-29T17:34:06Z,"This issue builds on the ideas discussed in #5.  We can broadly separate out three use-cases for RDFa annotations: 

1. Extending EML.  If we want to provide additional machine-readable / structured metadata that cannot be expressed in EML, we can always add this information in an additionalMetadata section. (Need not be semantic, could just be XML).  This is already illustrated in #50.  

2. Providing semantic versions of EML terms that can be understood by more generic tools.  We could duplicate information that is already specified elsewhere in the EML, but express it here in RDFa (e.g. dc:title, dc:creator, etc).  This would allow a generic RDFa distiller to extract the metadata into a triples library where it could be easily queried with SPARQL tools.  On the other hand, this feels a bit like a hack -- perhaps an XSLT conversion of (some subset of?) EML to RDF would make more sense?

3. Adding semantic meaning to EML metadata fields that are currently expressed only in free-form text.  This is the real use case from #5 and #8, allowing us to provide semantic definitions of units and measurements such that we can reason with them, e.g.


```xml
<additionalMetadata>
     <describes>1838</describes> <!--reference the attribute's id-->
     <metadata>
      <subject about=""http://some.namespace#1838"" xmlns:o=""http:/oboe-core#"">
          <meta property=""o:entity"" content=""Air"" datatype=""xsd:string""/>
          <meta property=""o:characteristic"" content=""Temperature"" datatype=""xsd:string""/>
          <meta property=""o:unit"" content=""Celsius"" datatype=""xsd:string""/>
      </subject>
    </metadata>
  </additionalMetadata>
```

",open,9,Tools to facilitate serializing/parsing of RDFa in additionalMetadata
59,23640793,cpfaff,2013-12-03T13:00:42Z,2013-12-08T18:35:06Z,"There is various metadata formats out there that are based on xml. Many small 
projects also define their own xml scheme for metadata. Why not make the package so general
that it would be easy to use it with an own scheme? Is this feasable or does this 
not make any sense at all?
",closed,2,Question/Idea: Make the package more general (long term)
58,23602388,cpfaff,2013-12-02T21:16:49Z,2013-12-02T22:16:05Z,"I read a tutorial about s4 classes. 

http://www.bioconductor.org/help/course-materials/2013/CSAMA2013/friday/afternoon/S4-tutorial.pdf

It is mentioned that the `representation` function for slots in s4 functions is deprecated and the `slot = c(....)` should be preferred. 

=> Replace all `representation()` function calls by the preferred way!",closed,3,representation deprecated?
57,21986519,cboettig,2013-11-01T21:45:31Z,2013-11-01T23:02:56Z,"Is there an EML-discuss listserve where users could pose questions about recommended way to encode certain things?  Could potentially be a valuable interface between users and developers (kinda the way r-sig-phylo, and to lesser extent, nexml-discuss lists work).

I encounter this kind of challenge all the time now as I think about adding EML annotation to my own work. Forcing myself to write out the metadata would have helped me avoid errors like the one that scared me for the last 24 hours where I mistook a 'profits' column as a profits-costs column.  I'm often working with simulated data, which I have no idea how to annotate properly; first because the units are often arbitrary, and second because it's obviously crucial it does not get mistaken for 'real' data if it is ever archived on a service like KNB. Would love to tap some community guidance on how to handle simulated data.

As we step into semantics/ontology realm, being able to poll community input becomes particularly important, both in making choices and recognizing priorities for future ontology development, as we're discovering in NeXML where essentially all metadata must be semantic. (e.g. https://github.com/ropensci/RNeXML/issues/24, https://github.com/ropensci/RNeXML/issues/26)",closed,1,EML-discuss listserve?
56,21945843,mbjones,2013-11-01T07:55:13Z,2013-11-04T01:15:07Z,"@cboettig Could you please archive any metadata files, data files, and resource maps that you might have created on the KNB during the course of testing?  We would prefer to only have real content on the KNB.  A preliminary list can be seen here:

https://knb.ecoinformatics.org/m/#data/search/Boettiger

curl could produce a more comprehensive list.  

You can also use curl to do so by calling the [MN.archive() REST service ](http://mule1.dataone.org/ArchitectureDocs-current/apis/MN_APIs.html#MNStorage.archive) (unfortunately, the API call is not in the R client yet).",closed,11,archive test content from the KNB
55,21923427,cboettig,2013-10-31T20:16:02Z,2013-11-03T23:25:48Z,"If you do:

```coffee
dat = eml_read(""inst/examples/hf205.xml"")
```
you get an error because R attempts to read the associated `csv` file into a `data.frame`, but cannot find the file.  The EML file contains only the filename of the csv file, which it looks for in the working directory, not in the matching `inst/examples/`. 

Further, `eml_read` might be modified to try the online URL if it is available and no matching local file can be found.  
",closed,1,eml_read should handle local paths and automatic downloads
54,21922873,cboettig,2013-10-31T20:07:09Z,2013-10-31T21:59:29Z,"Currently `reml` simply ignores writing an `access` node.  

- Now that we can publish to the KNB (#20), a repository that presumably respects the authentication and access controls defined in this node, perhaps we should be doing so (even for public files)?  

- Is there a use-case for adding an access node declaring public read access for EML that is simply going to be a public file, without necessarily having a clear home? (e.g. maybe because it is going to be uploaded to figshare, dryad, or a journal's supplemental materials section, etc)?  

- Should an access node be generated by any call to `eml_write`, or just when publishing to the KNB (e.g. added by the `eml_knb` function?)  

Obviously there is a very compelling case for publishing to KNB over the alternatives, since the former actually understands and indexes the metadata, but that is a case we can hopefully make in the manuscript.  ",closed,2,when to write an access node?
53,21349335,cboettig,2013-10-21T22:29:15Z,2013-12-03T17:45:50Z,"Not sure if this idea makes sense, so looking for feedback.  

It seems like it would be possible to provide utilities (or preferably interfaces to existing utilities) that convert between EML and other formats (As 
1:1 mappings are probably often not available, this might be largely one-way mappings, but could still be useful).  

@mbjones might you comment on what of these might be useful and if you have any pointers to existing utilities we can wrap to provide this capacity?  

- [ ] For instance, 10.1146/annurev.ecolsys.37.091305.110031 mentions a partial mapping being available to BDP (biological data profile used by NBII). 

Other options might include

- [ ] netcdf representation.  This could use existing R tools to write EML to netcdf (possibly valuable for compression/speed and interfacing with existing tools designed for netcdf. Unfortunately it looks like the RNetCDF package only supports netcdf3... 

- [ ] Dublin core elements, etc.  Obviously natural to extract and provide the very basic metadata as dublin core; and potentially write a bunch of dublin core into the suitable EML slots (title, contributor, coverage, etc.  e.g. at the level used by Dryad: http://datadryad.org/resource/doi:10.5061/dryad.50hd2/1?show=full) 

- [ ] ... add additional ideas below.",closed,9,Support conversion between metadata standards
52,21138766,SimonHStats,2013-10-17T08:30:23Z,2013-10-17T08:31:48Z,"I've posted this question on Stack Overflow, but I wonder if I'm better coming straight here.

Here's the discussion so far on SO.

Ok, I'm trying to convert the following JSON data into an R data frame.

For some reason fromJSON in the RJSONIO package only reads up to about character 380 and then it stops converting the JSON properly.

Here is the JSON:-


    ""{\""metricDate\"":\""2013-05-01\"",\""pageCountTotal\"":\""33682\"",\""landCountTotal\"":\""11838\"",\""newLandCountTotal\"":\""8023\"",\""returnLandCountTotal\"":\""3815\"",\""spiderCountTotal\"":\""84\"",\""goalCountTotal\"":\""177.000000\"",\""callGoalCountTotal\"":\""177.000000\"",\""callCountTotal\"":\""237.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.50\"",\""callConversionPerc\"":\""74.68\""}\n{\""metricDate\"":\""2013-05-02\"",\""pageCountTotal\"":\""32622\"",\""landCountTotal\"":\""11626\"",\""newLandCountTotal\"":\""7945\"",\""returnLandCountTotal\"":\""3681\"",\""spiderCountTotal\"":\""58\"",\""goalCountTotal\"":\""210.000000\"",\""callGoalCountTotal\"":\""210.000000\"",\""callCountTotal\"":\""297.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.81\"",\""callConversionPerc\"":\""70.71\""}\n{\""metricDate\"":\""2013-05-03\"",\""pageCountTotal\"":\""28467\"",\""landCountTotal\"":\""11102\"",\""newLandCountTotal\"":\""7786\"",\""returnLandCountTotal\"":\""3316\"",\""spiderCountTotal\"":\""56\"",\""goalCountTotal\"":\""186.000000\"",\""callGoalCountTotal\"":\""186.000000\"",\""callCountTotal\"":\""261.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.68\"",\""callConversionPerc\"":\""71.26\""}\n{\""metricDate\"":\""2013-05-04\"",\""pageCountTotal\"":\""20884\"",\""landCountTotal\"":\""9031\"",\""newLandCountTotal\"":\""6670\"",\""returnLandCountTotal\"":\""2361\"",\""spiderCountTotal\"":\""51\"",\""goalCountTotal\"":\""7.000000\"",\""callGoalCountTotal\"":\""7.000000\"",\""callCountTotal\"":\""44.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.08\"",\""callConversionPerc\"":\""15.91\""}\n{\""metricDate\"":\""2013-05-05\"",\""pageCountTotal\"":\""20481\"",\""landCountTotal\"":\""8782\"",\""newLandCountTotal\"":\""6390\"",\""returnLandCountTotal\"":\""2392\"",\""spiderCountTotal\"":\""58\"",\""goalCountTotal\"":\""1.000000\"",\""callGoalCountTotal\"":\""1.000000\"",\""callCountTotal\"":\""8.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.01\"",\""callConversionPerc\"":\""12.50\""}\n{\""metricDate\"":\""2013-05-06\"",\""pageCountTotal\"":\""25175\"",\""landCountTotal\"":\""10019\"",\""newLandCountTotal\"":\""7082\"",\""returnLandCountTotal\"":\""2937\"",\""spiderCountTotal\"":\""62\"",\""goalCountTotal\"":\""24.000000\"",\""callGoalCountTotal\"":\""24.000000\"",\""callCountTotal\"":\""47.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.24\"",\""callConversionPerc\"":\""51.06\""}\n{\""metricDate\"":\""2013-05-07\"",\""pageCountTotal\"":\""35892\"",\""landCountTotal\"":\""12615\"",\""newLandCountTotal\"":\""8391\"",\""returnLandCountTotal\"":\""4224\"",\""spiderCountTotal\"":\""62\"",\""goalCountTotal\"":\""239.000000\"",\""callGoalCountTotal\"":\""239.000000\"",\""callCountTotal\"":\""321.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.89\"",\""callConversionPerc\"":\""74.45\""}\n{\""metricDate\"":\""2013-05-08\"",\""pageCountTotal\"":\""34106\"",\""landCountTotal\"":\""12391\"",\""newLandCountTotal\"":\""8389\"",\""returnLandCountTotal\"":\""4002\"",\""spiderCountTotal\"":\""90\"",\""goalCountTotal\"":\""221.000000\"",\""callGoalCountTotal\"":\""221.000000\"",\""callCountTotal\"":\""295.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.78\"",\""callConversionPerc\"":\""74.92\""}\n{\""metricDate\"":\""2013-05-09\"",\""pageCountTotal\"":\""32721\"",\""landCountTotal\"":\""12447\"",\""newLandCountTotal\"":\""8541\"",\""returnLandCountTotal\"":\""3906\"",\""spiderCountTotal\"":\""54\"",\""goalCountTotal\"":\""207.000000\"",\""callGoalCountTotal\"":\""207.000000\"",\""callCountTotal\"":\""280.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.66\"",\""callConversionPerc\"":\""73.93\""}\n{\""metricDate\"":\""2013-05-10\"",\""pageCountTotal\"":\""29724\"",\""landCountTotal\"":\""11616\"",\""newLandCountTotal\"":\""8063\"",\""returnLandCountTotal\"":\""3553\"",\""spiderCountTotal\"":\""139\"",\""goalCountTotal\"":\""207.000000\"",\""callGoalCountTotal\"":\""207.000000\"",\""callCountTotal\"":\""301.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.78\"",\""callConversionPerc\"":\""68.77\""}\n{\""metricDate\"":\""2013-05-11\"",\""pageCountTotal\"":\""22061\"",\""landCountTotal\"":\""9660\"",\""newLandCountTotal\"":\""6971\"",\""returnLandCountTotal\"":\""2689\"",\""spiderCountTotal\"":\""52\"",\""goalCountTotal\"":\""3.000000\"",\""callGoalCountTotal\"":\""3.000000\"",\""callCountTotal\"":\""40.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.03\"",\""callConversionPerc\"":\""7.50\""}\n{\""metricDate\"":\""2013-05-12\"",\""pageCountTotal\"":\""23341\"",\""landCountTotal\"":\""9935\"",\""newLandCountTotal\"":\""6960\"",\""returnLandCountTotal\"":\""2975\"",\""spiderCountTotal\"":\""45\"",\""goalCountTotal\"":\""0.000000\"",\""callGoalCountTotal\"":\""0.000000\"",\""callCountTotal\"":\""12.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.00\"",\""callConversionPerc\"":\""0.00\""}\n{\""metricDate\"":\""2013-05-13\"",\""pageCountTotal\"":\""36565\"",\""landCountTotal\"":\""13583\"",\""newLandCountTotal\"":\""9277\"",\""returnLandCountTotal\"":\""4306\"",\""spiderCountTotal\"":\""69\"",\""goalCountTotal\"":\""246.000000\"",\""callGoalCountTotal\"":\""246.000000\"",\""callCountTotal\"":\""324.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.81\"",\""callConversionPerc\"":\""75.93\""}\n{\""metricDate\"":\""2013-05-14\"",\""pageCountTotal\"":\""35260\"",\""landCountTotal\"":\""13797\"",\""newLandCountTotal\"":\""9375\"",\""returnLandCountTotal\"":\""4422\"",\""spiderCountTotal\"":\""59\"",\""goalCountTotal\"":\""212.000000\"",\""callGoalCountTotal\"":\""212.000000\"",\""callCountTotal\"":\""283.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.54\"",\""callConversionPerc\"":\""74.91\""}\n{\""metricDate\"":\""2013-05-15\"",\""pageCountTotal\"":\""35836\"",\""landCountTotal\"":\""13792\"",\""newLandCountTotal\"":\""9532\"",\""returnLandCountTotal\"":\""4260\"",\""spiderCountTotal\"":\""94\"",\""goalCountTotal\"":\""187.000000\"",\""callGoalCountTotal\"":\""187.000000\"",\""callCountTotal\"":\""258.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.36\"",\""callConversionPerc\"":\""72.48\""}\n{\""metricDate\"":\""2013-05-16\"",\""pageCountTotal\"":\""33136\"",\""landCountTotal\"":\""12821\"",\""newLandCountTotal\"":\""8755\"",\""returnLandCountTotal\"":\""4066\"",\""spiderCountTotal\"":\""65\"",\""goalCountTotal\"":\""192.000000\"",\""callGoalCountTotal\"":\""192.000000\"",\""callCountTotal\"":\""260.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.50\"",\""callConversionPerc\"":\""73.85\""}\n{\""metricDate\"":\""2013-05-17\"",\""pageCountTotal\"":\""29564\"",\""landCountTotal\"":\""11721\"",\""newLandCountTotal\"":\""8191\"",\""returnLandCountTotal\"":\""3530\"",\""spiderCountTotal\"":\""213\"",\""goalCountTotal\"":\""166.000000\"",\""callGoalCountTotal\"":\""166.000000\"",\""callCountTotal\"":\""222.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.42\"",\""callConversionPerc\"":\""74.77\""}\n{\""metricDate\"":\""2013-05-18\"",\""pageCountTotal\"":\""23686\"",\""landCountTotal\"":\""9916\"",\""newLandCountTotal\"":\""7335\"",\""returnLandCountTotal\"":\""2581\"",\""spiderCountTotal\"":\""56\"",\""goalCountTotal\"":\""5.000000\"",\""callGoalCountTotal\"":\""5.000000\"",\""callCountTotal\"":\""34.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.05\"",\""callConversionPerc\"":\""14.71\""}\n{\""metricDate\"":\""2013-05-19\"",\""pageCountTotal\"":\""23528\"",\""landCountTotal\"":\""9952\"",\""newLandCountTotal\"":\""7184\"",\""returnLandCountTotal\"":\""2768\"",\""spiderCountTotal\"":\""57\"",\""goalCountTotal\"":\""1.000000\"",\""callGoalCountTotal\"":\""1.000000\"",\""callCountTotal\"":\""14.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.01\"",\""callConversionPerc\"":\""7.14\""}\n{\""metricDate\"":\""2013-05-20\"",\""pageCountTotal\"":\""37391\"",\""landCountTotal\"":\""13488\"",\""newLandCountTotal\"":\""9024\"",\""returnLandCountTotal\"":\""4464\"",\""spiderCountTotal\"":\""69\"",\""goalCountTotal\"":\""227.000000\"",\""callGoalCountTotal\"":\""227.000000\"",\""callCountTotal\"":\""291.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.68\"",\""callConversionPerc\"":\""78.01\""}\n{\""metricDate\"":\""2013-05-21\"",\""pageCountTotal\"":\""36299\"",\""landCountTotal\"":\""13174\"",\""newLandCountTotal\"":\""8817\"",\""returnLandCountTotal\"":\""4357\"",\""spiderCountTotal\"":\""77\"",\""goalCountTotal\"":\""164.000000\"",\""callGoalCountTotal\"":\""164.000000\"",\""callCountTotal\"":\""221.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.24\"",\""callConversionPerc\"":\""74.21\""}\n{\""metricDate\"":\""2013-05-22\"",\""pageCountTotal\"":\""34201\"",\""landCountTotal\"":\""12433\"",\""newLandCountTotal\"":\""8388\"",\""returnLandCountTotal\"":\""4045\"",\""spiderCountTotal\"":\""76\"",\""goalCountTotal\"":\""195.000000\"",\""callGoalCountTotal\"":\""195.000000\"",\""callCountTotal\"":\""262.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.57\"",\""callConversionPerc\"":\""74.43\""}\n{\""metricDate\"":\""2013-05-23\"",\""pageCountTotal\"":\""32951\"",\""landCountTotal\"":\""11611\"",\""newLandCountTotal\"":\""7757\"",\""returnLandCountTotal\"":\""3854\"",\""spiderCountTotal\"":\""68\"",\""goalCountTotal\"":\""167.000000\"",\""callGoalCountTotal\"":\""167.000000\"",\""callCountTotal\"":\""231.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.44\"",\""callConversionPerc\"":\""72.29\""}\n{\""metricDate\"":\""2013-05-24\"",\""pageCountTotal\"":\""28967\"",\""landCountTotal\"":\""10821\"",\""newLandCountTotal\"":\""7396\"",\""returnLandCountTotal\"":\""3425\"",\""spiderCountTotal\"":\""106\"",\""goalCountTotal\"":\""167.000000\"",\""callGoalCountTotal\"":\""167.000000\"",\""callCountTotal\"":\""203.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.54\"",\""callConversionPerc\"":\""82.27\""}\n{\""metricDate\"":\""2013-05-25\"",\""pageCountTotal\"":\""19741\"",\""landCountTotal\"":\""8393\"",\""newLandCountTotal\"":\""6168\"",\""returnLandCountTotal\"":\""2225\"",\""spiderCountTotal\"":\""78\"",\""goalCountTotal\"":\""0.000000\"",\""callGoalCountTotal\"":\""0.000000\"",\""callCountTotal\"":\""28.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.00\"",\""callConversionPerc\"":\""0.00\""}\n{\""metricDate\"":\""2013-05-26\"",\""pageCountTotal\"":\""19770\"",\""landCountTotal\"":\""8237\"",\""newLandCountTotal\"":\""6009\"",\""returnLandCountTotal\"":\""2228\"",\""spiderCountTotal\"":\""79\"",\""goalCountTotal\"":\""0.000000\"",\""callGoalCountTotal\"":\""0.000000\"",\""callCountTotal\"":\""8.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.00\"",\""callConversionPerc\"":\""0.00\""}\n{\""metricDate\"":\""2013-05-27\"",\""pageCountTotal\"":\""26208\"",\""landCountTotal\"":\""9755\"",\""newLandCountTotal\"":\""6779\"",\""returnLandCountTotal\"":\""2976\"",\""spiderCountTotal\"":\""82\"",\""goalCountTotal\"":\""26.000000\"",\""callGoalCountTotal\"":\""26.000000\"",\""callCountTotal\"":\""40.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.27\"",\""callConversionPerc\"":\""65.00\""}\n{\""metricDate\"":\""2013-05-28\"",\""pageCountTotal\"":\""36980\"",\""landCountTotal\"":\""12463\"",\""newLandCountTotal\"":\""8226\"",\""returnLandCountTotal\"":\""4237\"",\""spiderCountTotal\"":\""132\"",\""goalCountTotal\"":\""208.000000\"",\""callGoalCountTotal\"":\""208.000000\"",\""callCountTotal\"":\""276.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.67\"",\""callConversionPerc\"":\""75.36\""}\n{\""metricDate\"":\""2013-05-29\"",\""pageCountTotal\"":\""34190\"",\""landCountTotal\"":\""12014\"",\""newLandCountTotal\"":\""8279\"",\""returnLandCountTotal\"":\""3735\"",\""spiderCountTotal\"":\""90\"",\""goalCountTotal\"":\""179.000000\"",\""callGoalCountTotal\"":\""179.000000\"",\""callCountTotal\"":\""235.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.49\"",\""callConversionPerc\"":\""76.17\""}\n{\""metricDate\"":\""2013-05-30\"",\""pageCountTotal\"":\""33867\"",\""landCountTotal\"":\""11965\"",\""newLandCountTotal\"":\""8231\"",\""returnLandCountTotal\"":\""3734\"",\""spiderCountTotal\"":\""63\"",\""goalCountTotal\"":\""160.000000\"",\""callGoalCountTotal\"":\""160.000000\"",\""callCountTotal\"":\""219.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.34\"",\""callConversionPerc\"":\""73.06\""}\n{\""metricDate\"":\""2013-05-31\"",\""pageCountTotal\"":\""27536\"",\""landCountTotal\"":\""10302\"",\""newLandCountTotal\"":\""7333\"",\""returnLandCountTotal\"":\""2969\"",\""spiderCountTotal\"":\""108\"",\""goalCountTotal\"":\""173.000000\"",\""callGoalCountTotal\"":\""173.000000\"",\""callCountTotal\"":\""226.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.68\"",\""callConversionPerc\"":\""76.55\""}\n\r\n""

and here is my R output

     metricDate 
    ""2013-05-01"" 
    pageCountTotal 
    ""33682"" 
    landCountTotal 
    ""11838"" 
    newLandCountTotal 
    ""8023"" 
    returnLandCountTotal 
    ""3815"" 
    spiderCountTotal 
    ""84"" 
    goalCountTotal 
    ""177.000000"" 
    callGoalCountTotal 
    ""177.000000"" 
    callCountTotal 
    ""237.000000"" 
    onlineGoalCountTotal 
    ""0.000000"" 
    conversionPerc 
    ""1.50"" 
    callConversionPerc

 
    ""74.68\""}{\""metricDate\"":\""2013-05-02\"",\""pageCountTotal\"":\""32622\"",\""landCountTotal\"":\""11626\"",\""newLandCountTotal\"":\""7945\"",\""returnLandCountTotal\"":\""3681\"",\""spiderCountTotal\"":\""58\"",\""goalCountTotal\"":\""210.000000\"",\""callGoalCountTotal\"":\""210.000000\"",\""callCountTotal\"":\""297.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.81\"",\""callConversionPerc\"":\""70.71\""}{\""metricDate\"":\""2013-05-03\"",\""pageCountTotal\"":\""28467\"",\""landCountTotal\"":\""11102\"",\""newLandCountTotal\"":\""7786\"",\""returnLandCountTotal\"":\""3316\"",\""spiderCountTotal\"":\""56\"",\""goalCountTotal\"":\""186.000000\"",\""callGoalCountTotal\"":\""186.000000\"",\""callCountTotal\"":\""261.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.68\"",\""callConversionPerc\"":\""71.26\""}{\""metricDate\"":\""2013-05-04\"",\""pageCountTotal\"":\""20884\"",\""landCountTotal\"":\""9031\"",\""newLandCountTotal\"":\""6670\"",\""returnLandCountTotal\"":\""2361\"",\""spiderCountTotal\"":\""51\"",\""goalCountTotal\"":\""7.000000\"",\""callGoalCountTotal\"":\""7.000000\"",\""callCountTotal\"":\""44.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.08\"",\""callConversionPerc\"":\""15.91\""}{\""metricDate\"":\""2013-05-05\"",\""pageCountTotal\"":\""20481\"",\""landCountTotal\"":\""8782\"",\""newLandCountTotal\"":\""6390\"",\""returnLandCountTotal\"":\""2392\"",\""spiderCountTotal\"":\""58\"",\""goalCountTotal\"":\""1.000000\"",\""callGoalCountTotal\"":\""1.000000\"",\""callCountTotal\"":\""8.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.01\"",\""callConversionPerc\"":\""12.50\""}{\""metricDate\"":\""2013-05-06\"",\""pageCountTotal\"":\""25175\"",\""landCountTotal\"":\""10019\"",\""newLandCountTotal\"":\""7082\"",\""returnLandCountTotal\"":\""2937\"",\""spiderCountTotal\"":\""62\"",\""goalCountTotal\"":\""24.000000\"",\""callGoalCountTotal\"":\""24.000000\"",\""callCountTotal\"":\""47.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.24\"",\""callConversionPerc\"":\""51.06\""}{\""metricDate\"":\""2013-05-07\"",\""pageCountTotal\"":\""35892\"",\""landCountTotal\"":\""12615\"",\""newLandCountTotal\"":\""8391\"",\""returnLandCountTotal\"":\""4224\"",\""spiderCountTotal\"":\""62\"",\""goalCountTotal\"":\""239.000000\"",\""callGoalCountTotal\"":\""239.000000\"",\""callCountTotal\"":\""321.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.89\"",\""callConversionPerc\"":\""74.45\""}{\""metricDate\"":\""2013-05-08\"",\""pageCountTotal\"":\""34106\"",\""landCountTotal\"":\""12391\"",\""newLandCountTotal\"":\""8389\"",\""returnLandCountTotal\"":\""4002\"",\""spiderCountTotal\"":\""90\"",\""goalCountTotal\"":\""221.000000\"",\""callGoalCountTotal\"":\""221.000000\"",\""callCountTotal\"":\""295.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.78\"",\""callConversionPerc\"":\""74.92\""}{\""metricDate\"":\""2013-05-09\"",\""pageCountTotal\"":\""32721\"",\""landCountTotal\"":\""12447\"",\""newLandCountTotal\"":\""8541\"",\""returnLandCountTotal\"":\""3906\"",\""spiderCountTotal\"":\""54\"",\""goalCountTotal\"":\""207.000000\"",\""callGoalCountTotal\"":\""207.000000\"",\""callCountTotal\"":\""280.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.66\"",\""callConversionPerc\"":\""73.93\""}{\""metricDate\"":\""2013-05-10\"",\""pageCountTotal\"":\""29724\"",\""landCountTotal\"":\""11616\"",\""newLandCountTotal\"":\""8063\"",\""returnLandCountTotal\"":\""3553\"",\""spiderCountTotal\"":\""139\"",\""goalCountTotal\"":\""207.000000\"",\""callGoalCountTotal\"":\""207.000000\"",\""callCountTotal\"":\""301.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.78\"",\""callConversionPerc\"":\""68.77\""}{\""metricDate\"":\""2013-05-11\"",\""pageCountTotal\"":\""22061\"",\""landCountTotal\"":\""9660\"",\""newLandCountTotal\"":\""6971\"",\""returnLandCountTotal\"":\""2689\"",\""spiderCountTotal\"":\""52\"",\""goalCountTotal\"":\""3.000000\"",\""callGoalCountTotal\"":\""3.000000\"",\""callCountTotal\"":\""40.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.03\"",\""callConversionPerc\"":\""7.50\""}{\""metricDate\"":\""2013-05-12\"",\""pageCountTotal\"":\""23341\"",\""landCountTotal\"":\""9935\"",\""newLandCountTotal\"":\""6960\"",\""returnLandCountTotal\"":\""2975\"",\""spiderCountTotal\"":\""45\"",\""goalCountTotal\"":\""0.000000\"",\""callGoalCountTotal\"":\""0.000000\"",\""callCountTotal\"":\""12.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""0.00\"",\""callConversionPerc\"":\""0.00\""}{\""metricDate\"":\""2013-05-13\"",\""pageCountTotal\"":\""36565\"",\""landCountTotal\"":\""13583\"",\""newLandCountTotal\"":\""9277\"",\""returnLandCountTotal\"":\""4306\"",\""spiderCountTotal\"":\""69\"",\""goalCountTotal\"":\""246.000000\"",\""callGoalCountTotal\"":\""246.000000\"",\""callCountTotal\"":\""324.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.81\"",\""callConversionPerc\"":\""75.93\""}{\""metricDate\"":\""2013-05-14\"",\""pageCountTotal\"":\""35260\"",\""landCountTotal\"":\""13797\"",\""newLandCountTotal\"":\""9375\"",\""returnLandCountTotal\"":\""4422\"",\""spiderCountTotal\"":\""59\"",\""goalCountTotal\"":\""212.000000\"",\""callGoalCountTotal\"":\""212.000000\"",\""callCountTotal\"":\""283.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.54\"",\""callConversionPerc\"":\""74.91\""}{\""metricDate\"":\""2013-05-15\"",\""pageCountTotal\"":\""35836\"",\""landCountTotal\"":\""13792\"",\""newLandCountTotal\"":\""9532\"",\""returnLandCountTotal\"":\""4260\"",\""spiderCountTotal\"":\""94\"",\""goalCountTotal\"":\""187.000000\"",\""callGoalCountTotal\"":\""187.000000\"",\""callCountTotal\"":\""258.000000\"",\""onlineGoalCountTotal\"":\""0.000000\"",\""conversionPerc\"":\""1.36\"",\""callConversionPerc\"":\""72.48\""}{\""metricDate\"":\""2013-05-


(I've truncated the output a little).

The R output has been read properly up until ""callConversionPerc"" and after that the JSON parsing seems to break. Is there some default parameter that I've missed that could couse this behaviour? I have checked for unmasked speechmarks and anything obvious like that I didn't see any.

Surely it wouldn't be the new line operator that occurs shortly after, would it?

**EDIT: So this does appear to be a new line issue.**

Here's another 'JSON' string I've  pulled into R, again the double quote marks are all escaped

    ""{\""modelId\"":\""7\"",\""igrp\"":\""1\"",\""modelName\"":\""Equally Weighted\"",\""modelType\"":\""spread\"",\""status\"":200,\""matchCriteria\"":\""\"",\""lookbackDays\"":90}\n{\""modelId\"":\""416\"",\""igrp\"":\""1\"",\""modelName\"":\""First and Last Click Weighted \"",\""modelType\"":\""spread\"",\""status\"":200,\""matchCriteria\"":\""\"",\""lookbackDays\"":90,\""firstWeight\"":3,\""lastWeight\"":3}\n{\""modelId\"":\""5\"",\""igrp\"":\""1\"",\""modelName\"":\""First Click\"",\""modelType\"":\""first\"",\""status\"":200,\""matchCriteria\"":\""\"",\""lookbackDays\"":90}\n{\""modelId\"":\""8\"",\""igrp\"":\""1\"",\""modelName\"":\""First Click Weighted\"",\""modelType\"":\""spread\"",\""status\"":200,\""matchCriteria\"":\""\"",\""lookbackDays\"":90,\""firstWeight\"":3}\n{\""modelId\"":\""128\"",\""igrp\"":\""1\"",\""modelName\"":\""First Click Weighted across PPC\"",\""modelType\"":\""spread\"",\""status\"":200,\""matchCriteria\"":\""\"",\""lookbackDays\"":90,\""firstWeight\"":3,\""channelsMode\"":\""include\"",\""channels\"":[5]}\n{\""modelId\"":\""6\"",\""igrp\"":\""1\"",\""modelName\"":\""Last Click\"",\""modelType\"":\""last\"",\""status\"":200,\""matchCriteria\"":\""\"",\""lookbackDays\"":90}\n{\""modelId\"":\""417\"",\""igrp\"":\""1\"",\""modelName\"":\""Last Click Weighted \"",\""modelType\"":\""spread\"",\""status\"":200,\""matchCriteria\"":\""\"",\""lookbackDays\"":90,\""lastWeight\"":3}\n\r\n""


When I try to parse this using `fromJSON` I get the same problem, it gets to the last term on the first line and then stop parsing properly. Note that in this new case the output is slightly different from before returning `NULL` for the last item (instead of the messy string from the previous example.

    $modelId
    [1] ""7""
    
    $igrp
    [1] ""1""
    
    $modelName
    [1] ""Equally Weighted""
    
    $modelType
    [1] ""spread""
    
    $status
    [1] 200
    
    $matchCriteria
    [1] """"
    
    $lookbackDays
    NULL

As you can see, the components now use the ""$"" convention as if they are naming components and the last item is null.

I am wondering if this is to do with the way that `fromJSON` is parsing the strings, and when it is asked to create a variable with the same name as a variable that already exists it then fails and just returns a string or a NULL.

I would have thought that dealing with that sort of case would be coded into RJSONIO as it's pretty standard for JSON data to have repeating names.

I'm stumped as to how to fix this.


I'll be very grateful if you can advise as to what I'm doing wrong! Is there some parameter I need to be specifying to get it to recognise variable names properly?

Cheers,

Simon",closed,1,RJSONIO not parsing correctly using fromJSON 
51,21121921,cboettig,2013-10-16T23:52:48Z,2013-10-16T23:53:09Z,"This issue is mostly note to myself in thinking out potential illustrative use cases.  no input really needed at this time, kinda trivial example here.  

What would eml look like to encode replicate models?  Can we easily convert metadata to additional column when combining data of matching column descriptions but differing metadata, e.g. 

---
model: Allen
parameters: r=1, K = 10, C = 5
nuisance parameters: sigma_g = 0.1
seed: 1234

---


value | density 
-------- | ------
0.0    |  0.12
0.1    |  0.14
0.2    |  0.22
0.3    |  0.4


And: 


---
model: Myers
parameters: r=1, K = 10, theta = 1
nuisance parameters: sigma_g = 0.1
seed: 1234

---


value | density 
-------- | ------
0.0    |  0.14
0.1    |  0.11
0.2    |  0.33
0.3    |  0.33

",open,0,potential use case in converting to long forms when combining data frames
50,21040002,cboettig,2013-10-15T19:51:05Z,2013-12-03T18:53:22Z,,closed,2,Example adding arbitrary semantics
49,21039978,cboettig,2013-10-15T19:50:40Z,2014-01-21T01:53:39Z,"Add a motivating example using attribute-level metadata. 

- [ ] Unit conversions
- [ ] Other ideas?  ",closed,1,Example using attribute level metadata
48,21039839,cboettig,2013-10-15T19:48:10Z,2014-01-21T01:40:51Z,"Taxonomic coverage, geographic coverage and temporal coverage are both common and rather essential metadata we should illustrate the use of. 

This should include tools to generate coverage nodes from columns of the data frame: species names, lat/longs to bounding boxes, time frame from series of times.  

Also include tools to summarize coverage metadata, including extraction from columns and extraction into a separate data.frame (or appropriate R spatial object). ",closed,4,"Coverage metadata: Examples writing, reading, and plotting"
47,19512546,cboettig,2013-09-15T00:47:23Z,2014-08-09T16:52:44Z,"An underlying philosophy of `reml` has been to map native R objects to EML structure (as opposed to either the raw XML or the S4 representations, which won't be familiar to most users).  While `data.frame` is the natural candidate, it doesn't include essential metadata such as units.  

I've taken a stab at extending the `data.frame` class as `data.set`, providing the additional attributes `unit.defs` and `col.defs`. See [data.set.R](https://github.com/ropensci/reml/tree/master/R/data.set.R).  This object should be able to be used wherever a `data.frame` can be applied, but we can also define additional methods that operate on this metadata. 

This also suggests an alternative way to define metadata of a data.frame in place of the current approach illustrated in the README.  I've added a function so that this `data.set` object can be created analogously to a data.frame: 

```coffee
dat = data.set(river = c(""SAC"",  ""SAC"",   ""AM""),
               spp   = c(""king"",  ""king"", ""ccho""),
               stg   = c(""smolt"", ""parr"", ""smolt""),
               ct    = c(293L,    410L,    210L),
               col.defs = c(""River site used for collection"",
                            ""Species common name"",
                            ""Life Stage"", 
                            ""count of live fish in traps""),
               unit.defs = list(c(SAC = ""The Sacramento River"", 
                                  AM = ""The American River""),
                                c(king = ""King Salmon"", 
                                  ccho = ""Coho Salmon""),
                                c(parr = ""third life stage"", 
                                  smolt = ""fourth life stage""),
                                ""number""))

```

an existing data.frame can also be passed in along with `col.defs` and `unit.defs`.  


#### Questions:

@duncantl @mbjones Is the implementation of the extension sensible?  `data.frame` is actually a rather confusing class -- cannot tell if it is S4 or S3 (e.g. `new('data.frame')` creates an S4 object, but `data.frame()` does not...), and has attributes like `names` and `row.names` which may or may not be S4 slots... e.g. they can be accessed by `slot()` but not `@`...).  [data.set.R](https://github.com/ropensci/reml/tree/master/R/data.set.R)


What other metadata do we want?  e.g. we could make full eml act like this, but that makes rather big data.frames...",closed,4,Extend data.frame to allow more metadata
46,19000480,cboettig,2013-09-04T20:39:42Z,2013-12-03T19:12:41Z,"In our early discussions about validation, we agreed it was really just part of the developer testing suite.  For a user consuming EML, having the software complain the file isn't valid isn't really helpful, it's best just to give it our best shot anyway.  For writing EML, since this is programmatically generated we can assure it is valid ... or can we?

The S4 R objects we use mimic the schema, but they don't enforce required vs optional slots (in fact, all slots are always 'present' in the S4 objects, so an operational definition of ""empty"" is that the slot has an empty S4 object (recursive) or a length 0 character/numeric/logical string.)  A user can create an S4 object and pass it into their EML file (seems like a useful/powerful option to have, particularly for reusing elements).  If the object is missing some required elements, this will create invalid EML.  

We can avoid this in several ways: 

- We could write a validation check as part of each S4 method.  Rather tedious, this also seems redundant with the schema validation check.  On the other hand,  this approach provides a nice warning earlier to the advanced user. 

- We could instead write constructor functions for each object. Also tedious, but allows clear indication of optional and required parameters and can be easier to use than the `new` constructor.  This is the strategy we employ so far, but we still permit pre-built S4 nodes to be passed to some constructors to facilitate reuse (but bypassing the protection regarding required elements).  

-  Run the validator by default on calls to write_eml (would require an internet connection or packaging the schema).  If we we check only by validating the final EML file, the user may be at some trouble to find just what they need to change.  On the other hand, it is perhaps the surest way to guarantee validity.  

",closed,3,Workflow to ensure users write valid EML?
45,18985513,emhart,2013-09-04T17:34:51Z,2014-06-29T19:26:41Z,Some missing commas prevented the package from being built.,closed,0,Fixed issue #44
44,18985372,emhart,2013-09-04T17:32:19Z,2013-09-04T18:26:28Z,"> install_github(""reml"", ""ropensci"")
Installing github repo(s) reml/master from ropensci
Downloading reml.zip from https://github.com/ropensci/reml/archive/master.zip
Installing package from C:\Users\thart\AppData\Local\Temp\RtmpuCh7ZJ/reml.zip
Installing reml
""C:/PROGRA~1/R/R-30~1.1/bin/i386/R"" --vanilla CMD INSTALL  \
  ""C:\Users\thart\AppData\Local\Temp\RtmpuCh7ZJ\reml-master""  \
  --library=""C:/Users/thart/Documents/R/win-library/3.0"" --with-keep.source --install-tests 

* installing *source* package 'reml' ...
** R
Error in parse(outFile) : 
  C:/Users/thart/AppData/Local/Temp/RtmpuCh7ZJ/reml-master/R/attribute.R:213:52: unexpected 'function'
212:  
213: setMethod(""extract"", signature(""measurementScale"") function
                                                       ^
ERROR: unable to collate and parse R files for package 'reml'
* removing 'C:/Users/thart/Documents/R/win-library/3.0/reml'",closed,6,Can't build package due to some missing commas.
43,18918592,cboettig,2013-09-03T16:31:55Z,2013-09-04T03:33:43Z,"@mbjones would it make sense to timestamp the EML file with date it is generated?  The files have most of the information you might want to cite the data: creator, title, url or identifier, potentially organization or repository responsible, but I don't see a date associated with this.  If so, where would the logical place for such a date go?  (presumably this would not be ambiguous to the date data was actually collected, e.g. `temporalCoverage`)...

",closed,3,if/where to date / timestamp EML files? 
42,18910208,cboettig,2013-09-03T14:09:29Z,2013-09-06T19:58:48Z,"@duncantl For some reason I cannot get your `ext_validate` to run successfully any more.  e.g. running [this test](https://github.com/ropensci/reml/blob/ceb8bf9d81990c2c591f5dfa7a1d47fc67927503/inst/tests/test_ext_validate.R) gives the error: 


```
""): non-character argument
1: eml_validate(txt) at test_ext_validate.R:17
2: .reader(ans) at /home/cboettig/Documents/code/reml/R/ext_validate.R:59
3: strsplit(ans, "": "") at /home/cboettig/Documents/code/reml/R/ext_validate.R:76
```

not sure what's up on this one.  
",closed,1,ext_validate broken?
41,18799969,cboettig,2013-08-30T17:25:32Z,2013-09-03T19:30:16Z,@mbjones Can you clarify where any of the elements inherit some of their definition from existing definitions?  e.g. I think that's what is going on with `entityGroup` and `referenceGroup`.  Wondering if there is also a base class inherited by most everything that defines the `id` attribute?  Or is that just manually added to each definition where appropriate? ,closed,5,Understanding inheritance in the schema
40,17924979,cboettig,2013-08-12T05:41:44Z,2013-09-06T19:59:17Z,"_Here's a running list of questions I have for @duncantl or whomever, largely arising as I try to understand the S4 based approach to representing the schema and various puzzles that arise in the process.  _

### generic R isses

- [ ]  Why do I lose names when coercing between (named) character strings and lists (and vice versa)? (try `as(list(a=1, b=2), ""character"")`)
 
- [ ]  I set prototype for a slot, install package, remove prototype for that slot, reinstall package, prototype is still set! Huh? (Deleting the installed package location and the local namespace and then re-roxygening and installing seems to fix this).  
 
- [ ]  What's up with ""labels"" on factors? They overwrite my levels and everything.  

### XML and schema issues

- [ ]  Best way to handle optional elements (when we'd rather not specify default values)?  Ideally would have something such that `addChildren(parent, class@empty_option)` would not do anything when the slot was truly empty.  
 
- [x]  ""One or more"", ""Zero or more"" everything needs a `ListOf` class.  Goodness, but this is annoying.  

> yup, tedious but mindless.

- [x]  Related: In the schema, sometimes a list of elements has a parent node, e.g. `<attributeList>`, followed by `<attribute>` `<attribute>` , ... sometimes it doesn't.  Does it make sense to write an extra object class associated with the first case (e.g. class for attributeList?)?  

> yup, classes for all elements, and more classes for ListOf
 
- [x]  Stategies for file organization when defining S4 objects?  Do I need to define the elementary types first?  How do I avoid all the ""class not defined"" errors on installing package?

Set the `collate` order for the files, describing which order they should be loaded in (e.g. class basic class definitions before richer ones, classes before methods).  In Roxygen, the order is set by using `@include fileA.R` ton the documentation of `fileB.R` to indicate `fileA` has definitions needed for `fileB`.  
 
- [x]  Attributes: A bit annoying that the S4 representation we're using uses slots for attributes as well as node/values without indication of which is which, but I guess it's okay.  We could map them in S4 to a attr slot..

> Yup.  Tedious again but not problematic.  Writing to/from methods takes care of this explicitly.  
 
- [x]  Do we write into XML as a coercion method, `setAs(""class"", ""XMLInternalNode"", function(from){...` or with some other kind of function?  

> sure, though sometimes preferable to define as a method, allowing us to make use of `callNextMethod` to convert against the inherited slots.  

- [ ]  How do we handle coercion of S4 objects into S3 objects (and vice versa)?  Can we make S4 objects work in S3 functions?  What's the deal with `setOldClass` and `S3part`?  

- [x]  XML Schema (XSD I guess I could say) has notion of sharing a bunch of slots using entity-groups.  I guess we just put all members of the group in as seperate slots each time? (Simple enough when XMLSchema is generating the class definitions I guess).  

> Answer: Just use `contains` in the `setClass` definition (Inheritance)

- [ ]  Strategies for mixing XMLSchema generated class definitions and manual class definitions of schema objects?  (namespaces for classes)

- [x]  Coercion to promote types or better to explicitly call new?  

> Answer: Using ""new"", we must know the slot name corresponding to the type.  Coercion allows us to specify the type, e.g.

```
setAs(""eml:nominal"", ""eml:measurementScale"", function(from) new(""eml:measurementScale"", nominal = from))
setAs(""eml:ordinal"", ""eml:measurementScale"", function(from) new(""eml:measurementScale"", ordinal = from))
```

can be used with `as(from[[3]], from[[4]])`, reading the class name from a varaible instead of hardwiring the slot name. The coercion methods take care of mapping the class names to the appropriate slot names.  


- [x]  Common attributes for all nodes an easy way? For example, character / numeric types, in the schema can all have id attributes.  I suppose programmatic solution is to make every node a class? would we have `setClass('somenode', representation(title=""eml:title""...` and then `setClass(""eml:title"", representation(title = ""character"", id = ""character"")`?  

> How about just having all inherit from a common base?",closed,0,R Puzzles 
39,17588600,cboettig,2013-08-03T00:44:31Z,2013-08-29T00:36:19Z,"Currently, to convert a `data.frame` into EML, we use a workflow that passes a data.frame, a list of `column_metadata` and a list of `unit_metadata` to a function,

```
  dat = data.frame(river=factor(c(""SAC"", ""SAC"", ""AM"")),
                        spp = factor(c(""king"", ""king"", ""ccho"")),
                        stg = factor(c(""smolt"", ""parr"", ""smolt"")),
                        ct =  c(293L, 410L, 210L))
  col_metadata = c(river = ""http://dbpedia.org/ontology/River"",
                   spp = ""http://dbpedia.org/ontology/Species"",
                   stg = ""Life history stage"",
                   ct = ""count of number of fish"")
  unit_metadata =
     list(river = c(SAC = ""The Sacramento River"", AM = ""The American River""),
          spp = c(king = ""King Salmon"", ccho = ""Coho Salmon""),
          stg = c(parr = ""third life stage"", smolt = ""fourth life stage""),
          ct = ""number"")
```

Then eml is created by passing these objects to the high-level function `eml_write`

```
  doc <- eml_write(dat, col_metadata, unit_metadata)
```

I'm not sure if this is a good way to ask the users for metadata.  One of the design goals is to reuse the natural R structures as much as possible and avoid asking for redundant information.  

One problem with this is that it structures the metadata by column headings, rather than column by column, which might suggest something like this: 


```
metadata <- 
  list(""river"" = list(""River site used for collection"",
                      c(SAC = ""The Sacramento River"", AM = ""The American River"")),
       ""spp"" = list(""Species common name"", 
                    c(king = ""King Salmon"", ccho = ""Coho Salmon"")),
       ""stg"" = list(""Life Stage"", 
                    c(parr = ""third life stage"", smolt = ""fourth life stage"")),
       ""ct""  = list(""count"", 
                    ""number"")
```

Which provides a more column by column approach.  Still, this seems unsatisfactory, as we don't reuse the `levels` of a factor in a column (e.g. `SAC` and `AM`), instead requiring they be rewritten; likewise we still have to repeat the column headings in our named list.  

Rather than using a named list, we might also do better to capture the attribute metadata in the object, e.g. 

```
river_metadata <- list(""river"",
       ""River site used for collection"",
       c(SAC = ""The Sacramento River"", AM = ""The American River""))
```

which maps better to the schema `attribute`.  Still none of these make maximum re-use of the data.frame objects and all are a bit cumbersome.  

A more natural solution would be to write directly into the S4 slots, but I'm not clear on how this would would work.  Using the above structures we could do 

```
as(""eml:attributeList"", metadata)
```

and a more low-level option:

```
as(""eml:attribute"", river_metadata)
```


but not sure if that would feel more natural to users than the function calls (particularly since most R using ecologists are not familiar with S4 methods).

@SChamberlain @karthikram @mbjones @duncantl 
Would love any feedback on this or generally how the API should look to specify these values.  Can we attach them to the data.frame/columns more directly, and is that better? (e.g. I considered `labels` option for factors, but that just overwrites the `levels`).... ",closed,1,Formats for user entry of dataTable metadata
38,17228529,cboettig,2013-07-25T18:52:22Z,2013-09-03T01:47:13Z,"A somewhat more elegant approach to reading in XML is to define an S4 class for a given node and then just cast the XML into the S4 slots using `xmlToS4`.  Undefined slots are ignored.  I provide an illustration of how to do this in my [advice on the RNeXML package](https://github.com/shumelchyk/RNeXML/issues/11), which compares it to alternative methods of reading XML.  

This approach has the particular advantage that, at least in principle, we shouldn't need to define these classes by hand the way I show there, since their definitions can be extracted programmatically from the schema.  The `XMLSchema` package should soon be able to do this.  

Not only does this streamline our approach to reading in the EML into R objects, but it provides several other benefits.  We can define coercion methods that take each of these S4 objects and coerce them into the appropriate R objects.  For instance `<dataTable>` node into an R `data.frame`, along with appropriate metadata available (in S4, since there is not a natural way to attach metadata to R objects...), or `<person>` node into an R `person`, etc.

Somewhat more powerful and potentially more tricky is using the S4 approach as a write method.  Rather than constructing the XML node by node as we do currently with `newXMLNode` and `addChildren`, etc, we would simply coerce our R objects (`data.frames`, `person` or strings, R `DESCRIPTION` files of (R) software, etc etc) into these S4 class definitions we extracted from the schema (which can be done automatically by matching slot names if the matches are good enough? e.g. `person$givenName` to `<person><givenName>`?).  With luck(?), xmlSchema will be able to use the schema to figure out how to write this S4 object into XML (e.g. which slots are encoded as attributes, which as child nodes, ordering of slots, etc).  

As XMLSchema is probably not up to this task yet (particularly on the `write` end?), we may do well to continue as we are ""by hand""; though perhaps we should still be leveraging the S4 class definition in the process (and then manually turning it to XML with the calls to `newXMLNode`, etc...?)

@duncantl will hopefully clarify some of these questions and anything I've misstated about this strategy.  ",closed,6,Use the S4 approach for reading and writing XML
37,17015468,cboettig,2013-07-21T06:21:41Z,2013-10-15T17:36:29Z,"- [x] eml_dataTable should use title for name of csv file if not provided, rather than the col-name trick.  
- [x] reml method node should include dateTime of generation
- [x] Add a publication date
- [x] add a License
- [x] eml_person should be able to take strings such as `Carl Boettiger <cboettig@gmail.com>` and coerce to R person object, then to eml_person object.  
- [x] eml_publish should be able to take a return object from eml_write, (or perhaps take the eml_write arguments directly?)
",closed,1,basic eml_write tasks
36,17008587,cboettig,2013-07-20T16:57:58Z,2013-07-21T22:51:55Z,@mbjones `eml_write` is assigning the namespace 2.1.0.  Any reason we shouldn't be using 2.1.1?,closed,1,Use EML 2.1.1 ?
35,17008547,mbjones,2013-07-20T16:54:00Z,2013-09-04T03:40:03Z,"Currently, in the `dataone` package, we can download EML and parse it with an XML parser to extract some metadata for use in our script.  For example:

```
library(dataone)
cli <- D1Client()

# Download and parse some EML metadata
obj4 <- getMember(pkg, ""doi:10.5063/AA/nceas.982.3"")
getFormatId(obj4)
metadata <- xmlParse(getData(obj4))

# Extract and print a list of all attribute names in the metadata
attList <- sapply(getNodeSet(metadata, ""//attributeName""), xmlValue)
attList
```

This seems like it would be better handled by handing the eml document off to the `reml` package for parsing, which might provide some nicer accessor methods, plus the ability to insert new metadata or change existing fields.  I've been thinking about how its best to do this.  @cboettig Should the `dataone` package load `reml` to do its parsing, or should the `reml` package load `dataone` to handle its downloading.  I'm thinking of this in terms of other metadata standards as well, such as FGDC or ISO 19115, and wanting to support those through the `dataone` library as well.  Thoughts?",closed,1,Use reml to parse EML metadata coming from the dataone library
34,16986072,cboettig,2013-07-19T18:43:59Z,2014-07-23T17:27:33Z,"We should be able to do 

```ruby
nex <- readSchema(""eml.xsd"")
defineClasses(nex)

doc <- xmlParse(""my_eml_data.xml"")
fromXML(doc)
```

@duncantl is adjusting `XMLSchema` to handle EML's schema for this (some challenges in the recursive referencing of schema files).  

@mbjones Do we have an online URI for `eml.xsd`?  So far I've had to download a tarball from http://knb.ecoinformatics.org/software/download.jsp#eml


",closed,1,Get XMLSchema running on EML
33,16506998,cboettig,2013-07-09T04:09:23Z,2013-12-03T19:02:43Z,"Thus far issues have been divided between read, write, and publish, or integrate EML. Development has mostly focused on writing EML.  Publish is relatively straightforward extension of writing the EML, just adds a few extra fields to the EML file and pushes the data to the appropriate repository with appropriate metadata.    

Reading EML is potentially more of a challenge since (a) we assume the user doesn't know xpath and (b) want to provide conversion into native R objects wherever possible.  We have only the basic proof-of-principle based on the trivial write-EML example which imports the csv file into an appropriately labeled data.frame.  

Not sure if searching across EML is a read-eml issue or a separate task, since in general such a query might be posed across a database of EML files rather than a single XML file.  

To have a focal example, I'll just borrow one posed by one of my PIs: 

> ""Find all data that involves a morphometric measurement of an invertebrate species at 3 or more geographically distinct locations. 

(e.g. 3+ different populations of the same species) This kind of data would be useful for all sorts of within-species variation comparisons (when put against environmental variables, etc), but is remarkably difficult to find, as vertically integrated databases tend to omit morphological data (like most GBIF entries), or else aggregate at the species level, discarding the geographic data.  Many papers have less than three populations, and it is all but impossible to find another paper that makes the same morphometric measurements on the same species at a unique location.  

It seems like this is the kind of query we could construct in EML; and in particular perform the aggregation step.  But that assumes a model in which we query directly against all available EML files. I'm not sure if that is sensible or if there's a more clever way to do these queries. (particularly as we would have to do some computation in the process - e.g. to isolate data with invertebrate coverage we would have to query the taxonomic coverage and then query against ITIS or something to determine if the species etc listed was an invertebrate)  @mbjones is there a better way to think about complex queries (metacat?)?
",closed,4,EML search queries
32,16438443,cboettig,2013-07-07T03:12:02Z,2013-07-25T18:57:14Z,"An EML software node needs:

- [x] version (txt)
- [x] licenseURL or license (text)
- [x] implementation

Optionally, 

- [x] dependencies

`<implementation>` minimally needs a url, via 

- [x] a  `<distribution>` node, which we also use elsewhere (e.g. when publishing data to a url).  


For R packages, we can extract all the necessary information by providing the R package name.  A wrapper function can use eml_software and the package name to create this (using the `packageDescription` function)

Perhaps there is no need to provide the optional R software entries, (e.g. all the optional fields under implementation) since such data is already programmatically available knowing the package distribution URL...  
",closed,4,function to create an eml_software node
31,16435606,cboettig,2013-07-06T21:11:31Z,2013-09-05T21:51:35Z,"For a first-time user of the reml package, it might be easier to simply call `eml_write` on an R `data.frame` object and have the function coach them through what minimal metadata they must add, prompting them for inputs along the way (""Define column ""X1:""  ).  


No doubt any regular user would find this frustrating to repeat each time, and would rather provide a list of metadata ahead-of-time, possibly generated programmatically (e.g. pulled from an existing file in the same format) or specified in a configuration file (e.g. no need to ask me my name every time).  ",closed,3,Provide prompts / wizard to help user enter metadata if not provided
30,16245761,mbjones,2013-07-02T02:40:01Z,2013-10-15T17:53:15Z,"The R dataone package also has some preliminary EML parsing routines, which extract relevant metadata from EML and make it available for use in the dataone client.  This is partially used for the asDataFrame() method that converts a dataone binary file to a data frame.  These classes may be able to be replaced with more capable reml package methods.  See:

https://repository.dataone.org/software/cicore/trunk/itk/d1_client_r/dataone/R/EMLParser-class.R
https://repository.dataone.org/software/cicore/trunk/itk/d1_client_r/dataone/R/EMLParser-methods.R
",closed,1,can we replace the EMLParser class from dataone R package?
29,16243095,cboettig,2013-07-02T00:41:33Z,2014-07-29T00:40:57Z,"R's read.table() function (which read.csv is aliased to) provides lots of options that we should 

* [ ] be encoded in the metadata when we write csv
* [ ] be reading in from the metadata when we read csv


```ruby
 read.table(file, header = FALSE, sep = """", quote = ""\""'"",
                dec = ""."", row.names, col.names,
                as.is = !stringsAsFactors,
                na.strings = ""NA"", colClasses = NA, nrows = -1,
                skip = 0, check.names = TRUE, fill = !blank.lines.skip,
                strip.white = FALSE, blank.lines.skip = TRUE,
                comment.char = ""#"",
                allowEscapes = FALSE, flush = FALSE,
                stringsAsFactors = default.stringsAsFactors(),
                fileEncoding = """", encoding = ""unknown"", text)
```

See `?read.table` for details.  Particularly important for the read interface.  ",open,0,Reading and writing CSV files should have full implementation / translation of read.table API
28,16185573,mbjones,2013-06-30T09:07:30Z,2013-06-30T09:41:29Z,"The currently generated EML is not valid and needs to be fixed.  I have identified the following issues to be fixed:

- [x] missing `@packageId` attribute on root `<eml>` element
- [x] missing `@system` attribute on root `<eml>` element
- [x] missing `<title>` field
- [x] missing `<creator>` field
- [x] missing text values in `<contact>` field
- [x] `<entityDescription>` field is empty
- [x] misspelled `<recorDelimiter>`, should be `<recordDelimiter>`
- [x] `<numericDomain>` is out of order and should follow `<unit>`
",closed,1,fix validity issues with generated EML
27,16179898,cboettig,2013-06-29T20:41:43Z,2014-01-20T23:55:03Z,"We can get citation information for R packages with `citation(""reml"")`, so it would be natural to get the citation information for an EML object with:

```ruby
eml <- read_eml(""my_eml_file.xml"")
citation(eml)
```

- [x] `read_eml` should make use of the S3 class `eml` and return a pointer to the XML root node as `doc`
- [ ] Create the function `eml_citation` with alias `citation.eml` that extracts the appropriate data citation.  
- [ ] Citation metadata should include DOI if published to figshare or dryad, etc. 
- [x] Use the `bibentry` R class, so that citation can be returned in various formats (e.g. `print(citation(doc), style='bibtex')`)

",closed,29,eml_citation
26,16178234,mbjones,2013-06-29T18:25:26Z,2013-07-02T20:19:47Z,"Immediately after cloning the reml repo on a Mac, git status shows the man/eml_datatable.Rd file as modified, without any editing.  

This seems to be due to the existence of files that differ only by case -- particularly man/eml_datatable.Rd and man/eml_dataTable.Rd.  Removing the duplicate file should fix the problem, but at the moment I am unclear as to which is the right one to remove, or if in fact both are needed.

",closed,2,rename files that only differ by case
25,16177815,mbjones,2013-06-29T17:51:10Z,2014-06-12T21:26:39Z,Include eml_figshare.R in the DESCRIPTION to allow the package to build.,closed,0,Include eml_figshare.R in Collate
24,16171248,cboettig,2013-06-29T05:40:24Z,2014-07-29T17:34:06Z,"- [ ] Could add figshare as a `<publisher>` node when using `eml_figshare`
- [ ] Should re-use a general-purpose function for adding publisher to data
- [ ] Should check that publisher node isn't already set to avoid duplicating entries.  ",open,0,add appropriate publisher information when pushing to figshare
23,16171173,cboettig,2013-06-29T05:30:05Z,2014-01-22T01:48:42Z,,open,5,Add DOI to EML when publishing publicly to figshare 
22,16163481,cboettig,2013-06-28T21:49:40Z,2013-07-25T18:55:17Z,"That way if someone doesn't like the EML, they know who to blame ;-)

- [ ] e.g. should have plain-text description of REML, contact info & bug report info.  Have to figure out the best syntax for this.  
- [x] A richer implementation could document the R function calls used to generate the EML.  
- [ ] Include citation to REML (e.g. as a [software node](http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-software.html) and/or [literature node](http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-literature.html))
",closed,22,reml-generated EML should include metadata stating so
21,16157933,cboettig,2013-06-28T19:38:28Z,2013-09-03T05:04:18Z,"eml_read and eml_write both assume online endpoints for all files.  

- [ ] Need to see how to provide option to use/support local destinations at custom file paths (perhaps just physical filename of EML file and datafiles).  

- [ ] Add option/toggle on eml_read to attempt local access (with specified path?)

- [ ] eml_write needs to include it's own filename in the EML(?)

- [x] eml_write probably shouldn't write a `<distribution><online>` node.  That should be added by `eml_publish`.  ",closed,1,Work with local files instead of online files
20,16157794,cboettig,2013-06-28T19:35:18Z,2015-02-02T22:26:16Z,"Presumably we should be able to push directly to KNB as well (many benefits, including being a DataONE node...).  

May need Matt's help on getting API tools to do this...",open,20,Add KNB as a publish node?
19,16157664,cboettig,2013-06-28T19:32:19Z,2013-09-03T05:08:00Z,"So far `eml_read` only extracts the three objects in the proof-of-principle test.  Of course we will want generic access to all metadata objects, probably with a variety of tools for their extraction. 

- [x] In particular, we still need utilities for `<coverage>`, see #9 
- [ ] In general, we may want to leverage `xmlSchema` and `xmlToS4` to provide generic access to metadata.  
- [x] As well as plain-text summaries, see #1

",closed,1,Extract all additional metadata provided
18,16157589,sckott,2013-06-28T19:30:40Z,2013-07-02T20:20:07Z,"```coffee
install_github(""reml"", ""ropensci"")

Quitting from lines 62-63 (vingette.Rmd) 
Error: processing vignette 'vingette.Rmd' failed with diagnostics:
argument "".contact"" is missing, with no default
Execution halted
Error: Command failed (1)
```",closed,1,Install fails due to bug while compiling vignette
17,16156269,cboettig,2013-06-28T18:59:45Z,2014-09-29T16:46:00Z,"I think that the `dateTime` of a single observation should always be given in a single column. For reasons unfathomable to me, [some data](http://harvardforest.fas.harvard.edu:8080/exist/xquery/data.xq?id=hf205) represents year as a column, month as another column, day as another column, etc.

This is really only a problem when we do not have good metadata to recognize that these all refer to the same observation.  For instance, in the dataset linked above, we can tell that all the columns are ""dateTime"" objects, but we have generally no way to be sure that the ""year"" in column 2 is the year that corresponds to the ""day"" in column 3.  These could be independent dateTime observations, such as the start time and end time of a study, etc.

While it seems obvious that a single observation should get a single cell, apparently it isn't.  I'm open to ideas on how to approach these issues. 

This is a problem for `read_eml` for two reasons:

1. If we are to render any of these as time (`POSIXt` class) objects, we need to be able to associate them. A crude `as.POSIXt` would instead render the date as the current year, rather than that given in the column.  

2. Presumably a researcher would like to access the individual points in time, so ideally we would provide a combined column as date-time format, even if we return the original columns unformatted.  

### Questions

- Perhaps in such cases we leave the objects as character strings and go on? 

- When can we safely convert a dateTime object from a character string to a time object, and what class of time object should we use?  e.g. POSIXt, or objects from the [chron](http://cran.r-project.org/web/packages/chron/index.html) or [date](http://cran.r-project.org/web/packages/date/) packages?

",open,1,Dealing with dateTimes: when dates are defined over multiple columns
16,16120845,cboettig,2013-06-27T23:51:27Z,2013-07-07T00:48:49Z,"http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-dataTable.html#numberOfRecords

Optional value providing non-header lines in CSV.  Can simply be pulled from `dim(dataframe)[1]`",closed,1,add numberOfRecords node to dataTable 
15,16109705,cboettig,2013-06-27T19:29:27Z,2014-07-29T17:41:06Z,"With a robust XMLSchema package, a few other things become easy.  Integration should happen at the more universal level of the schemas themselves rather than the R level, just hopefully something we can take advantage of from there.  

* [Darwin Core](http://rs.tdwg.org/dwc/terms/simple/index.htm),  [schema](http://rs.tdwg.org/dwc/xsd/tdwg_dwcterms.xsd)

* [NeXML](http://www.nexml.org/)

* ...

EML already maps to [Biological Data Profile](http://www.fgdc.gov/standards/projects/FGDC-standards-projects/metadata/biometadata), BDP, from the Federal Geographic Data Committee (used by the now-defunct National Biological Information Ifrastructure, NBII).  But reverse mapping is not available ([Jones & Co, 2006](http://10.1146/annurev.ecolsys.37.091305.110031, ""The New Bioinformatics: Integrating Ecological Data from the Gene to the Biosphere"")).",open,5,Integration with related schemas?
14,16106524,cboettig,2013-06-27T18:24:55Z,2013-07-02T20:20:07Z,"- Example EML file parses (as XML) 

coverage: `eml_write`, `eml_dataset`, `eml_dataTable`, `eml_attributeList`

- Example EML file validates (as EML)

coverage: `eml_write`, `eml_dataset`, `eml_dataTable`, `eml_attributeList`

- Check certain entries in EML, e.g. `<attribute><definition>` from xpath matches definition passed to `eml_write`.


Continue developing unit tests as (or before) development proceeds.  
",closed,1,Start a unit test suite
13,16105478,cboettig,2013-06-27T18:04:49Z,2013-07-25T21:20:43Z,"Truly automatic data integration needs some level of formal semantics.  Should start thinking about how semantics would fit into the `reml` workflow, even though most are still in their infancy.  

An ideal system would allow authors to contribute to existing ontologies, or at least push to a 'working' or 'draft' ontology that could later be formalized / mapped to a more central effort like [OBOE](https://semtools.ecoinformatics.org/oboe)

Not sure if we yet have any R-based tools for semantic reasoning, etc.  (Though we do have [SPARQL](http://cran.r-project.org/web/packages/SPARQL/)).  Ultimately this might require a separate repository to tackle implementation and reasoning of semantic terms.  (Hopefully developed by some actual domain experts in the R community).  

",closed,1,Semantics integration
12,16102158,cboettig,2013-06-27T17:07:15Z,2014-07-30T23:05:14Z,"Currently units must come from the [EML Standard Units](http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-unitTypeDefinitions.html#StandardUnitDictionary) list, already written according to the specification there (camelCase and all).  

- [ ] We should provide a utility to match plain text descriptions to the Standard Units.  
- [ ] We should provide support for specifying custom units

[customUnits](http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-attribute.html#customUnit) must be completely defined in [STMML syntax](http://www.ch.ic.ac.uk/rzepa/codata2/), making them a bit more onerous to use than custom string types.  ",closed,22,"attributeList needs to support custom units, unit matching"
11,16101484,cboettig,2013-06-27T16:57:07Z,2013-09-03T05:11:37Z,"The package makes frequent use of camelCase.  This is to provide consistent mapping to EML nodes, which are all defined in camelCase, e.g. `<dataset>` but `<dataTable>`.  Deal with it.

Package functions use `=` instead of `<-` for assignment.  Should be fixed.  XML package has lots of nice syntactic sugar, and this makes it a bit more fluid to move the definitions around.  

Need to stick with consistent and transparent use of `addChildren` vs, ... `parent=`, etc.  ",closed,2,Stylistic issues
10,16100751,cboettig,2013-06-27T16:43:18Z,2013-11-12T21:40:37Z,"Some metadata a user would probably rather set once in some global configuration than have to specify each time, such as their personal contact information.  The package API currently uses `eml$set` and `eml$get` to handle this.  

At the same time, if a user needs to adjust the contact information for a particular file, they should be able override these values without altering their global configuration.  

Need to be careful to avoid collisions in the `eml$set` approach.  As implemented, it won't support structuring metadata (e.g. contact_givenName, contact_surName, ...).  Ultimately we might want to be more clever about this, or just go to the yaml approach entirely.  

Need also to be careful in avoiding lengthy and fragile function APIs.  `eml$set` helps with this, as the function can get the data it needs without passing down through many levels, but also makes the override issue harder.  

Once we have more implementation examples, we can give this some hard thought.  Meanwhile: 


### Potential methods of providing metadata

- specify metadata in R, e.g. `eml$set(contact_email = ""cboettig@ropensci.org"")`

- Pull in generic metadata from an external file, e.g. YAML:

```yaml
contact:
  email: cboettig@ropensci.org
```

- Pull in generic metadata information from a DOI or a website -- e.g. rather than having to enter all authors manually.  (FundRef as well as CrossRef data...?)

",closed,5,Dealing with settings / generic metadata
9,16100085,cboettig,2013-06-27T16:29:23Z,2013-09-04T18:02:13Z,"EML [coverage](http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-coverage.html) nodes specify taxanomic, geographic, and temporal coverage.  

They can refer to a dataset node but can also be used to define coverage of individual columns (e.g. a species column) or individual cells in a column (e.g. the species name).  The latter is much richer but less commonly implemented.  

### taxonomic coverage

[see eml taxa documentation](http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-coverage.html#TaxonomicClassificationType)

@SChamberlain  I think ideally taxonomic coverage would make use of `taxize_` to help identify and correct species names.  While higher taxonomic information can be specified, this would probably best be reserved for cases not referring to a particular species, since (a) we can already programmatically recover the rest of the classification given the genus and species, and (b) higher taxonomy may be inconsistent anyway.   

### temporal coverage

[See eml temporal coverage documentation](http://knb.ecoinformatics.org/software/eml/eml-2.1.1/eml-coverage.html#TemporalCoverage)  

We'll want to automatically decide if the coverage is a specific range of calendar dates, an estimated timescale (geological timescale), approximate uncertainty, and whether to include any citations to literature describing dating method (e.g. carbon dating).  Could be a whole wizard / module.... 

Meanwhile, just supporting manual definition of this structure would be a good start.  

### geographic coverage 

Can be bounding box, polygon, or geographicDescription (e.g. ""Oregon"").  Tempting to process natural language descriptions into coordinates, but that throws out true data in place of estimated data (e.g. best left to read-eml world, not the write-eml).  


",closed,4,utilities for coverage metadata
8,16044852,cboettig,2013-06-26T17:15:16Z,2014-07-29T17:34:23Z,"This is the __holy grail__ of metadata infrastructure and ostensibly the primary purpose of EML, see [Jones et al 2006].  Despite that, integration is not actually possible without semantic definitions as well, see [Michener & Jones 2012], from which we adapt this minimal example below.  

This example provides minimal and sometimes missing semantics; which may make it unresolvable.  A complete semantic solution is diagrammed in the figure from  [Michener & Jones 2012].  


### Dataset 1 

```ruby
 dat = data.frame(river=c(""SAC"", ""SAC"", ""AM""), 
                   spp = c(""king"", ""king"", ""ccho""), 
                   stg = c(""smolt"", ""parr"", ""smolt""),
                   ct =  c(293L, 410L, 210L))

 col_metadata = c(river = ""http://dbpedia.org/ontology/River"",
                  spp = ""http://dbpedia.org/ontology/Species"", 
                  stg = ""Life history stage"",
                  ct = ""count"")

 unit_metadata = 
  list(river = c(SAC = ""The Sacramento River"", AM = ""The American River""),
       spp = c(king = ""King Salmon"", ccho = ""Coho Salmon""),
       stg = c(parr = ""third life stage"", smolt = ""fourth life stage""),
       ct = ""number"")

```

### Dataset 2

```ruby
 dat = data.frame(site = c(""SAC"", ""AM"", ""AM""), 
                   species = c(""Chinook"", ""Chinook"", ""Silver""), 
                   smct = c(245L, 511L, 199L),
                   pcnt =  c(290L, 408L, 212L))

 col_metadata = c(site = ""http://dbpedia.org/ontology/River"",
                  species = ""http://dbpedia.org/ontology/Species"", 
                  smct = ""Smolt count"",
                  pcnt = ""Parr count"")

 unit_metadata = 
  list(river = c(SAC = ""The Sacramento River"", AM = ""The American River""),
       spp = c(Chinook = ""King Salmon"", Silver = ""Coho Salmon""),
       smct = ""number"",
       pcnt = ""number"")

```

### Figure 

![ontology_synthesis2](https://f.cloud.github.com/assets/222586/710800/c0d567b6-de83-11e2-94b1-52090b0c9a5f.png)




[Jones et al 2006]: http://dx.doi.org/10.1146/annurev.ecolsys.37.091305.110031 ""The New Bioinformatics: Integrating Ecological Data from the Gene to the Biosphere in Annual Review of Ecology, Evolution, and Systematics""
 
[Michener & Jones 2012]: http://dx.doi.org/10.1016/j.tree.2011.11.016 ""Ecoinformatics: supporting ecology as a data-intensive science; in Trends in Ecology & Evolution. doi: 10.1016/j.tree.2011.11.016""
",open,7,Integrate two datatables based on EML spec and ontology
7,16043849,cboettig,2013-06-26T16:54:48Z,2014-01-02T06:22:08Z,"- [ ] Read in an EML file into R.  
- [ ] Validate against the schema with appropriate warnings.  
- [ ] Extract R objects for data types (see #6 )
- [ ] Potentially generate the appropriate class and function types from XMLSchema",closed,23,Parse and Validate EML against schema
6,16043720,cboettig,2013-06-26T16:52:38Z,2013-07-01T01:39:07Z,"Given the EML file defining a CSV and metadata types, extract the R object information.  This should allow the user to reconstruct the following R objects from EML generated by #2 


```ruby
 dat = data.frame(river=c(""SAC"", ""SAC"", ""AM""), 
                   spp = c(""king"", ""king"", ""ccho""), 
                   stg = c(""smolt"", ""parr"", ""smolt""),
                   ct =  c(293L, 410L, 210L))
```

with the following accompanying metadata:

```ruby
 col_metadata = c(river = ""http://dbpedia.org/ontology/River"",
                  spp = ""http://dbpedia.org/ontology/Species"", 
                  stg = ""Life history stage"",
                  ct = ""count"")
 unit_metadata = 
  list(river = c(SAC = ""The Sacramento River"", AM = ""The American River""),
       spp = c(king = ""King Salmon"", ccho = ""Coho Salmon""),
       stg = c(parr = ""third life stage"", smolt = ""fourth life stage""),
       ct = ""number"")

```

Ensure that all objects have the correct object type: e.g. (ordered) factors should be (ordered) factors, etc.  
",closed,3,Extract appropriate R objects from EML dataTable
5,16015351,cboettig,2013-06-26T05:05:29Z,2013-12-03T18:53:41Z,"A running list of questions that I might direct to Matt if I cannot figure them out:

- Can we get (public) endpoints/URLs to all the public EML files in KNB?

- How should we be generating `id` attributes for `<attribute>` elements? 
- How can we replace `definition` with a URI to an existing ontology?  

Units already have clear semantic definitions, but assigning good definitions to columns or values for character strings (such as species names, or geographic sites, etc) is considerably less developed.  We do have a somewhat round-about way to attach things like ""Coverage"" definitions to columns (attributes).  Replacing definitions with URIs would seem simpler...

- Isn't having `<attributeDefinition>` and `<textDomain><definition>` redundant in the case of character string columns?  (e.g. see example below)


```xml
            <attribute id=""1354213311470"">
               <attributeName>run.num</attributeName>
               <attributeDefinition>which run number. (integer)</attributeDefinition>
               <measurementScale>
                  <nominal>
                     <nonNumericDomain>
                        <textDomain>
                           <definition>which run number</definition>
                        </textDomain>
                     </nonNumericDomain>
                  </nominal>
               </measurementScale>
            </attribute>
```",closed,6,Questions about EML design & implementation
4,16015198,cboettig,2013-06-26T04:57:49Z,2013-10-15T17:41:23Z,"Deploying EML file and associated data objects on the gh-pages branch of a github repository would provide a more natural URL endpoint, and facilitate forking, pull requests, and rapid versioning.  

Would require many of the same steps as #3, but because we don't have a native R interface to Git the actual commit and push could be left to an external script or the user.  Most natural simply to specify the repository end-point to form the appropriate URLs.  ",closed,7,Add function to publish EML data through Github
3,16012790,cboettig,2013-06-26T02:39:11Z,2013-07-01T01:39:07Z,"Involves several steps:

* [ ] Update the EML publisher information as figshare
* [x] Update the EML distributed online URL location with the figshare link
* [x] Update EML link of csv and other data files to their download URLs from figshare
* [x] package EML and all data files (CSV, images, code, etc) as figshare file object. (Will mean using `fs_create` to establish the figshare link metadata first.  Files will need to be uploaded to get their URLs, then the EML file will need to be modified with those URLs.)  
* [x] EML keywordLists for both the figshare tags and figshare categories (with categories reflecting figshare limited thesaurus).  
* [x] Likewise figshare metadata should all come from the EML file.  ",closed,1,Add function to publish EML data through figshare
2,16012599,cboettig,2013-06-26T02:29:12Z,2013-07-01T01:39:07Z,"To begin with, consider rendering a data.frame such as this

```ruby
 dat = data.frame(river=c(""SAC"", ""SAC"", ""AM""), 
                   spp = c(""king"", ""king"", ""ccho""), 
                   stg = c(""smolt"", ""parr"", ""smolt""),
                   ct =  c(293L, 410L, 210L))
```

with the following accompanying metadata:

```ruby
 col_metadata = c(river = ""http://dbpedia.org/ontology/River"",
                  spp = ""http://dbpedia.org/ontology/Species"", 
                  stg = ""Life history stage"",
                  ct = ""count"")
 unit_metadata = 
  list(river = c(SAC = ""The Sacramento River"", AM = ""The American River""),
       spp = c(king = ""King Salmon"", ccho = ""Coho Salmon""),
       stg = c(parr = ""third life stage"", smolt = ""fourth life stage""),
       ct = ""number"")

```

into EML.  
",closed,1,Generate an EML file given a data.frame and appropriate metadata
1,16012453,cboettig,2013-06-26T02:22:30Z,2013-09-03T05:07:43Z,"
A couple of options on how to do this:

* Provide in markdown formatting?
* More crudely: convert directly to YAML instead? `as.yaml(xmlToList(eml.xml))`

Ideal rendering would drop some of the less essential markup (e.g. stuff intended more for machines than people -- unit definitions, numeric types, etc).  ",closed,0,Add a function to generate plain-text summaries of EML metadata
