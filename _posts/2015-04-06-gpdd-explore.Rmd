---
layout: post
category: ecology

#published: false

---


### setup 

```{r}
devtools::install_github("ropensci/rgpdd")
```

```{r}
library("ggplot2")
library("dplyr")
library("tidyr")
library("knitcitations")
library("rgpdd")
library("FKF")
```


```{r}
# For some tidy printing
main <- as.tbl(gpdd_main) 
data <- as.tbl(gpdd_data)
```

Working through `r citet("10.1111/j.1461-0248.2011.01702.x")` provides a nice way to play around with the GPDD data and Kalman filtering.  More interested in exploring the data and methods than in just replicating the results, which are important but also rather intuitive and thus I expect rather robust -- as we add (observational) uncertainty, or indeed, any additional parameters that must be ended, we should expect to have less power to pin down a particular parameter associated with density dependence, as the paper illustrates rather nicely.  



------------



## Data preparation



> 627 time series with population indices obtained from the GPDD (NERC Centre for Population Biology 1999). Data sets were filtered out from the database by removing harvest and non-index based data, data sampled at non-annual intervals and time series taking less than 15 unique values.

Excluding time-series shorter than a minimum length is intuitive. Excluding harvest data makes some sense, as these aren't scientific samples; in particular, they do not necessarily reflect a uniform sampling effort over time.  Not quite clear why one would exclude non-annual intervals.  Not really clear what "non-index based data" even means. Note that this 2012 paper still cites the original 1999 version instead of the 2010 version, which adds 123 datasets (for a total of 5,156).

Anyway, we can roughly infer what these filters mean in terms of the columns and values defined in the "MAIN" table of the GPDD, as described in the [GPDD User Manual](http://www3.imperial.ac.uk/cpb/databases/gpdd).  `dplyr` makes it quick to implement these filters in R:


```{r}
gpdd_main %>% 
  filter(SamplingProtocol == "Count",
         SourceDimension %in% c("Count", "Index"), 
         SamplingFrequency == "1",
         DatasetLength >= 15) %>%
  select(MainID) %>%
  arrange(MainID) ->
filtered
```

Note that we might imagine different selection criteria, looking a bit more closely at the relevant fields. 

------------------


Unfortunately these don't quite align with the paper, even allowing for the 123 additional datasets. Most obviously, we have `r length(filtered$MainID)` matches instead of 627. Not only have we somehow grabbed more datasets than expected, but the Knape et al list includes quite a few datasets that do not meet these criteria:

```{r}
source("http://ropensci.github.io/rgpdd/data/knape.R")
gpdd_main %>% 
  filter(MainID %in% knape_ids) %>%
  select(MainID, SamplingProtocol, SourceDimension, SamplingFrequency, DatasetLength)  %>%
  arrange(MainID) %>% 
  as.tbl() -> knape_data

tail(sort(table(knape_data$SourceDimension)))
tail(sort(table(knape_data$SamplingProtocol)))
```

Those summaries show data with attributes we excluded, including Harvest as a sampling protocol.  We see over 100 such data sets:



```{r}
gpdd_main %>% 
  filter(MainID %in% knape_ids) %>%
  filter(SamplingProtocol == "Count",
         SourceDimension %in% c("Count", "Index"), 
         SamplingFrequency == "1",
         DatasetLength >= 15) %>% dim()
```

The reason for this discrepancy isn't clear, but we can proceed with our filtered data instead:


------------------

Selecting the time-series identified by our filter, we also add a column for $\log(N)$ population:

```{r}
gpdd_data %>% 
  filter(MainID %in% filtered$MainID) %>%
  select(MainID, Population, SampleYear) %>%
  group_by(MainID) %>% 
  mutate(logN = log(Population)) ->
df
```

Interestingly there are no missing data reported:

```{r}
sum(sapply(df$Population, is.na))
df %>% filter(Population < 0) # -9999 is elsewhere used for missing data
```

but some population values are equal to 0, creating some `-Inf` terms in our log data, which will create trouble in fitting the models. The authors don't say how they handled this case. We will manually set them to the smallest value observed:

```{r}
i <- which(df$logN == -Inf)
df$logN[i] <- min(df$logN[-i])-1
```


## Gompertz model

> We use the stochastic Gompertz population model to analyse the strength of density dependence in the data.The model is defined through

$$N_{t+1} = N_t \exp(a - b \log N_t + \epsilon_t$$ 

> where $N_t$ is population density or size in year $t$, $a$ is an intercept, $b$ is a measure of the strength of density dependence and $\eta$ is normally distributed process error with mean zero and standard deviation $\tau$.  By log transforming the population abundance and putting $x_t = \log N_t$ this simplifies to

$$x_{t+1} = a _ c x_t + \epslion_t$$

> where $c = 1 - b$ is the lag 1 autocorrelation of the log transformed population abundance when the process is stationary.

and uncertainty in measurements, $y_t$ are simply normal random variates around $x_t$:

$$y_t = x_t + \eta_t $$ 

where $\eta_t \sim N(0,\sigma^2)$


## Kalman Filtering

Since the model is linear we can compute the likelihood directly by means of a Kalman filter. There are various ways of going about this, and the paper provides some general details:

> The Kalman filter was initiated by assuming a wide prior distribution on the initial state centred around
the first observation, $x_1 \sim N( y_1, 10)$. For each model and data set the numerical maximisation (using the BFGS algorithm implemented in the optim function) was repeated for 50 random starting values to ensure that we found the global optimum

Some useful detail here, but still a bit vague for our purposes, or are things we might have done differently.

- Doesn't state which Kalman filter (their are a few algorithms and several packages, but the differences seem quite small: [this JSS paper](http://www.jstatsoft.org/v39/i02) has a good overview.) 

- Not clear that we wouldn't want to scale the prior variance by the data sample, e.g. `var(y)`, but since we're on a log scale `10` is indeed pretty wide. 

- `BFGS` doesn't seem as robust as some alternatives; (e.g. gives a rather different result than Nelder-Meade or `StructTS()` model on the classic Nile data set example from the FKF package). 

- Also challenging are the "50 random starting values", which does not tell us from what distribution they were drawn.  Too wide a distribution will start to include values for which the likelihood cannot be evaluated, while too narrow serves little purpose.  Moreover it is unclear if this is really preferable to simply using fewer starting points and a more robust algorithm. For simplicity, we'll ignore this and just choose justifiable starting conditions. 



> Four variants of the model defined by (1) and (2) were fitted to each data set; a full model with
both uncertainty about population abundance and density dependence denoted by SSG (state space Gompertz), a model with uncertainty about population abundance, but no density dependence (c fixed to one) denoted SSRW (state space random walk), a model with density dependence, but no uncertainty about population abundance (r2 fixed to zero) denoted G (Gompertz) and a model with neither uncertainty about population abundance nor density dependence (c fixed to one and r2 fixed to zero) denoted RW (random walk)

This is both clear and straight forward, we define each of the models as described.  Note that we define the models here in the notation of FKF:

$$\alpha_{t+1} = d_t + T_t \alpha_t H_t \eta_t$$
$$\y_t = c_t + Z_t \alpha_t + G_t \eta_t$$



Where 

$$\begin{multline}
c \to T_t
a \to d_t
sigma^2 \to G_t'G_t
\tau^2 \to H_t'H_t
\end{multline}$$

```{r}
kalman <- function(df, # data.frame
                   ... # additional arguments to optim
                   ){

y <- df$logN

ssg <- optim(c(dt = mean(y), Tt = 1, HHt = var(y)/2, GGt = var(y)/2),
                 fn =  function(par, ...)
                   -fkf(dt = matrix(par[1]),
                        Tt = matrix(par[2]),
                        HHt = matrix(par[3]), 
                        GGt = matrix(par[4]), 
                        ...)$logLik,   
                 a0 = y[1], 
                 P0 = matrix(10), 
                 ct = matrix(0),
                 Zt = matrix(1), 
                 yt = rbind(y), 
                 check.input = FALSE, 
                 ...)

ssrw <- optim(c(dt=mean(y), HHt = var(y)/2, GGt = var(y)/2),
                 fn =  function(par, ...)
                   -fkf(dt = matrix(par[1]), HHt = matrix(par[2]), 
                        GGt = matrix(par[3]), ...)$logLik,   
                 a0 = y[1], P0 = matrix(10), ct = matrix(0), Tt = matrix(1),
                 Zt = matrix(1), yt = rbind(y), check.input = FALSE, 
                 ...)

g <- optim(c(dt = mean(y), Tt=1, HHt = var(y)),
                 fn =  function(par, ...)
                   -fkf(dt = matrix(par[1]), Tt = matrix(par[2]), 
                        HHt = matrix(par[3]), ...)$logLik,   
                 a0 = y[1], P0 = matrix(10), ct = matrix(0), GGt = matrix(0),
                 Zt = matrix(1), yt = rbind(y), check.input = FALSE, 
                 ...)

rw <- optim(c(dt=mean(y), HHt = var(y)),
                 fn =  function(par, ...)
                   -fkf(dt = matrix(par[1]), HHt = matrix(par[2]), ...)$logLik,   
                 a0 = y[1], P0 = matrix(10), ct = matrix(0), Tt = matrix(1),
                 GGt = matrix(0), Zt = matrix(1), yt = rbind(y), check.input = FALSE, 
                 ...)

fits <- 
  rbind(ssg = data.frame(t(c(model="SSG", ssg$par, mloglik = ssg$value, converge=ssg$convergence))),
        ssrw = data.frame(t(c(model="SSRW", ssrw$par, Tt = 1, mloglik = ssrw$value, converge=ssrw$convergence))),
        g = data.frame(t(c(model="G", g$par, GGt = 0, mloglik = g$value, converge=g$convergence))),
        rw = data.frame(t(c(model="RW", rw$par, Tt = 1, GGt = 0, mloglik = rw$value, converge=rw$convergence))))

# Not clear why classes are not being detected correctly.
for(i in c("dt", "Tt", "HHt", "GGt", "mloglik")) 
 fits[[i]] <- as.numeric(fits[[i]])

fits

}

```

Having defined a function that takes and returns a `data.frame`, `dplyr::do` gives us a consise syntax to apply this by group (recall `df` is already `group_by(MainID)`.)  


```{r message=FALSE}
df %>% do(kalman(.)) -> fits
```

## Side note on coding strategy

`dplyr` makes the data filtering steps fast, consise, and clear.  Dealing with model outputs here is actually far less straight forward than cleaning the raw data. For instance, I have collected the output of each model fit into it's own row.  The models have different numbers of parameters, so I must add the fixed parameters to avoid rows of different length; but clearly this does not generalize to the case where the different models can have arbitrarily different parameters.  

[David Robinson](https://github.com/dgrtwo) has done an excellent job in starting to tackle this thorny problem in a package called `broom` with much the same elegance and care that Hadley has done with `dplyr`.  David argues very persuasively that it makes more sense to summarize parameter estimates from each model fit in two columns - a column for parameter names and a column for values.  He has now added support for `optim()` output to the `broom::tidy()` function, which does just that.  

This doesn't translate immediately to my use case here, since I need to keep track of the likelihood and convergence which are not captured by `broom::tidy()`.  As these are scalar valued outputs of the model fit, instead of a vector like parameters, they are extracted and summarized in `broom::glance()` as a single row.  An easy generalization would just be to `cbind` the outputs of `glance()` and `tidy()`; though David argues for a different approach in which we defer any manipulation of model output. Instead, he suggests an `expand.grid` over the names of the groups (models and datasets):

```r
# not run

expand.grid(model = names(models),
            dataset = names(datasets)) %>%
    group_by(model, dataset) %>%
    do(o = optim_func(.$model, .$dataset)) -> david
```

Where `optim_func` uses the name of the model and name of the dataset to apply `optim()`, rather than passing the data explicitly.  We can then use `glance` and `tidy` on the resulting output, though we would need to `inner_join` the results instead of `cbind`.  

This is indeed an elegant, more generic approach, though passing names instead of data and tidying & joining more complex aggregate data frames seems a bit less transparent to me.

------------

Note that BFGS is more problematic:

```{r, error=FALSE}
df %>% do(kalman(., method="BFGS")) -> fits_bfgs
```


We now have a nice table of parameter estimates and likelihoods by model for each data set:

```{r}
fits
```

A bit worrying that one of our simpler models sometimes have better (smaller) minus log likelihood scores than the full SSG model; but that model (3, SSRW) also has a flag indicating convergence issues.  

and here is our version then of Figure 1b:

```{r}
fits %>% 
  group_by(MainID, model) %>% 
  select(Tt) %>% 
  spread(model, Tt) %>% 
  select(SSG, G) -> 
  c_values
ggplot(c_values, aes(SSG, G)) + geom_point() + geom_abline(intercept=0, slope=1) + labs(title="c estimates")
```

```{r}
fits %>% 
  group_by(MainID, model) %>% 
  select(Tt) %>%
  filter(model %in% c("SSG", "G")) -> tmp
ggplot(tmp) + geom_histogram(aes(Tt), binwidth=0.1)
```


```{r}
fits %>% 
  group_by(MainID, model) %>% 
  select(GGt) %>% 
  spread(model, GGt) %>% 
  select(SSG, SSRW) -> 
  sigma2_values
ggplot(sigma2_values, aes(SSG, SSRW)) + geom_point() + geom_abline(intercept=0, slope=1) + labs(title="sigma2 (observation error) estimates")
```


















---------

Running the example from the FKF package on the Nile data but with the `BFGS` algorithm, the fit is much farther from StructTS implementation...

```{r}
y <- Nile
y[c(3, 10)] <- NA # NA values can be handled
## Set constant parameters:
dt <- ct <- matrix(0)
Zt <- Tt <- matrix(1)
a0 <- y[1] # Estimation of the first year flow
P0 <- matrix(100) # Variance of 'a0'
## Estimate parameters:
fit.fkf <- optim(c(HHt = var(y, na.rm = TRUE) * .5,
GGt = var(y, na.rm = TRUE) * .5),
fn = function(par, ...) -fkf(HHt = matrix(par[1]), GGt = matrix(par[2]), ...)$logLik,
yt = rbind(y), a0 = a0, P0 = P0, dt = dt, ct = ct, Zt = Zt, Tt = Tt, check.input = FALSE,
method = "BFGS")
## Filter Nile data with estimated parameters:
fkf.obj <- fkf(a0, P0, dt, ct, Tt, Zt, HHt = matrix(fit.fkf$par[1]),
GGt = matrix(fit.fkf$par[2]), yt = rbind(y))
## Compare with the stats' structural time series implementation:
fit.stats <- StructTS(y, type = "level")
fit.fkf$par
fit.stats$coef
## Plot the flow data together with fitted local levels:
plot(y, main = "Nile flow")
lines(fitted(fit.stats), col = "green")
lines(ts(fkf.obj$att[1, ], start = start(y), frequency = frequency(y)), col = "blue")
legend("top", c("Nile flow data", "Local level (StructTS)", "Local level (fkf)"),
col = c("black", "green", "blue"), lty = 1)
```
