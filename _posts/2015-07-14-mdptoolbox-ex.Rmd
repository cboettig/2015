---
layout: post
category: ecology

---



Adapted from MARESCOT ET AL. APPENDIX 5


## STEP 1: DEFINE OBJECTIVES


This is a conceptual step which does not require coding


## STEP 2: DEFINE STATES

```{r}
K <- 250 # state space limit
states <- 0:K # Vector of all possible states
```

## STEP 3: DEFINE CONTROL ACTIONS


```{r}
# Vector of actions: rate of the population that can be removed, ranging #from 0 to 1
H <- seq(0, 1, 1/(K+1))
```

## STEP 4: DEFINE DYNAMIC MODEL (WITH DEMOGRAPHIC PARAMETERS)

```{r}
# Population growth rate
lambda <- 1.25

# Function for the exponential growth of the dynamic model
dynamic <- function(actualpop, action) {
	nextpop <- actualpop*lambda*(1-action)
	return(nextpop)
}
```

# STEP 5: DEFINE UTILITY

```{r}
# Maximum objective threshold for population abundance
Nmax <- 250
# Minimum objective threshold for population abundance
Nmin <- 50

# Utility function
get_utility <- function(x) {
	return(ifelse(x < Nmin | x > Nmax, 0, x))
}
```

## STEP 6: SOLVE BELLMAN EQUATION WITH VALUE ITERATION

```{r}
# Initialize transition matrix
transition <- array(0, dim = c(length(states), length(states), length(H)))

# Initialize utility matrix
utility <- array(0, dim = c(length(states), length(H)))
```


```{r}
# Fill in the transition and utility matrix
# Loop on all states
for (k in 0:K) {

	# Loop on all actions
	for (i in 1:length(H)) {

# Calculate the transition state at the next step, given the 
# current state k and the harvest Hi
		nextpop <- dynamic(k, H[i])
		
	# Implement demographic stochasticity by drawing 
  # probability from a Poisson density function
		transition[k+1, , i] <- dpois(states,nextpop)
		# We need to correct this density for the final capping state 
		# EDIT: this can be negative, due to floating-point errors. so we take max(v,0) to avoid
		transition[k+1, K+1, i] <- max(1 - sum(transition[k+1, -(K+1), i]),0)
		
		# Compute utility
		utility[k+1, i] <- get_utility(nextpop)

	} # end of action loop
} # end of state loop

# Discount factor
discount <- 0.9

# Action value vector at tmax
Vtmax <- numeric(length(states))

# Action value vector at t and t+1
Vt <- numeric(length(states))
Vtplus <- numeric(length(states))

# Optimal policy vector
D <- numeric(length(states))

# Time horizon
Tmax <- 150
```


## Solution calculated explicitly:

The backward iteration consists in storing action values in the vector Vt which is the maximum of
utility plus the future action values for all possible next states. Knowing the final action 
values, we can then backwardly reset the next action value Vtplus to the new value Vt. We start 
The backward iteration at time T-1 since we already defined the action 
value at Tmax.

```{r}
for (t in (Tmax - 1):1) {

# We define a matrix Q that stores the updated action values for 
# all states (rows)
# actions (columns)
	Q <- array(0, dim = c(length(states), length(H)))
	
	for (i in 1:length(H)) {
	
# For each harvest rate we fill for all states values (row) 
# the ith column (Action) of matrix Q
# The utility of the ith action recorded for all states is 
# added to the product of the transition matrix of the ith 
# action by the action value of all states 
		Q[,i] <- utility[, i] + discount * (transition[,,i] %*% Vtplus)
	
	} # end of the harvest loop

	# Find the optimal action value at time t is the maximum of Q
	Vt <- apply(Q, 1, max)

# After filling vector Vt of the action values at all states, we 
# update the vector Vt+1 to Vt and we go to the next step standing 
# for previous time t-1, since we iterate backward
	Vtplus <- Vt

} # end of the time loop

# Find optimal action for each state
for (k in 0:K) {
# We look for each state which column of Q corresponds to the #maximum of the last updated value 
# of Vt (the one at time t + 1). If the index vector is longer than 1 #(if there is more than one optimal value we chose the minimum 
# harvest rate)
	D[k + 1] <- H[(min(which(Q[k + 1, ] == Vt[k + 1])))]
}
```




## PLOT SOLUTION

```{r}
plot(states, D, xlab="Population size", ylab="harvest rate")
```


## PROOF OF OPTIMALITY: COMPARE WITH ANALYTICAL SOLUTION

```{r}
exact_policy <- rep(0,K)
for (k in 0:K) {
	exact_policy[k+1] <- max(0, 1 - K/(k*lambda))
}

# The difference between Bellman equation solution and the analytical #solution is small:
lines(states, exact_policy)
D - exact_policy

```

## Using toolbox


```{r}
library("MDPtoolbox")
mdp_check(P = transition, R = utility)
out <- mdp_value_iteration(transition, utility, discount = discount, epsilon = 0.001, max_iter = 5e3, V0 = Vtmax)

```



```{r}
lines(states, H[out$policy], col="red", lty=2)
```