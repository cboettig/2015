---
layout: post
category: ecology
published: false
tags:
  - gpdd
---



Load libraries and data as before

```{r cache=FALSE}
library("ggplot2")
library("dplyr")
library("tidyr")
library("knitcitations")
library("rgpdd")
library("FKF")
knitr::opts_chunk$set(cache=FALSE)
```


Prepare data, as before: we filter on the stated criteria

```{r}
gpdd_main %>% 
  filter(SamplingProtocol == "Count",
         SourceDimension %in% c("Count", "Index"), 
         SamplingFrequency == "1",
         DatasetLength >= 15) %>%
  select(MainID) %>%
  arrange(MainID) ->
filtered
```

and select data matching this filter. We add a column for the log of the population size and group by data ID:

```{r}
gpdd_data %>% 
  filter(MainID %in% filtered$MainID) %>%
  select(MainID, Population, SampleYear) %>%
  group_by(MainID) %>% 
  mutate(logN = log(Population)) ->
df
```

Lastly, we replace `-Inf` (introduced from `log(0)` terms) with smallest finite values observed. (arbitrary, authors do not specify how these values are handled.)


```{r}
i <- which(df$logN == -Inf)
df$logN[i] <- min(df$logN[-i])-1
```


We import our previous model definitions:

```{r}
downloader::download("https://github.com/ropensci/rgpdd/raw/master/inst/scripts/knape-de-valpine.R", "knape-de-valpine.R")
source("knape-de-valpine.R")
unlink("knape-de-valpine.R")
```


## Simulating, bootstrapping

FKF package doesn't bother to define a simulation method, so we can simply define one directly from the state equations. Though a C implementation would be preferrable, fitting will always be much more rate-limiting. (We will also ignore the multi-variate definition for simplicity here).

```{r}

use <- function(x, default){
  if(is.null(x))
    default
  else
    x
}

sim_fkf <- function(fit){
  n <- fit[["n"]]
  dt <- fit[["dt"]]
  HHt <- fit[["HHt"]]
  Tt <- use(fit[["Tt"]], 1)
  GGt <- use(fit[["GGt"]], 0)
  a0 <- fit[["a0"]]
  ct <- 0
  Zt <- 1
  
  a <- numeric(n)
  y <- numeric(n)
  eta <- rnorm(n, dt, sqrt(HHt))
  epsilon <- rnorm(n, ct, sqrt(GGt))
  a[1] <- a0
	for(t in 1:(n-1)){
	  a[t+1] <- Tt * a[t] + eta[t]
	  y[t] <- Zt * a[t] + epsilon[t]
	}
		y[n] <- Zt * a[n] + epsilon[n]
  y
}

```

With fitting and simulating functions in place, defining the bootstrap is straight forward. We define these separately for the state-space Gompertz (ssg; i.e. the model with both density dependence and observational errors) and the Gompertz (g; density dependence, no observational error).  We compare in each case to the simulations of the corresponding model without density dependence.  


```{r}
bootstrap_ssg <- function(df, N=100){
  y <- df$logN
  
  ssg <- robust_fit("ssg", y)
  ssrw <- robust_fit("ssrw", y)
  sims <- as.data.frame(t(replicate(N, sim_fkf(ssrw))))
  
  # We use a relaxed version of robust_fit, with N=3
  sims %>% rowwise() %>% do(robust_fit("ssrw", y = as.numeric(.), N = 3)) %>% select(mloglik) -> null
  
  # compute p value of observed LR statistic relative to null distribution
  lr <- 2 * (ssrw$mloglik - ssg$mloglik)
  null_dist <- 2 * (null$mloglik - ssg$mloglik) 
  sum(null_dist < lr)/N
}
```

```{r}
bootstrap_g <- function(df, N=100){
  g <- robust_fit("g", df$logN)
  rw <- robust_fit("rw", df$logN)
  sims <- replicate(N, sim_fkf(rw)) 
  null <- sapply(sims, function(y) robust_fit("rw", y)$value)
  
  lr <- 2 * (rw$value - g$value)
  null_dist <- 2 * (null - g$value) 
  sum(which(null_dist < lr))/N
}

  
```

With these functions defined, we can perform the actual analysis:

```{r}
df %>% group_by(MainID) %>% do(bootstrap_ssg(.)) -> ssg_p_values
```


```{r}
df %>% group_by(MainID) %>% do(bootstrap_g(.)) -> ssg_p_values
```


The study also creates simulated datasets based on the real data but explicitly making the assumption of either density independence (DI) or density dependence (DD).  For each dataset, a density-independent simulated dataset is created by simulating under the SSRW model that was fit. The density-dependent model is created by explicitly fixing the density dependent parameter ($c$ in the language of the paper, `Tt` in FKF notation) to 0.8 and estimating the other parameters of this modified SSG model.  We can define this model analgously to the others, only this time fixing `Tt = 0.8`:



```{r}
fit_dd <- function(y, 
                   init = c(dt = mean(y), HHt = log(var(y)/2), GGt = log(var(y)/2)),
                   ...){
    
    o <- optim(init,
                 fn =  function(par, ...)
                   -fkf(dt = matrix(par[1]), HHt = matrix(exp(par[2])), 
                        GGt = matrix(exp(par[3])), ...)$logLik,
                 Tt = matrix(0.8), a0 = y[1], P0 = matrix(10), 
                 ct = matrix(0), Zt = matrix(1), yt = rbind(y), 
                 check.input = FALSE, ...)
  o$par[["HHt"]] <- exp(o$par[["HHt"]])
  o$par[["GGt"]] <- exp(o$par[["GGt"]])
  c(o, list(a0 = y[1], n = length(y)))
   
}

```


The script adds a method for this to `robust_fit()` as well; though given the computational cost it is not clear if a robust fit is actually used in generating the data.


```{r}
sim_di <- function(df) data.frame(logN = sim_fkf(robust_fit("ssrw", df$logN, N=3)))
df %>% group_by(MainID) %>% do(sim_di(.)) -> DI
```

```{r}
sim_dd <- function(df) data.frame(logN = sim_fkf(robust_fit("dd", df$logN, N=3)))
df %>% group_by(MainID) %>% do(sim_dd(.)) -> DD
```


With a little patience, we can then use these two collections of datasets just as before:

```{r}
DD %>% do(kalman(.)) -> DD_fits
```

```{r}
DI %>% do(kalman(.)) -> DI_fits
```




