<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Lab Notebook - </title>
  <link href="//.xml" rel="self"/>
  <link href="/"/>
  <updated>2015-06-19T18:15:45+00:00</updated>
  <id>http://www.carlboettiger.info/</id>
  <author>
    <name>Carl Boettiger</name>
    <email>cboettig@gmail.com</email>
  </author>

  
<entry>
  <title>Open Science Post Doc</title>
  <link href="http://www.carlboettiger.info/2015/04/20/open-science-post-doc.html"/>
 <updated>2015-04-20T00:00:00+00:00</updated>
  <id>/04/20/open-science-post-doc</id>
  <content type="html">&lt;p&gt;This post is a response to the query posted by Titus Brown on &lt;a href=&quot;http://ivory.idyll.org/blog/2015-how-to-find-openscience-advisor.html&quot;&gt;advice for doing an Open Science Post doc&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Open Science is a broad tent, and I believe there are many ways to engage in open science without crossing boundaries of collaborators or PIs. Some of the most valuable open science practices are those least likely to cross boundaries: in particular, those practices associated with post-publication of academic papers. Many widely regarded journals, including &lt;em&gt;Nature&lt;/em&gt; and &lt;em&gt;Science&lt;/em&gt;, have relatively strong data publication requirements, increasingly strong code publication expectations, and are compatible with the use of pre-print archives. Yes, their are those who would hesitate even to comply with the most minimal interpretation of these expectations; but there are many more who, given the expectation to do this at all, would appreciate your knowledge and effort on doing these things well and consistent with best practices. Using data repositories instead of supplemental material; exhibiting good management of data and code; providing good metadata in consistent formats; citing software and data appropriately.&lt;/p&gt;
&lt;p&gt;Many of the practices promoted by open scientists work almost as well in a setting that is closed until you ‘flip the switch’ to make them public; even if that is long after publication. Good data management, a private Github repository, or a private electronic lab notebook are all ways to leverage best practices tools and approaches in a setting that can either be shared securely or made open later. Attitudes to post-publication sharing are rarely black-and-white, and having everything curated and ready to go ahead of time can help nudge collaborators in the direction of best practices.&lt;/p&gt;
&lt;p&gt;I’ve been a post-doc for just over 2 years while enjoying a relatively open-science approach to my research: I’ve kept an open lab notebook, posted my papers ahead of publication on pre-print archives, released code, data, and knitr versions of my papers, discuss my work on social media, sign reviews, shared grant applications, contributed open source software and been active in open science communities.&lt;/p&gt;
&lt;p&gt;I was fortunate to have independent funding for most but not all of my post-doc, and to work in a field where researchers are often given substantial independence and in which open practices are common. Nonetheless these were not practices shared by either of my two excellent co-advisers, who gave only a neutral or vaguely positive response to these ideas.&lt;/p&gt;
&lt;p&gt;Nevertheless, I learned more about &lt;em&gt;old-school&lt;/em&gt; open science from them than I had ever imagined. When I sent Marc the first draft of our paper, I had buried almost all of the equations in a curt appendix with minimal and jargon-laden supporting text. No sir, those equations not only had to appear in the main text, but I must endeavor to explain the meaning and relevance of each one in a language clear and concise enough for any ecologist to follow. I won’t claim to have succeeded, but boy did continuous integration on my unit tests feel like a low bar for openness by comparison.&lt;/p&gt;
&lt;p&gt;Meanwhile, I also felt I had the support and mentorship of an online community of open scientists even without going to work for one of them. I am thankful that my mentors have always been tolerant of my open science experimentation, but I would have enjoyed working with them even if it had been otherwise. There was much to learn from them, much to learn from the open science community, and after all, a post-doc position doesn’t last forever.&lt;/p&gt;
</content>
</entry>

<entry>
  <title>Question-and-Answer-post on my use of Docker in research</title>
  <link href="http://www.carlboettiger.info/2015/02/24/docker-science-question-and-answer.html"/>
 <updated>2015-02-24T00:00:00+00:00</updated>
  <id>/02/24/docker-science-question-and-answer</id>
  <content type="html">&lt;p&gt;I received an interesting email from an academic computing unit regarding the use of Docker in my research:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We’ve been reading, listing, and prototyping best practices for building base images, achieving image composition, addressing interoperability, and standardizing on common APIs. When I read your paper, I thought you might have some opinions on the subject.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Would you be willing to share your experiences using Docker for research with our team? It doesn’t have to be a formal presentation. In fact, we generally prefer interactive conversations over slides, abstracts, etc. I appreciate that you must be terribly busy with your postdoc fellowship and rOpenScience responsibilities. If you’re not able to speak, perhaps you can answer a few questions about your use of Docker.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here are some quick answers in reply; though like the questions themselves my replies are on the technical end and don’t give a useful overview of how I’m actually using Docker. Maybe that’s a subject for another post some time.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Are you currently still using Docker for your research? If so, how are you integrating that into your more demanding computational needs?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Yes. Docker helps me quickly provision and deploy a temporary compute environment on a cloud server with resources appropriate to the computation. This model much more accurately reflects the nature of computational research in a discipline such as ecology than does the standard HPC cluster model. My research typically involves many different tasks that can be easily separated and do not need the specialized architecture of a supercomputer, but do rely on a wide range of existing software tools and frequently also rely on internet connectivity for accessing certain data resources, etc. Because Docker makes it very easy for me to deploy customized software environments locally and on cloud computing resources, it facilitates my process of testing, scaling and distributing tasks to the appropriate computational resources quickly, while also increasing the portability &amp;amp; reproducibility of my work by colleagues who can benefit from the prebuilt environment provided by the container.&lt;/p&gt;
&lt;ol start=&quot;2&quot; type=&quot;1&quot;&gt;
&lt;li&gt;How/do you make use of data containers?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I rarely make use of data containers. I find they, and container orchestration more generally, are very well suited for deploying a service app (such as an API), but are less natural for composing scientific research environments which requires orchestrating volumes instead of tcp links. For instance, at present, there is no interface in &lt;code&gt;--volumes-from&lt;/code&gt; to mount the shared volume at a different mount point in the different container. Thus one cannot just link libraries from different containers with a &lt;code&gt;-v /usr/lib&lt;/code&gt; or &lt;code&gt;-v /usr/bin&lt;/code&gt;, as this would clobber the existing libraries.&lt;/p&gt;
&lt;p&gt;Also, it’s rather a nuisance that on the current Debian/Ubuntu kernels at least, &lt;code&gt;docker rm&lt;/code&gt; does not fully clean up space from data containers (though we now have &lt;code&gt;docker rm -v&lt;/code&gt;)&lt;/p&gt;
&lt;ol start=&quot;3&quot; type=&quot;1&quot;&gt;
&lt;li&gt;What are you using to run your containers in production?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Production is a diverse notion in scientific research – from a software perspective scientific work is almost 100% development and 0% production. For containers running public, always-on services, I tend to run from a dedicated, appropriately resourced cloud server such as Digital Ocean. I don’t write such services very often (though we have been doing this to deploy public APIs for data recently), so this is the closest I get to ‘production’. I run my development environment and all my research code out of containers as well, both locally and on various cloud servers.&lt;/p&gt;
&lt;p&gt;In all cases, I tend to run containers using the Docker CLI. I’ve found fig places larger resource requirements to run the same set of containers – so much so that it will fail to start on a machine that can run the containers fine from CLI or Fleet. fig also feels immature; it does not provide anything close to parity with the Docker CLI options.&lt;/p&gt;
&lt;p&gt;Further, while I find orchestration a powerful concept that is well suited for certain use-cases (our recent API uses five containers), for many academic research uses I find that orchestration is both unnecessary and a barrier to use. Orchestration works really well for professionally designed, web native, open source stack: our recent API deployment uses Redis, MySQL, NGINX, Sinatra, Unicorn, Logstash, ElasticSearch and Kibana – services that are all readily composed from official Docker containers into an existing application. Most scientific work looks nothing like this – the common elements tend to be shared libraries that are not well adapted to the same abstraction into separate services.&lt;/p&gt;
&lt;ol start=&quot;4&quot; type=&quot;1&quot;&gt;
&lt;li&gt;How are you sharing them aside from the Docker Hub?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Primarily through making the Dockerfiles available on Github. This makes it easy for others to build the images locally, and also fork and modify the Dockerfile directly. I maintain a private Docker registry as well but rarely have need for it.&lt;/p&gt;
&lt;ol start=&quot;5&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Do you have practical experience and advice about achieving real portability with Docker across hosting environments (ie. stick with X as an OS, use a sidekick and data container for data backups and snapshotting, etc)?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Overall this hasn’t been much of an issue. Sharing volumes with the host machine on hosts that require virtualization/boot2docker was an early problem, though this has been much better since Docker 1.3. In a similar vein, getting &lt;code&gt;boot2docker&lt;/code&gt; running on older hardware can be problematic. And of course docker isn’t really compatible with 32 bit machines.&lt;/p&gt;
&lt;p&gt;After spending some time with CoreOS, I tend to use Ubuntu machines when running in the cloud: ‘highly available’ isn’t much of a priority in the research context, where few things are at a scale where hardware failure is an issue. I found CoreOS worked poorly on individual machines or cluster sizes that might shrink below 2; while the new OS model was a barrier to entry for myself and for collaborators. I suspect this situation will improve dramatically as these tools gain polish and abstraction that requires less manual tooling for common patterns (I find that ambassador containers, sidekick containers, etc place too many OS-level tasks on the shoulders of the user). Of course there is a large ecosystem of solutions in this space, which also needs time to harden into standards.&lt;/p&gt;
&lt;p&gt;Perhaps my comments re: CLI vs fig in Q3 are also relevant here?&lt;/p&gt;
&lt;ol start=&quot;6&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Have the computational requirements of your research codes outgrown a physical node?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Not at the present. I’ve run prior work on clusters on a campus and at the DOE’s Carver machine at NERSC, though at this time I can almost always meet computational needs with the larger single instances of a service like EC2 or DigitalOcean. Much more often I have the need to run many different codes (sometimes related things that could be parallelized in a single command but are better off distributed, but much more often unrelated tasks) at the same time. Being able to deploy these in isolated, pre-provisioned environments on one or multiple machines using Docker has thus been a huge gain in both my efficiency and realized computational performance. If any particular task becomes too intensive, Docker makes it very easy to move over to a new cloud instance with more resources that will not interfere with other tasks.&lt;/p&gt;
&lt;ol start=&quot;7&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Do you have a workflow for versioning and releasing images comparable to GitFlow?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Nope, though maybe this would be a good idea. I work almost exclusively with automated builds and hardly ever use &lt;code&gt;docker commit&lt;/code&gt;. Though the Dockerfiles themselves are versioned, obviously the software that is installed between different automated builds can differ substantially, and there is in general no way to recover an earlier build. Using a &lt;code&gt;docker commit&lt;/code&gt; workflow instead would provide this more versioned history and greater stability of the precise binaries installed, but also feels more like a black box as the whole thing cannot then be reproduced from scratch.&lt;/p&gt;
&lt;ol start=&quot;8&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Do you version your Dockerfile/image/source code separately?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I version all my Dockerfiles with git. I version my images as needed but more rarely, and in a way that reflects the version of the predominant software they install (e.g. r-base:3.1.2).&lt;/p&gt;
&lt;ol start=&quot;9&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Do you prefer using the entrypoint or command to run your containers by default?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I prefer command (&lt;code&gt;CMD&lt;/code&gt;), as it is more semantic, easier to alter the default and can take longer strings (no need for a flag).&lt;/p&gt;
</content>
</entry>

<entry>
  <title>Notebook Maintenance And Scaling</title>
  <link href="http://www.carlboettiger.info/2015/01/01/notebook-maintenance-and-scaling.html"/>
 <updated>2015-01-01T00:00:00+00:00</updated>
  <id>/01/01/notebook-maintenance-and-scaling</id>
  <content type="html">&lt;p&gt;Electronic notebooks may not run out of pages like a paper notebook, but with five years of entries (963 posts, with a repository size approaching half a gigabyte), together with various files, layouts, experimentation and version history, some thought must be given to scale. Two closely related considerations add to this further: dynamic builds with &lt;code&gt;knitr&lt;/code&gt; from &lt;code&gt;.Rmd&lt;/code&gt; versions and hosting image files directly in the notebook repository rather than uploading to an external site (previously flickr or on the gh-pages of other project repositories). This has several advantages (more on that later) but in the immediate term it makes building the repository potentially slower (though knitr’s caching helps) and increases the repository size more rapidly (even with text-based &lt;code&gt;svg&lt;/code&gt; images).&lt;/p&gt;
&lt;p&gt;The current Jekyll system keeps all posts in a single repository and rebuilds the HTML files for each every time. This is already showing some strains: for instance, for some reason the git hashes when generating the site automatically on Travis cease updating for older posts, though this problem doesn’t occur when building locally. Overall, the Jekyll platform is rather snappy so this isn’t an unmanageable size, but is sufficient to demonstrate that the approach isn’t able to scale indefinitely either.&lt;/p&gt;
&lt;p&gt;So, as with the paper notebook whose pages are filled, it’s time to crack open a new binding and shelve the old notebooks – somewhere handy to be sure, but no longer one voluminous tome on the desk.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/2015/assets/figures/posts/2015-01-01/notebooks-shelf.jpg&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;a-multi-repository-approach&quot;&gt;A multi-repository approach&lt;/h2&gt;
&lt;p&gt;To address this, I’m am trying out breaking the notebook over multiple repositories: using a new repository for each year’s worth of entries, and an additional repository to provide the basic pages (&lt;code&gt;home&lt;/code&gt;, &lt;code&gt;teaching&lt;/code&gt;, &lt;code&gt;vita&lt;/code&gt;, etc. from the navbar) along with the assets used by all the other sites (css, fonts, javascript, etc). This avoids rebuilding the posts of notebooks from all previous years every time the Jekyll site is compiled, keeping the repositories smaller, the site more modular and more easy to scale.&lt;/p&gt;
&lt;p&gt;This raises some challenges such as keeping the layout and appearance consistent without maintaining copies of layout files across multiple repositories; managing URLs and paths across different repositories, and aggregating metadata (posts, tags, categories).&lt;/p&gt;
&lt;h2 id=&quot;repos-paths-and-urls-for-the-multi-notebook&quot;&gt;Repos, Paths, and URLs for the multi-notebook&lt;/h2&gt;
&lt;p&gt;Even with the source files (such as &lt;code&gt;.md&lt;/code&gt; entries, templates, etc.) in different repositories it would be simple enough to combine the generated HTML files from each repository into a single output directory serving the site (on Github or elsewhere). However, GitHub’s &lt;code&gt;gh-pages&lt;/code&gt; provide an elegantly more modular way to do this already. GitHub uses the URL of the user’s repository (the repo named &lt;code&gt;username.github.io&lt;/code&gt;, which also serves as the site URL unless a different domain is specified using a CNAME file) as the root domain for all other &lt;code&gt;gh-pages&lt;/code&gt; branches on the Github repo.&lt;/p&gt;
&lt;p&gt;Thus, I have created repositories named &lt;code&gt;2015&lt;/code&gt;, &lt;code&gt;2014&lt;/code&gt;, etc, which will serve the notebooks for the corresponding year from their own &lt;code&gt;gh-pages&lt;/code&gt; branch. Moving my &lt;code&gt;www.carlboettiger.info&lt;/code&gt; (the use of a subdomain such as &lt;code&gt;www&lt;/code&gt; is required in order to benefit from Github’s CDN, though if it is omitted the domain provider will add it) from my &lt;code&gt;labnotebook&lt;/code&gt; repo to my &lt;code&gt;cboettig.github.io&lt;/code&gt; repository means that the annual repositories now have base URLs such as &lt;code&gt;www.carlboettiger.info/2015&lt;/code&gt;, &lt;code&gt;www.carlboettiger.info/2014&lt;/code&gt;. Adjusting the &lt;code&gt;_config.yml&lt;/code&gt; to omit &lt;code&gt;/year:&lt;/code&gt; from the permalink, since it is already in the base URL, is all that is needed to ensure that the posts of all my old URLs will still resolve to the same pages. Excellent.&lt;/p&gt;
&lt;p&gt;Dealing with the site pages is more tricky than dealing with the posts. Pages come in two variates: some, like &lt;code&gt;index.html&lt;/code&gt;, &lt;code&gt;research.html&lt;/code&gt;, &lt;code&gt;vita.html&lt;/code&gt;, contain only content that is independent of whatever is in the notebook pages and thus can live quite happily in the &lt;code&gt;cboettig.github.io&lt;/code&gt; repository. Others, like &lt;code&gt;tags.html&lt;/code&gt;, &lt;code&gt;categories.html&lt;/code&gt;, &lt;code&gt;archive.html&lt;/code&gt;, &lt;code&gt;lab-notebook.html&lt;/code&gt;, &lt;code&gt;atom.xml&lt;/code&gt; and other tag-specific RSS feeds are dynamically generated by Jekyll using the metadata of the posts, and thus need to live in the individual notebook repositories instead.&lt;/p&gt;
&lt;p&gt;This instead of just having the page: &lt;a href=&quot;http://carlboettiger.info/tags&quot;&gt;carlboettiger.info/tags&lt;/a&gt;, each year begins a new notebook with it’s own tags, categories, etc: &lt;a href=&quot;http://carlboettiger.info/2014/tags&quot;&gt;carlboettiger.info/2014/tags&lt;/a&gt;, &lt;a href=&quot;http://carlboettiger.info/2013/tags&quot;&gt;carlboettiger.info/2013/tags&lt;/a&gt;. For tags, categories,it makes some sense to have this information aggregated by year, avoiding the clutter of too many or too stale tags or categories (though perhaps something is lost by not being able to see this in aggregate across all years, at least not without some effort). Likewise for the list of posts by date (previously at &lt;code&gt;archive.html&lt;/code&gt;, now just turned into &lt;code&gt;index.html&lt;/code&gt;) is produced for each annual notebook, such that &lt;a href=&quot;http://carlboettiger.info/2014&quot;&gt;carlboettiger.info/2014&lt;/a&gt; resolves a reverse-chronological list of posts for that year alone.&lt;/p&gt;
&lt;p&gt;I must then address what to do about the original URLs such as &lt;a href=&quot;http://carlboettiger.info/tags&quot;&gt;carlboettiger.info/tags&lt;/a&gt;. Using a Jekyll liquid filter it is easy to define automatic redirects for &lt;code&gt;/tags.html&lt;/code&gt; and &lt;code&gt;/categories.html&lt;/code&gt; that will forward to the current year’s tag’s and categories, though perhaps an aggregated view would be preferable. For &lt;a href=&quot;http://carlboettiger.info/archive&quot;&gt;carlboettiger.info/archive&lt;/a&gt; I have provided manual links to the index of each annual notebook rather than a redirect to the index of only the most current notebook. Likewise for one of my most popular pages, &lt;a href=&quot;http://carlboettiger.info/lab-notebook&quot;&gt;carlboettiger.info/lab-notebook&lt;/a&gt;, I have retained the automated feeds from Github, Twitter, and Mendeley, but replaced the previews of the most recent posts with the less aesthetic link to the notebook by year. Meanwhile, I have provided each notebook with it’s own nine-panel preview page such as &lt;a href=&quot;http://carlboettiger.info/2014/lab-notebook&quot;&gt;carlboettiger.info/2014/lab-notebook&lt;/a&gt;, which has the preview but not the network feeds (Perhaps it would be better to move this to the index page). In this way, the social feeds can be updated merely by updating the &lt;code&gt;cboettig.github.io&lt;/code&gt; repo (since these are rendered as static text rather than javascript, written using the relevant API at the time the site is built.)&lt;/p&gt;
&lt;p&gt;A more tricky case is that of the atom feeds. It doesn’t really make sense to subscribe to a &lt;code&gt;carlboettiger.info/2015/blog.xml&lt;/code&gt; feed that will be inactive in a year. Using HTML redirects in a &lt;code&gt;.xml&lt;/code&gt; file doesn’t make too much sense, so I will try the RSS-flavor redirect:&lt;/p&gt;
&lt;pre class=&quot;sourceCode xml&quot;&gt;&lt;code class=&quot;sourceCode xml&quot;&gt;&lt;span class=&quot;kw&quot;&gt;&amp;lt;newLocation&amp;gt;&lt;/span&gt;
http://www.carlboettiger.info/2015/blog.xml
&lt;span class=&quot;kw&quot;&gt;&amp;lt;/newLocation&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;though this seems less than ideal.&lt;/p&gt;
&lt;h2 id=&quot;automated-deploy&quot;&gt;Automated deploy&lt;/h2&gt;
&lt;p&gt;As I use the &lt;code&gt;jeykll-pandoc&lt;/code&gt; gem to have pandoc render the markdown, along with a few other custom plugins, I cannot take advantage of Github’s automated build for Jekyll and have instead relied on the trick of having Travis-CI build and deploy the site. Adding automated knitr building to the mix will make this too heavy for travis, even for more modular notebooks. Instead, I am relying on local building, together with automated builds from my own server running a Drone CI instance. More on this in a separate post.&lt;/p&gt;
&lt;h2 id=&quot;site-assets-templates&quot;&gt;Site assets, templates&lt;/h2&gt;
&lt;p&gt;Individual notebook repositories are thus much more light-weight. All css assets are in the root &lt;code&gt;cboettig.github.io&lt;/code&gt; repository or already provided by external CDNs (such as the FontAwesome icons or MathJax, and Bootstrap javascript). However, it is necessary that both all annual notebook repositories and the base repo have the Jekyll &lt;code&gt;_layouts&lt;/code&gt; and &lt;code&gt;_includes&lt;/code&gt; files required to template and build the pages. This is unfortunate, since it means maintaining multiple copies of the same file, but I haven’t figured out an easy way around it.&lt;/p&gt;
&lt;h2 id=&quot;pruning-history&quot;&gt;Pruning history&lt;/h2&gt;
&lt;p&gt;In breaking &lt;code&gt;labnotebook&lt;/code&gt; into component repos by year, I only want to preserve the history of that year, thus keeping the repositories small. This is particularly important for the root repo, &lt;code&gt;cboettig.github.io&lt;/code&gt;, since it will remain active.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;edit &lt;code&gt;_config.yml&lt;/code&gt; to remove &lt;code&gt;/:year&lt;/code&gt; from &lt;code&gt;_config.yml&lt;/code&gt; (the repository name will automatically be used as part of the URL)&lt;/li&gt;
&lt;li&gt;delete all posts from different years (preferable to just wait until deleting their history, which will remove the files as well), e.g. for 2014:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;ot&quot;&gt;files=&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;`echo&lt;/span&gt; &lt;span class=&quot;dt&quot;&gt;{_posts/2008-*,_posts/2009-*,_posts/2010-*,_posts/2011-*,_posts/2012-*,_posts/2013-*}&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;git&lt;/span&gt; filter-branch --index-filter &lt;span class=&quot;st&quot;&gt;&amp;quot;git rm -rf --cached --ignore-unmatch &lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;$files&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt; HEAD&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and remove the temporary backups immediately so that repository actually shrinks in size:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git update-ref -d refs/original/refs/heads/master
git reflog expire --expire=now --all
git gc --prune=now&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is more important in the root repository, since this will remain active. If the annual notebook entry repositories have some extra stuff in their &lt;code&gt;.git&lt;/code&gt; history it isn’t such an issue since they no longer need to grow or be moved around as much. (See this &lt;a href=&quot;http://stackoverflow.com/questions/2100907&quot;&gt;SO on rewriting git history&lt;/a&gt;.)&lt;/p&gt;
&lt;h2 id=&quot;my-progress-notes-during-the-remapping&quot;&gt;My progress notes during the remapping:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;[x] delete the CNAME file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] delete all the relatively static pages files that will be hosted directly from &lt;code&gt;cboettig.github.io&lt;/code&gt; (&lt;code&gt;index.html&lt;/code&gt;, &lt;code&gt;research.md&lt;/code&gt;, etc., but not dynamically created &lt;code&gt;tags.html&lt;/code&gt; etc).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] adjust &lt;code&gt;repo:&lt;/code&gt; in &lt;code&gt;_config.yml&lt;/code&gt; to match the repository year. This will automatically fix the sha and history links in the sidebar.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] Other tweaks to the sidebar: &lt;code&gt;site.repo&lt;/code&gt; liquid must be added to categories, tags, next, and previous links.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] Automated deploy for active and root repositories.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] Plan for &lt;code&gt;labnotebook&lt;/code&gt; repo. History is preserved, but issues, github stars, etc. Use as template for the new years?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] Activate! Remove CNAME from &lt;code&gt;labnotebook&lt;/code&gt; repo, add &lt;code&gt;www&lt;/code&gt; CNAME to &lt;code&gt;cboettig.github.io&lt;/code&gt;. Consider removing &lt;code&gt;gh-pages&lt;/code&gt; branch of lab-notebook?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] Fix / workaround for the root atom feeds.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] Syncing assets, layout, and deploy scripts? Perhaps it is best to allow these to diverge and newer notebooks to look different than older ones?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
</entry>




</feed>
