<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Lab Notebook - </title>
  <link href="//.xml" rel="self"/>
  <link href="/"/>
  <updated>2016-01-03T22:28:13+00:00</updated>
  <id>http://www.carlboettiger.info/</id>
  <author>
    <name>Carl Boettiger</name>
    <email>cboettig@gmail.com</email>
  </author>

  
<entry>
  <title>Reflecting on five years of the open lab notebook</title>
  <link href="http://www.carlboettiger.info/2015/12/31/reflections-on-the-open-lab-notebook.html"/>
 <updated>2015-12-31T00:00:00+00:00</updated>
  <id>/12/31/reflections-on-the-open-lab-notebook</id>
  <content type="html">&lt;p&gt;I have been keeping an open lab notebook now since &lt;a href=&quot;http://www.carlboettiger.info/2010/02/02/The-Lab-Notebook-Goes-Open-.html&quot;&gt;February of 2010&lt;/a&gt; &lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Those years have seen 271, 301, 164, 136, 86 and 44 posts, respectively. The notebook started on &lt;a href=&quot;http://openwetware.org/wiki/Lab_Notebook&quot;&gt;OpenWetWare&lt;/a&gt;, with code on (now defunct) Google Code (using svn!). &lt;a href=&quot;http://www.carlboettiger.info/2010/10/21/spatial-warning-signals-power-etc.html&quot;&gt;By October&lt;/a&gt; of that year I had moved to a Wordpress.org platform running on DreamHost, with code moving to GitHub earlier that year. Wordpress meant my own domain name, control of layout and plugins, better mobile support, and perhaps most importantly, avoided the occasional day-long down-times I had seen on OpenWetWare. By the end of 2010 I had also cobbled together &lt;a href=&quot;http://www.carlboettiger.info/2010/12/11/socialr-an-r-package-to-track-the-status-of-computations-with-social-network-tools.html&quot;&gt;a system&lt;/a&gt; in which my scripts committed themselves to GitHub and automatically posted figures the generated to &lt;a href=&quot;https://www.flickr.com/photos/cboettig&quot;&gt;flickr&lt;/a&gt; that included the Git hash. &lt;a href=&quot;https://cran.r-project.org/web/packages/knitr/index.html&quot;&gt;Knitr&lt;/a&gt; was first released to CRAN in January of 2012, and by March I announced a &lt;a href=&quot;http://www.carlboettiger.info/2012/03/21/knitr-github-and-a-new-phase-for-the-lab-notebook.html&quot;&gt;new phase&lt;/a&gt; for my lab notebook focused on notes written with knitr markdown living on GitHub. For the first time, part of the daily research narrative lived not directly in the lab notebook itself (thus the decline in total notebook posts starting that year). Most of my posts in the first two years are much more journal like; reflecting on what I have been working on or reading earlier that day, describing some equation or graph, or muddling over a sticking point. Some of this remained in the notebook, particularly for work unconnected to code (thoughts on some literature, mathematical derivations, etc), but more migrated to knitr documents outside of the notebook. While knitr brought the code, results, and discussion elements closer together, it also often meant I was writing with less distance from the results, and so the text shifted towards more technical and less reflective content. Notebook entries became more brief and bullet pointed, though also punctuated by an increasing frequency of notebook entries that really were blog posts, aimed at an audience and sometimes drawing lengthy comment sections, primarily on topics related to open science and research workflow.&lt;/p&gt;
&lt;p&gt;In May of 2012, I then shifted platforms again, this time &lt;a href=&quot;http://www.carlboettiger.info/2012/05/01/Jekyll-vs-Wordpress.html&quot;&gt;moving to Jekyll&lt;/a&gt;, primarily for reasons of performance and cost (as detailed in that post). I have always experimented heavily with different features and technology connected to the lab notebook, and Jekyll also opened new possibilities. Meanwhile, it fit nicely into the git/markdown-centric workflow I had already adapted. This made it easy to turn knitr outputs into notebook entries, though I waffled back on forth on whether it was more natural to post such material in the relevant project repository in GitHub or directly into the notebook. The notebook remained the home for work not (yet) connected to a project, and for blog posts, and occasionally cross-posted material on a particular project, often capturing a key result. Meanwhile more of my other work became open – in particular, I began writing my manuscripts in public GitHub project repositories from the start, and also making greater use of GitHub issues in projects. The end of 2012 also saw me leave graduate school and begin a post-doc, luckily with advisers that were equally neutral to my open notebook habit.&lt;/p&gt;
&lt;p&gt;The start of 2015 saw the next significant evolution in platform. Though still based on Jekyll, I first I broke up what had been growing into one rather large &lt;code&gt;labnotebook&lt;/code&gt; repo on GitHub into separate repos by year. This left me with a more lightweight repository and freed me to experiment with infrastructure without breaking all my old posts. I &lt;a href=&quot;http://www.carlboettiger.info/2015/01/01/notebook-maintenance-and-scaling.html&quot;&gt;began writing&lt;/a&gt; all posts directly in knitr’s &lt;code&gt;.Rmd&lt;/code&gt; format, allowing the code to be run automatically when the site was published, rather than executing it locally and copying over a &lt;code&gt;.md&lt;/code&gt; file into the notebook. That this was possible is thanks to my adoption of Docker (making the myriad dependencies portable), Continuous integration platforms that could run the code (using the Docker container) and automatically push the compiled site, and &lt;code&gt;servr&lt;/code&gt;, another Yihui Xie R package that facilitates knitr+jekyll integration. Some entries involved particularly heavy computation, including a few that required several hours on a large Amazon EC2 machine to complete. &lt;code&gt;knitr&lt;/code&gt;’s caching feature made it easy to snapshot these caches (which I did in a somewhat convoluted way with another docker container) so that the whole site could still be rebuilt on a public CI service (primarily circle-ci) within the 50 minute permitted window. The Docker+knitr+CI combination brought reproducible to a new level, though still not perfectly automated, since changing dependencies can still have some effect, particularly since I allow the container to be rebuilt from Dockerfile rather than committed as a binary image. I’ve been pleased that this system has worked remarkably well the entire year, though I’m frequently nervous that some piece of its increasingly complex architecture will break. Moreover, the complicated re-running of all the R code was increasingly redundant as most of my work continued to happen in GitHub repositories which had largely adopted their own system of continuous integration and unit testing (for basic functions and final results, if not for individual notebook posts).&lt;/p&gt;
&lt;p&gt;More and more, GitHub, knitr, and other such tools have allowed me to actually perform my research openly, not just discuss it in an open notebook. Perhaps ironically, this means less and less content in the ‘lab notebook’ itself (as I &lt;a href=&quot;http://www.carlboettiger.info/2012/09/28/Welcome-to-my-lab-notebook.html&quot;&gt;wrote in 2012&lt;/a&gt;, ‘the real notebook is on GitHub’). Interestingly, this has the side effect of making the ‘lab notebook’ much less visible. Living on my website, my open notebook has been hard to miss; a visitor could easily page through the entries to see what I had done recently without any of the familiarity it requires to navigate to the same material on GitHub. The idea that this was a scientist’s &lt;em&gt;notebook&lt;/em&gt;, a concept both immediately familiar and yet so rarely public, perhaps gave it an out-sized significance to having the same material scattered around GitHub (some under my account and other work in various GitHub Organization accounts), where openness was already the norm. It attracted the attention of &lt;a href=&quot;http://www.carlboettiger.info/vita#media-interviews&quot;&gt;reporters&lt;/a&gt; and, I’m told, my faculty search committee. So while I think &lt;em&gt;I&lt;/em&gt; have benefited from this visibility, for the science it is perhaps much the same.&lt;/p&gt;
&lt;p&gt;Halfway though 2015 also saw a different kind of change to my notebook as I transitioned from a post-doc to a faculty position. Just as my homepage (now maintained in its own GitHub repo, though still appearing in the same theme as my notebook) shifted from being “me” to being “my research group,” part of my time also shifted from “my research” to mentoring the research of others. While I continue to believe in transparency in scientific process and results, I have also appreciated that as a graduate student open science was something I was allowed to choose for myself. I want those I mentor to make their own choices as well. Thus we have begun projects with students primarily in private GitHub repositories (now hosted on &lt;a href=&quot;https://github.com/boettiger-lab&quot;&gt;github.com/boettiger-lab&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;So what does this mean for 2016 and the future of my open lab notebook? I suspect it will continue in its role as occasional blog and communication tool, where it’s greater visibility has a practical use, while becoming simpler technically, with the day-to-day of research (&amp;amp; teaching, etc) activity confined to their own repositories.&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;though some material going back to &lt;a href=&quot;http://www.carlboettiger.info/2000/lab-notebook&quot;&gt;2009&lt;/a&gt; and &lt;a href=&quot;http://www.carlboettiger.info/2008/lab-notebook&quot;&gt;2008&lt;/a&gt; was posted later.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
</entry>

<entry>
  <title>Open Science Post Doc</title>
  <link href="http://www.carlboettiger.info/2015/04/20/open-science-post-doc.html"/>
 <updated>2015-04-20T00:00:00+00:00</updated>
  <id>/04/20/open-science-post-doc</id>
  <content type="html">&lt;p&gt;This post is a response to the query posted by Titus Brown on &lt;a href=&quot;http://ivory.idyll.org/blog/2015-how-to-find-openscience-advisor.html&quot;&gt;advice for doing an Open Science Post doc&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Open Science is a broad tent, and I believe there are many ways to engage in open science without crossing boundaries of collaborators or PIs. Some of the most valuable open science practices are those least likely to cross boundaries: in particular, those practices associated with post-publication of academic papers. Many widely regarded journals, including &lt;em&gt;Nature&lt;/em&gt; and &lt;em&gt;Science&lt;/em&gt;, have relatively strong data publication requirements, increasingly strong code publication expectations, and are compatible with the use of pre-print archives. Yes, their are those who would hesitate even to comply with the most minimal interpretation of these expectations; but there are many more who, given the expectation to do this at all, would appreciate your knowledge and effort on doing these things well and consistent with best practices. Using data repositories instead of supplemental material; exhibiting good management of data and code; providing good metadata in consistent formats; citing software and data appropriately.&lt;/p&gt;
&lt;p&gt;Many of the practices promoted by open scientists work almost as well in a setting that is closed until you ‘flip the switch’ to make them public; even if that is long after publication. Good data management, a private Github repository, or a private electronic lab notebook are all ways to leverage best practices tools and approaches in a setting that can either be shared securely or made open later. Attitudes to post-publication sharing are rarely black-and-white, and having everything curated and ready to go ahead of time can help nudge collaborators in the direction of best practices.&lt;/p&gt;
&lt;p&gt;I’ve been a post-doc for just over 2 years while enjoying a relatively open-science approach to my research: I’ve kept an open lab notebook, posted my papers ahead of publication on pre-print archives, released code, data, and knitr versions of my papers, discuss my work on social media, sign reviews, shared grant applications, contributed open source software and been active in open science communities.&lt;/p&gt;
&lt;p&gt;I was fortunate to have independent funding for most but not all of my post-doc, and to work in a field where researchers are often given substantial independence and in which open practices are common. Nonetheless these were not practices shared by either of my two excellent co-advisers, who gave only a neutral or vaguely positive response to these ideas.&lt;/p&gt;
&lt;p&gt;Nevertheless, I learned more about &lt;em&gt;old-school&lt;/em&gt; open science from them than I had ever imagined. When I sent Marc the first draft of our paper, I had buried almost all of the equations in a curt appendix with minimal and jargon-laden supporting text. No sir, those equations not only had to appear in the main text, but I must endeavor to explain the meaning and relevance of each one in a language clear and concise enough for any ecologist to follow. I won’t claim to have succeeded, but boy did continuous integration on my unit tests feel like a low bar for openness by comparison.&lt;/p&gt;
&lt;p&gt;Meanwhile, I also felt I had the support and mentorship of an online community of open scientists even without going to work for one of them. I am thankful that my mentors have always been tolerant of my open science experimentation, but I would have enjoyed working with them even if it had been otherwise. There was much to learn from them, much to learn from the open science community, and after all, a post-doc position doesn’t last forever.&lt;/p&gt;
</content>
</entry>

<entry>
  <title>Question-and-Answer-post on my use of Docker in research</title>
  <link href="http://www.carlboettiger.info/2015/02/24/docker-science-question-and-answer.html"/>
 <updated>2015-02-24T00:00:00+00:00</updated>
  <id>/02/24/docker-science-question-and-answer</id>
  <content type="html">&lt;p&gt;I received an interesting email from an academic computing unit regarding the use of Docker in my research:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We’ve been reading, listing, and prototyping best practices for building base images, achieving image composition, addressing interoperability, and standardizing on common APIs. When I read your paper, I thought you might have some opinions on the subject.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Would you be willing to share your experiences using Docker for research with our team? It doesn’t have to be a formal presentation. In fact, we generally prefer interactive conversations over slides, abstracts, etc. I appreciate that you must be terribly busy with your postdoc fellowship and rOpenScience responsibilities. If you’re not able to speak, perhaps you can answer a few questions about your use of Docker.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here are some quick answers in reply; though like the questions themselves my replies are on the technical end and don’t give a useful overview of how I’m actually using Docker. Maybe that’s a subject for another post some time.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Are you currently still using Docker for your research? If so, how are you integrating that into your more demanding computational needs?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Yes. Docker helps me quickly provision and deploy a temporary compute environment on a cloud server with resources appropriate to the computation. This model much more accurately reflects the nature of computational research in a discipline such as ecology than does the standard HPC cluster model. My research typically involves many different tasks that can be easily separated and do not need the specialized architecture of a supercomputer, but do rely on a wide range of existing software tools and frequently also rely on internet connectivity for accessing certain data resources, etc. Because Docker makes it very easy for me to deploy customized software environments locally and on cloud computing resources, it facilitates my process of testing, scaling and distributing tasks to the appropriate computational resources quickly, while also increasing the portability &amp;amp; reproducibility of my work by colleagues who can benefit from the prebuilt environment provided by the container.&lt;/p&gt;
&lt;ol start=&quot;2&quot; type=&quot;1&quot;&gt;
&lt;li&gt;How/do you make use of data containers?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I rarely make use of data containers. I find they, and container orchestration more generally, are very well suited for deploying a service app (such as an API), but are less natural for composing scientific research environments which requires orchestrating volumes instead of tcp links. For instance, at present, there is no interface in &lt;code&gt;--volumes-from&lt;/code&gt; to mount the shared volume at a different mount point in the different container. Thus one cannot just link libraries from different containers with a &lt;code&gt;-v /usr/lib&lt;/code&gt; or &lt;code&gt;-v /usr/bin&lt;/code&gt;, as this would clobber the existing libraries.&lt;/p&gt;
&lt;p&gt;Also, it’s rather a nuisance that on the current Debian/Ubuntu kernels at least, &lt;code&gt;docker rm&lt;/code&gt; does not fully clean up space from data containers (though we now have &lt;code&gt;docker rm -v&lt;/code&gt;)&lt;/p&gt;
&lt;ol start=&quot;3&quot; type=&quot;1&quot;&gt;
&lt;li&gt;What are you using to run your containers in production?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Production is a diverse notion in scientific research – from a software perspective scientific work is almost 100% development and 0% production. For containers running public, always-on services, I tend to run from a dedicated, appropriately resourced cloud server such as Digital Ocean. I don’t write such services very often (though we have been doing this to deploy public APIs for data recently), so this is the closest I get to ‘production’. I run my development environment and all my research code out of containers as well, both locally and on various cloud servers.&lt;/p&gt;
&lt;p&gt;In all cases, I tend to run containers using the Docker CLI. I’ve found fig places larger resource requirements to run the same set of containers – so much so that it will fail to start on a machine that can run the containers fine from CLI or Fleet. fig also feels immature; it does not provide anything close to parity with the Docker CLI options.&lt;/p&gt;
&lt;p&gt;Further, while I find orchestration a powerful concept that is well suited for certain use-cases (our recent API uses five containers), for many academic research uses I find that orchestration is both unnecessary and a barrier to use. Orchestration works really well for professionally designed, web native, open source stack: our recent API deployment uses Redis, MySQL, NGINX, Sinatra, Unicorn, Logstash, ElasticSearch and Kibana – services that are all readily composed from official Docker containers into an existing application. Most scientific work looks nothing like this – the common elements tend to be shared libraries that are not well adapted to the same abstraction into separate services.&lt;/p&gt;
&lt;ol start=&quot;4&quot; type=&quot;1&quot;&gt;
&lt;li&gt;How are you sharing them aside from the Docker Hub?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Primarily through making the Dockerfiles available on Github. This makes it easy for others to build the images locally, and also fork and modify the Dockerfile directly. I maintain a private Docker registry as well but rarely have need for it.&lt;/p&gt;
&lt;ol start=&quot;5&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Do you have practical experience and advice about achieving real portability with Docker across hosting environments (ie. stick with X as an OS, use a sidekick and data container for data backups and snapshotting, etc)?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Overall this hasn’t been much of an issue. Sharing volumes with the host machine on hosts that require virtualization/boot2docker was an early problem, though this has been much better since Docker 1.3. In a similar vein, getting &lt;code&gt;boot2docker&lt;/code&gt; running on older hardware can be problematic. And of course docker isn’t really compatible with 32 bit machines.&lt;/p&gt;
&lt;p&gt;After spending some time with CoreOS, I tend to use Ubuntu machines when running in the cloud: ‘highly available’ isn’t much of a priority in the research context, where few things are at a scale where hardware failure is an issue. I found CoreOS worked poorly on individual machines or cluster sizes that might shrink below 2; while the new OS model was a barrier to entry for myself and for collaborators. I suspect this situation will improve dramatically as these tools gain polish and abstraction that requires less manual tooling for common patterns (I find that ambassador containers, sidekick containers, etc place too many OS-level tasks on the shoulders of the user). Of course there is a large ecosystem of solutions in this space, which also needs time to harden into standards.&lt;/p&gt;
&lt;p&gt;Perhaps my comments re: CLI vs fig in Q3 are also relevant here?&lt;/p&gt;
&lt;ol start=&quot;6&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Have the computational requirements of your research codes outgrown a physical node?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Not at the present. I’ve run prior work on clusters on a campus and at the DOE’s Carver machine at NERSC, though at this time I can almost always meet computational needs with the larger single instances of a service like EC2 or DigitalOcean. Much more often I have the need to run many different codes (sometimes related things that could be parallelized in a single command but are better off distributed, but much more often unrelated tasks) at the same time. Being able to deploy these in isolated, pre-provisioned environments on one or multiple machines using Docker has thus been a huge gain in both my efficiency and realized computational performance. If any particular task becomes too intensive, Docker makes it very easy to move over to a new cloud instance with more resources that will not interfere with other tasks.&lt;/p&gt;
&lt;ol start=&quot;7&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Do you have a workflow for versioning and releasing images comparable to GitFlow?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Nope, though maybe this would be a good idea. I work almost exclusively with automated builds and hardly ever use &lt;code&gt;docker commit&lt;/code&gt;. Though the Dockerfiles themselves are versioned, obviously the software that is installed between different automated builds can differ substantially, and there is in general no way to recover an earlier build. Using a &lt;code&gt;docker commit&lt;/code&gt; workflow instead would provide this more versioned history and greater stability of the precise binaries installed, but also feels more like a black box as the whole thing cannot then be reproduced from scratch.&lt;/p&gt;
&lt;ol start=&quot;8&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Do you version your Dockerfile/image/source code separately?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I version all my Dockerfiles with git. I version my images as needed but more rarely, and in a way that reflects the version of the predominant software they install (e.g. r-base:3.1.2).&lt;/p&gt;
&lt;ol start=&quot;9&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Do you prefer using the entrypoint or command to run your containers by default?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I prefer command (&lt;code&gt;CMD&lt;/code&gt;), as it is more semantic, easier to alter the default and can take longer strings (no need for a flag).&lt;/p&gt;
</content>
</entry>

<entry>
  <title>Notebook Maintenance And Scaling</title>
  <link href="http://www.carlboettiger.info/2015/01/01/notebook-maintenance-and-scaling.html"/>
 <updated>2015-01-01T00:00:00+00:00</updated>
  <id>/01/01/notebook-maintenance-and-scaling</id>
  <content type="html">&lt;p&gt;Electronic notebooks may not run out of pages like a paper notebook, but with five years of entries (963 posts, with a repository size approaching half a gigabyte), together with various files, layouts, experimentation and version history, some thought must be given to scale. Two closely related considerations add to this further: dynamic builds with &lt;code&gt;knitr&lt;/code&gt; from &lt;code&gt;.Rmd&lt;/code&gt; versions and hosting image files directly in the notebook repository rather than uploading to an external site (previously flickr or on the gh-pages of other project repositories). This has several advantages (more on that later) but in the immediate term it makes building the repository potentially slower (though knitr’s caching helps) and increases the repository size more rapidly (even with text-based &lt;code&gt;svg&lt;/code&gt; images).&lt;/p&gt;
&lt;p&gt;The current Jekyll system keeps all posts in a single repository and rebuilds the HTML files for each every time. This is already showing some strains: for instance, for some reason the git hashes when generating the site automatically on Travis cease updating for older posts, though this problem doesn’t occur when building locally. Overall, the Jekyll platform is rather snappy so this isn’t an unmanageable size, but is sufficient to demonstrate that the approach isn’t able to scale indefinitely either.&lt;/p&gt;
&lt;p&gt;So, as with the paper notebook whose pages are filled, it’s time to crack open a new binding and shelve the old notebooks – somewhere handy to be sure, but no longer one voluminous tome on the desk.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/2015/assets/figures/posts/2015-01-01/notebooks-shelf.jpg&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;a-multi-repository-approach&quot;&gt;A multi-repository approach&lt;/h2&gt;
&lt;p&gt;To address this, I’m am trying out breaking the notebook over multiple repositories: using a new repository for each year’s worth of entries, and an additional repository to provide the basic pages (&lt;code&gt;home&lt;/code&gt;, &lt;code&gt;teaching&lt;/code&gt;, &lt;code&gt;vita&lt;/code&gt;, etc. from the navbar) along with the assets used by all the other sites (css, fonts, javascript, etc). This avoids rebuilding the posts of notebooks from all previous years every time the Jekyll site is compiled, keeping the repositories smaller, the site more modular and more easy to scale.&lt;/p&gt;
&lt;p&gt;This raises some challenges such as keeping the layout and appearance consistent without maintaining copies of layout files across multiple repositories; managing URLs and paths across different repositories, and aggregating metadata (posts, tags, categories).&lt;/p&gt;
&lt;h2 id=&quot;repos-paths-and-urls-for-the-multi-notebook&quot;&gt;Repos, Paths, and URLs for the multi-notebook&lt;/h2&gt;
&lt;p&gt;Even with the source files (such as &lt;code&gt;.md&lt;/code&gt; entries, templates, etc.) in different repositories it would be simple enough to combine the generated HTML files from each repository into a single output directory serving the site (on Github or elsewhere). However, GitHub’s &lt;code&gt;gh-pages&lt;/code&gt; provide an elegantly more modular way to do this already. GitHub uses the URL of the user’s repository (the repo named &lt;code&gt;username.github.io&lt;/code&gt;, which also serves as the site URL unless a different domain is specified using a CNAME file) as the root domain for all other &lt;code&gt;gh-pages&lt;/code&gt; branches on the Github repo.&lt;/p&gt;
&lt;p&gt;Thus, I have created repositories named &lt;code&gt;2015&lt;/code&gt;, &lt;code&gt;2014&lt;/code&gt;, etc, which will serve the notebooks for the corresponding year from their own &lt;code&gt;gh-pages&lt;/code&gt; branch. Moving my &lt;code&gt;www.carlboettiger.info&lt;/code&gt; (the use of a subdomain such as &lt;code&gt;www&lt;/code&gt; is required in order to benefit from Github’s CDN, though if it is omitted the domain provider will add it) from my &lt;code&gt;labnotebook&lt;/code&gt; repo to my &lt;code&gt;cboettig.github.io&lt;/code&gt; repository means that the annual repositories now have base URLs such as &lt;code&gt;www.carlboettiger.info/2015&lt;/code&gt;, &lt;code&gt;www.carlboettiger.info/2014&lt;/code&gt;. Adjusting the &lt;code&gt;_config.yml&lt;/code&gt; to omit &lt;code&gt;/year:&lt;/code&gt; from the permalink, since it is already in the base URL, is all that is needed to ensure that the posts of all my old URLs will still resolve to the same pages. Excellent.&lt;/p&gt;
&lt;p&gt;Dealing with the site pages is more tricky than dealing with the posts. Pages come in two variates: some, like &lt;code&gt;index.html&lt;/code&gt;, &lt;code&gt;research.html&lt;/code&gt;, &lt;code&gt;vita.html&lt;/code&gt;, contain only content that is independent of whatever is in the notebook pages and thus can live quite happily in the &lt;code&gt;cboettig.github.io&lt;/code&gt; repository. Others, like &lt;code&gt;tags.html&lt;/code&gt;, &lt;code&gt;categories.html&lt;/code&gt;, &lt;code&gt;archive.html&lt;/code&gt;, &lt;code&gt;lab-notebook.html&lt;/code&gt;, &lt;code&gt;atom.xml&lt;/code&gt; and other tag-specific RSS feeds are dynamically generated by Jekyll using the metadata of the posts, and thus need to live in the individual notebook repositories instead.&lt;/p&gt;
&lt;p&gt;This instead of just having the page: &lt;a href=&quot;http://carlboettiger.info/tags&quot;&gt;carlboettiger.info/tags&lt;/a&gt;, each year begins a new notebook with it’s own tags, categories, etc: &lt;a href=&quot;http://carlboettiger.info/2014/tags&quot;&gt;carlboettiger.info/2014/tags&lt;/a&gt;, &lt;a href=&quot;http://carlboettiger.info/2013/tags&quot;&gt;carlboettiger.info/2013/tags&lt;/a&gt;. For tags, categories,it makes some sense to have this information aggregated by year, avoiding the clutter of too many or too stale tags or categories (though perhaps something is lost by not being able to see this in aggregate across all years, at least not without some effort). Likewise for the list of posts by date (previously at &lt;code&gt;archive.html&lt;/code&gt;, now just turned into &lt;code&gt;index.html&lt;/code&gt;) is produced for each annual notebook, such that &lt;a href=&quot;http://carlboettiger.info/2014&quot;&gt;carlboettiger.info/2014&lt;/a&gt; resolves a reverse-chronological list of posts for that year alone.&lt;/p&gt;
&lt;p&gt;I must then address what to do about the original URLs such as &lt;a href=&quot;http://carlboettiger.info/tags&quot;&gt;carlboettiger.info/tags&lt;/a&gt;. Using a Jekyll liquid filter it is easy to define automatic redirects for &lt;code&gt;/tags.html&lt;/code&gt; and &lt;code&gt;/categories.html&lt;/code&gt; that will forward to the current year’s tag’s and categories, though perhaps an aggregated view would be preferable. For &lt;a href=&quot;http://carlboettiger.info/archive&quot;&gt;carlboettiger.info/archive&lt;/a&gt; I have provided manual links to the index of each annual notebook rather than a redirect to the index of only the most current notebook. Likewise for one of my most popular pages, &lt;a href=&quot;http://carlboettiger.info/lab-notebook&quot;&gt;carlboettiger.info/lab-notebook&lt;/a&gt;, I have retained the automated feeds from Github, Twitter, and Mendeley, but replaced the previews of the most recent posts with the less aesthetic link to the notebook by year. Meanwhile, I have provided each notebook with it’s own nine-panel preview page such as &lt;a href=&quot;http://carlboettiger.info/2014/lab-notebook&quot;&gt;carlboettiger.info/2014/lab-notebook&lt;/a&gt;, which has the preview but not the network feeds (Perhaps it would be better to move this to the index page). In this way, the social feeds can be updated merely by updating the &lt;code&gt;cboettig.github.io&lt;/code&gt; repo (since these are rendered as static text rather than javascript, written using the relevant API at the time the site is built.)&lt;/p&gt;
&lt;p&gt;A more tricky case is that of the atom feeds. It doesn’t really make sense to subscribe to a &lt;code&gt;carlboettiger.info/2015/blog.xml&lt;/code&gt; feed that will be inactive in a year. Using HTML redirects in a &lt;code&gt;.xml&lt;/code&gt; file doesn’t make too much sense, so I will try the RSS-flavor redirect:&lt;/p&gt;
&lt;pre class=&quot;sourceCode xml&quot;&gt;&lt;code class=&quot;sourceCode xml&quot;&gt;&lt;span class=&quot;kw&quot;&gt;&amp;lt;newLocation&amp;gt;&lt;/span&gt;
http://www.carlboettiger.info/2015/blog.xml
&lt;span class=&quot;kw&quot;&gt;&amp;lt;/newLocation&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;though this seems less than ideal.&lt;/p&gt;
&lt;h2 id=&quot;automated-deploy&quot;&gt;Automated deploy&lt;/h2&gt;
&lt;p&gt;As I use the &lt;code&gt;jeykll-pandoc&lt;/code&gt; gem to have pandoc render the markdown, along with a few other custom plugins, I cannot take advantage of Github’s automated build for Jekyll and have instead relied on the trick of having Travis-CI build and deploy the site. Adding automated knitr building to the mix will make this too heavy for travis, even for more modular notebooks. Instead, I am relying on local building, together with automated builds from my own server running a Drone CI instance. More on this in a separate post.&lt;/p&gt;
&lt;h2 id=&quot;site-assets-templates&quot;&gt;Site assets, templates&lt;/h2&gt;
&lt;p&gt;Individual notebook repositories are thus much more light-weight. All css assets are in the root &lt;code&gt;cboettig.github.io&lt;/code&gt; repository or already provided by external CDNs (such as the FontAwesome icons or MathJax, and Bootstrap javascript). However, it is necessary that both all annual notebook repositories and the base repo have the Jekyll &lt;code&gt;_layouts&lt;/code&gt; and &lt;code&gt;_includes&lt;/code&gt; files required to template and build the pages. This is unfortunate, since it means maintaining multiple copies of the same file, but I haven’t figured out an easy way around it.&lt;/p&gt;
&lt;h2 id=&quot;pruning-history&quot;&gt;Pruning history&lt;/h2&gt;
&lt;p&gt;In breaking &lt;code&gt;labnotebook&lt;/code&gt; into component repos by year, I only want to preserve the history of that year, thus keeping the repositories small. This is particularly important for the root repo, &lt;code&gt;cboettig.github.io&lt;/code&gt;, since it will remain active.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;edit &lt;code&gt;_config.yml&lt;/code&gt; to remove &lt;code&gt;/:year&lt;/code&gt; from &lt;code&gt;_config.yml&lt;/code&gt; (the repository name will automatically be used as part of the URL)&lt;/li&gt;
&lt;li&gt;delete all posts from different years (preferable to just wait until deleting their history, which will remove the files as well), e.g. for 2014:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;ot&quot;&gt;files=&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;`echo&lt;/span&gt; &lt;span class=&quot;dt&quot;&gt;{_posts/2008-*,_posts/2009-*,_posts/2010-*,_posts/2011-*,_posts/2012-*,_posts/2013-*}&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;git&lt;/span&gt; filter-branch --index-filter &lt;span class=&quot;st&quot;&gt;&amp;quot;git rm -rf --cached --ignore-unmatch &lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;$files&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt; HEAD&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and remove the temporary backups immediately so that repository actually shrinks in size:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git update-ref -d refs/original/refs/heads/master
git reflog expire --expire=now --all
git gc --prune=now&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is more important in the root repository, since this will remain active. If the annual notebook entry repositories have some extra stuff in their &lt;code&gt;.git&lt;/code&gt; history it isn’t such an issue since they no longer need to grow or be moved around as much. (See this &lt;a href=&quot;http://stackoverflow.com/questions/2100907&quot;&gt;SO on rewriting git history&lt;/a&gt;.)&lt;/p&gt;
&lt;h2 id=&quot;my-progress-notes-during-the-remapping&quot;&gt;My progress notes during the remapping:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;[x] delete the CNAME file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] delete all the relatively static pages files that will be hosted directly from &lt;code&gt;cboettig.github.io&lt;/code&gt; (&lt;code&gt;index.html&lt;/code&gt;, &lt;code&gt;research.md&lt;/code&gt;, etc., but not dynamically created &lt;code&gt;tags.html&lt;/code&gt; etc).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] adjust &lt;code&gt;repo:&lt;/code&gt; in &lt;code&gt;_config.yml&lt;/code&gt; to match the repository year. This will automatically fix the sha and history links in the sidebar.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] Other tweaks to the sidebar: &lt;code&gt;site.repo&lt;/code&gt; liquid must be added to categories, tags, next, and previous links.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] Automated deploy for active and root repositories.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] Plan for &lt;code&gt;labnotebook&lt;/code&gt; repo. History is preserved, but issues, github stars, etc. Use as template for the new years?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] Activate! Remove CNAME from &lt;code&gt;labnotebook&lt;/code&gt; repo, add &lt;code&gt;www&lt;/code&gt; CNAME to &lt;code&gt;cboettig.github.io&lt;/code&gt;. Consider removing &lt;code&gt;gh-pages&lt;/code&gt; branch of lab-notebook?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] Fix / workaround for the root atom feeds.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[x] Syncing assets, layout, and deploy scripts? Perhaps it is best to allow these to diverge and newer notebooks to look different than older ones?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
</entry>




</feed>
